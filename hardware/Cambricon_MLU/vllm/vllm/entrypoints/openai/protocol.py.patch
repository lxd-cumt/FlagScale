diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index 79f0f200c..e139a735e 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -255,6 +255,15 @@ class ChatCompletionRequest(OpenAIBaseModel):
     # NOTE this will be ignored by vLLM -- the model determines the behavior
     parallel_tool_calls: Optional[bool] = False
     user: Optional[str] = None
+    
+    data_parallel_rank: Optional[int] = Field(
+        default=None,
+        description=(
+            "The data parallel rank to use for this request. "
+            "If specified, the request will be routed to the specified "
+            "data parallel rank. Must be between 0 and data_parallel_size-1."
+        ),
+    )
 
     # --8<-- [start:chat-completion-sampling-params]
     best_of: Optional[int] = None
@@ -771,6 +780,15 @@ class CompletionRequest(OpenAIBaseModel):
     top_p: Optional[float] = None
     user: Optional[str] = None
 
+    data_parallel_rank: Optional[int] = Field(
+        default=None,
+        description=(
+            "The data parallel rank to use for this request. "
+            "If specified, the request will be routed to the specified "
+            "data parallel rank. Must be between 0 and data_parallel_size-1."
+        ),
+    )
+
     # --8<-- [start:completion-sampling-params]
     use_beam_search: bool = False
     top_k: Optional[int] = None

