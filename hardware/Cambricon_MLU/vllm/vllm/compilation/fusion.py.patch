diff --git a/vllm/compilation/fusion.py b/vllm/compilation/fusion.py
index 7e2c5b4fe..3d1bc49db 100644
--- a/vllm/compilation/fusion.py
+++ b/vllm/compilation/fusion.py
@@ -30,8 +30,21 @@ def empty_fp32(*args, **kwargs):
     return torch.empty(*args, **kwargs, dtype=torch.float32, device="cuda")
 
 
-RMS_OP = torch.ops._C.rms_norm.default
-RMS_ADD_OP = torch.ops._C.fused_add_rms_norm.default
+'''
+=============================
+Modify by vllm_mlu
+=============================
+@brief: skip custom op implemention
+'''
+# RMS_OP = torch.ops._C.rms_norm.default
+# RMS_ADD_OP = torch.ops._C.fused_add_rms_norm.default
+RMS_OP = None
+RMS_ADD_OP = None
+'''
+==================
+End of MLU Hijack
+==================
+'''
 
 
 class QuantKey(NamedTuple):
@@ -58,13 +71,24 @@ kFp8StaticTensorSym = QuantKey(FP8_DTYPE, True, True, True)
 kFp8DynamicTensorSym = QuantKey(FP8_DTYPE, False, True, True)
 kFp8DynamicTokenSym = QuantKey(FP8_DTYPE, False, False, True)
 
+'''
+=============================
+Modify by vllm_mlu
+=============================
+@brief: skip fp8 quant op implemention
+'''
 QUANT_OPS: dict[QuantKey, OpOverload] = {
-    kFp8StaticTensorSym: torch.ops._C.static_scaled_fp8_quant.default,  # noqa
-    kFp8DynamicTensorSym:
-    torch.ops._C.dynamic_scaled_fp8_quant.default,  # noqa
-    kFp8DynamicTokenSym:
-    torch.ops._C.dynamic_per_token_scaled_fp8_quant.default,  # noqa
+    # kFp8StaticTensorSym: torch.ops._C.static_scaled_fp8_quant.default,  # noqa
+    # kFp8DynamicTensorSym:
+    # torch.ops._C.dynamic_scaled_fp8_quant.default,  # noqa
+    # kFp8DynamicTokenSym:
+    # torch.ops._C.dynamic_per_token_scaled_fp8_quant.default,  # noqa
 }
+'''
+==================
+End of MLU Hijack
+==================
+'''
 
 
 class FusedRMSQuantKey(NamedTuple):
@@ -80,17 +104,27 @@ class FusedRMSQuantKey(NamedTuple):
         return (f"FusedQuantKey({self.quant}, with"
                 f"{'' if self.fused_add else 'out'} residual)")
 
-
+'''
+=============================
+Modify by vllm_mlu
+=============================
+@brief: skip fp8 quant op implemention
+'''
 FUSED_OPS: dict[FusedRMSQuantKey, OpOverload] = {
-    FusedRMSQuantKey(kFp8StaticTensorSym, False):
-    torch.ops._C.rms_norm_static_fp8_quant.default,  # noqa
-    FusedRMSQuantKey(kFp8StaticTensorSym, True):
-    torch.ops._C.fused_add_rms_norm_static_fp8_quant.default,  # noqa
-    FusedRMSQuantKey(kFp8DynamicTokenSym, False):
-    torch.ops._C.rms_norm_dynamic_per_token_quant.default,  # noqa
-    FusedRMSQuantKey(kFp8DynamicTokenSym, True):
-    torch.ops._C.rms_norm_dynamic_per_token_quant.default,  # noqa
+    # FusedRMSQuantKey(kFp8StaticTensorSym, False):
+    # torch.ops._C.rms_norm_static_fp8_quant.default,  # noqa
+    # FusedRMSQuantKey(kFp8StaticTensorSym, True):
+    # torch.ops._C.fused_add_rms_norm_static_fp8_quant.default,  # noqa
+    # FusedRMSQuantKey(kFp8DynamicTokenSym, False):
+    # torch.ops._C.rms_norm_dynamic_per_token_quant.default,  # noqa
+    # FusedRMSQuantKey(kFp8DynamicTokenSym, True):
+    # torch.ops._C.rms_norm_dynamic_per_token_quant.default,  # noqa
 }
+'''
+==================
+End of MLU Hijack
+==================
+'''
 
 
 class QuantMultiOutputMatch(MultiOutputMatch):

