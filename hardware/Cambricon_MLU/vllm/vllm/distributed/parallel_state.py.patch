diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index 10f87c49b..abac4a407 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -381,6 +381,23 @@ class GroupCoordinator:
                               dim: int) -> torch.Tensor:
         return self.device_communicator.all_gather(input_, dim)
 
+    def all_gather_into_list(self, input_: torch.Tensor)-> list[torch.Tensor]:
+        """
+        NOTE: We assume that the input tensor is on the same device across
+        all the ranks.
+        """
+        world_size = self.world_size
+        # Bypass the function if we are using only 1 GPU.
+        if world_size == 1:
+            return [input_]
+        # Allocate output tensor.
+        gather_list = [torch.empty_like(input_) for _ in range(world_size)]
+        # All Gather.
+        group = (self.cpu_group
+                 if input_.device.type.startswith("cpu") else self.device_group)
+        torch.distributed.all_gather(gather_list, input_, group=group)
+        return gather_list
+
     def reduce_scatter(self,
                        input_: torch.Tensor,
                        dim: int = -1) -> torch.Tensor:
@@ -841,7 +858,9 @@ def init_model_parallel_group(
 _TP: Optional[GroupCoordinator] = None
 
 
-def get_tp_group() -> GroupCoordinator:
+def get_tp_group(tp_group: Any = None) -> GroupCoordinator:
+    if tp_group is not None:
+        return tp_group
     assert _TP is not None, ("tensor model parallel group is not initialized")
     return _TP
 
@@ -1068,6 +1087,11 @@ def initialize_model_parallel(
         _DP.rank_in_group, _PP.rank_in_group, _TP.rank_in_group,
         _EP.rank_in_group)
 
+    from vllm.distributed.mlu_parallel_state import initialize_model_parallel_mlu
+    initialize_model_parallel_mlu(tensor_model_parallel_size,
+                                  pipeline_model_parallel_size,
+                                  backend)
+
 
 def ensure_model_parallel_initialized(
     tensor_model_parallel_size: int,
@@ -1180,6 +1204,9 @@ def destroy_model_parallel():
         _EP.destroy()
     _EP = None
 
+    from vllm.distributed.mlu_parallel_state import destroy_model_parallel_mlu
+    destroy_model_parallel_mlu()
+
 
 def destroy_distributed_environment():
     global _WORLD

