diff --git a/vllm/distributed/communication_op.py b/vllm/distributed/communication_op.py
index 0a5a95176..87a158157 100644
--- a/vllm/distributed/communication_op.py
+++ b/vllm/distributed/communication_op.py
@@ -9,33 +9,44 @@ import torch.distributed
 from .parallel_state import get_tp_group
 
 
-def tensor_model_parallel_all_reduce(input_: torch.Tensor) -> torch.Tensor:
+def tensor_model_parallel_all_reduce(input_: torch.Tensor,
+                                     tp_group: Any = None) -> torch.Tensor:
     """All-reduce the input tensor across model parallel group."""
-    return get_tp_group().all_reduce(input_)
+    return get_tp_group(tp_group).all_reduce(input_)
 
 
 def tensor_model_parallel_all_gather(input_: torch.Tensor,
-                                     dim: int = -1) -> torch.Tensor:
+                                     dim: int = -1,
+                                     tp_group: Any = None) -> torch.Tensor:
     """All-gather the input tensor across model parallel group."""
-    return get_tp_group().all_gather(input_, dim)
+    return get_tp_group(tp_group).all_gather(input_, dim)
 
 
 def tensor_model_parallel_reduce_scatter(input_: torch.Tensor,
-                                         dim: int = -1) -> torch.Tensor:
+                                         dim: int = -1,
+                                         tp_group: Any = None) -> torch.Tensor:
     """Reduce-Scatter the input tensor across model parallel group."""
-    return get_tp_group().reduce_scatter(input_, dim)
+    return get_tp_group(tp_group).reduce_scatter(input_, dim)
 
 
 def tensor_model_parallel_gather(input_: torch.Tensor,
                                  dst: int = 0,
-                                 dim: int = -1) -> Optional[torch.Tensor]:
+                                 dim: int = -1,
+                                 tp_group: Any = None) -> Optional[torch.Tensor]:
     """Gather the input tensor across model parallel group."""
-    return get_tp_group().gather(input_, dst, dim)
+    return get_tp_group(tp_group).gather(input_, dst, dim)
 
 
 def broadcast_tensor_dict(tensor_dict: Optional[dict[Any, Union[torch.Tensor,
                                                                 Any]]] = None,
-                          src: int = 0):
+                          src: int = 0,
+                          tp_group: Any = None):
     if not torch.distributed.is_initialized():
         return tensor_dict
-    return get_tp_group().broadcast_tensor_dict(tensor_dict, src)
+    return get_tp_group(tp_group).broadcast_tensor_dict(tensor_dict, src)
+
+
+def tensor_model_parallel_all_gather_into_list(
+        input_: torch.Tensor, tp_group: Any = None) -> Optional[list[torch.Tensor]]:
+    """Gather the input tensor across model parallel group."""
+    return get_tp_group(tp_group).all_gather_into_list(input_)

