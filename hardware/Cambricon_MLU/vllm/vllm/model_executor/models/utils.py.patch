diff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py
index aa88f4210..727451c34 100644
--- a/vllm/model_executor/models/utils.py
+++ b/vllm/model_executor/models/utils.py
@@ -110,6 +110,7 @@ class AutoWeightsLoader:
         self,
         weights: Iterable[tuple[str, torch.Tensor]],
     ) -> Iterable[tuple[str, Iterable[tuple[str, torch.Tensor]]]]:
+        weights = sorted(weights, key=lambda x: x[0])
         weights_by_parts = ((weight_name.split(".", 1), weight_data)
                             for weight_name, weight_data in weights)
 
@@ -557,6 +558,14 @@ def maybe_offload_to_cpu(module: torch.nn.Module) -> torch.nn.Module:
         assert uva_available, ("V1 CPU offloading requires"
                                " uva (pin memory) support")
         uva_offloading = True
+        # NOTE: MLU-V1 not support uva now
+        from vllm.platforms import current_platform
+        if current_platform.is_out_of_tree():
+            logger.warning_once(
+                f"MLU-V1 not support UVA now, temporarily "
+                f"use H2D copy to support '--cpu-offload-gb'."
+            )
+            uva_offloading = False
     else:
         uva_offloading = False
 

