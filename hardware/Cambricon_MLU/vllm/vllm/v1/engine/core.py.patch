diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index f36a491a1..dd0f6f006 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -316,6 +316,9 @@ class EngineCore:
 
     def execute_dummy_batch(self):
         self.model_executor.collective_rpc("execute_dummy_batch")
+        
+    def response_remote_alloc_once(self):
+        self.model_executor.collective_rpc("response_remote_alloc_once")
 
     def add_lora(self, lora_request: LoRARequest) -> bool:
         return self.model_executor.add_lora(lora_request)
@@ -542,11 +545,24 @@ class EngineCoreProc(EngineCore):
             if logger.isEnabledFor(DEBUG) and self.input_queue.empty():
                 logger.debug("EngineCore waiting for work.")
                 waited = True
-            req = self.input_queue.get()
-            self._handle_client_request(*req)
+                
+            if self.vllm_config.kv_transfer_config is not None and \
+                self.vllm_config.kv_transfer_config.kv_role == "kv_consumer":
+                self.response_remote_alloc_once()
+                if self.input_queue.empty():
+                    continue
+                req = self.input_queue.get_nowait()
+                self._handle_client_request(*req)    
+            else:
+                req = self.input_queue.get()
+                self._handle_client_request(*req)
 
         if waited:
             logger.debug("EngineCore loop active.")
+            
+        if self.vllm_config.kv_transfer_config is not None and \
+            self.vllm_config.kv_transfer_config.kv_role == "kv_consumer":
+            self.response_remote_alloc_once()
 
         # Handle any more client requests.
         while not self.input_queue.empty():
@@ -873,12 +889,16 @@ class DPEngineCoreProc(EngineCoreProc):
                 local_unfinished_reqs)
 
             if not self.engines_running:
-                if self.dp_rank == 0:
+                if self.dp_rank == 0 or not self.has_coordinator:
                     # Notify client that we are pausing the loop.
                     logger.debug("Wave %d finished, pausing engine loop.",
                                  self.current_wave)
+                    # In the coordinator case, dp rank 0 sends updates to the
+                    # coordinator. Otherwise (offline spmd case), each rank
+                    # sends the update to its colocated front-end process.
+                    client_index = -1 if self.has_coordinator else 0
                     self.output_queue.put_nowait(
-                        (-1,
+                        (client_index,
                          EngineCoreOutputs(wave_complete=self.current_wave)))
                 self.current_wave += 1
 

