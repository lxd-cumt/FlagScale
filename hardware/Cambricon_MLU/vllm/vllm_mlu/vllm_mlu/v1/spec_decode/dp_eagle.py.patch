diff --git a/vllm_mlu/vllm_mlu/v1/spec_decode/dp_eagle.py b/vllm_mlu/vllm_mlu/v1/spec_decode/dp_eagle.py
new file mode 100644
index 000000000..f148c9f5e
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/spec_decode/dp_eagle.py
@@ -0,0 +1,592 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+from typing import List, Optional, Any
+import copy
+
+import torch
+import torch.nn.functional as F
+from vllm.forward_context import set_forward_context
+from vllm.model_executor.models.llama_eagle3 import Eagle3LlamaForCausalLM
+from vllm.v1.attention.backends.flash_attn import (CommonAttentionMetadata,
+                                                   FlashAttentionMetadata)
+from vllm.v1.sample.metadata import SamplingMetadata
+
+from vllm.v1.spec_decode.eagle import PADDING_SLOT_ID, logger
+
+from vllm_mlu.v1.attention.backends.utils import (
+    MLUCommonAttentionMetadata, COMMON_METADATA_STR)
+from vllm_mlu._mlu_utils import *
+from vllm_mlu.v1.attention.backends.utils import MLUInferMode
+
+from vllm_mlu.mlu_forward_context import MLUDPMetadata
+from vllm_mlu.v1.spec_decode.eagle import MluEagleProposer
+from vllm_mlu.model_executor.models.dp_utils import (
+    enable_data_parallel,
+    DataParallelRuntimeParams
+)
+from vllm.distributed.communication_op import tensor_model_parallel_all_gather_into_list
+from vllm.distributed import (
+    get_logits_tp_world_size,
+    get_logits_tp_group,
+    get_tensor_model_parallel_world_size,
+)
+from vllm_mlu.v1.attention.backends.flash_attn import pad_attn_metadata
+
+class DPMluEagleProposer(MluEagleProposer):
+
+    def get_logits_batch_sizes(self, batch_size: int) -> Optional[List[int]]:
+        tp_world_size, logits_batch_sizes = get_logits_tp_world_size(), None
+        if tp_world_size != get_tensor_model_parallel_world_size():
+            tp_tensor = torch.tensor([batch_size]).to(self.runner.device)
+            outputs = tensor_model_parallel_all_gather_into_list(tp_tensor, get_logits_tp_group())
+            # Convert device tensor to host list
+            outputs = torch.cat(outputs).tolist()
+            logits_batch_sizes = [outputs[i] for i in range(tp_world_size)]
+        return logits_batch_sizes
+
+    def propose_ds_execute_dummy_batch(
+        self,
+        # [num_tokens]
+        target_token_ids: torch.Tensor,
+        # [num_tokens]
+        target_positions: torch.Tensor,
+        # [num_tokens, hidden_size]
+        target_hidden_states: torch.Tensor,
+        dp_params: DataParallelRuntimeParams,
+    ) -> tuple[torch.Tensor, torch.Tensor]:
+        # num_scheduled_tokens
+        num_tokens = target_token_ids.shape[0]
+        input_ids = self.input_ids[:num_tokens]
+        # Shift the input ids by one token.
+        # E.g., [a1, b1, b2, c1, c2, c3] -> [b1, b2, c1, c2, c3, c3]
+        input_ids[:-1] = target_token_ids[1:]
+
+        # always skip attn compute
+        attn_metadata: Optional[dict[str, Any]] = None
+
+        # Get graph capture related infomation for deepseek model.
+
+        with set_forward_context(attn_metadata, self.vllm_config, num_tokens=num_tokens):
+            hidden_states = self.model(
+                input_ids=input_ids,
+                positions=target_positions,
+                previous_hidden_states=target_hidden_states,
+                intermediate_tensors=None,
+                inputs_embeds=None,
+                dp_params=dp_params,
+            )
+        if dp_params is not None:
+            dp_params.logits_batch_split_list = self.get_logits_batch_sizes(num_tokens)
+        _ = self.model.compute_logits(hidden_states, None, dp_params=dp_params)
+
+        if self.num_speculative_tokens == 1:
+            return
+        '''
+        =============================
+        Modify by vllm_mlu
+        @brief: support k > 1, need run draft model k-1 times
+        =============================
+        '''
+        # support k > 1
+        for _ in range(self.num_speculative_tokens - 1):
+            new_dp_params = self.runner._get_data_parallel_metadata(
+                num_tokens, num_tokens, True, [1] * num_tokens)
+            with set_forward_context(attn_metadata, self.vllm_config, num_tokens=num_tokens):
+                hidden_states = self.model(
+                    input_ids=input_ids,
+                    positions=target_positions,
+                    previous_hidden_states=target_hidden_states,
+                    intermediate_tensors=None,
+                    inputs_embeds=None,
+                    dp_params=new_dp_params,
+                )
+                _ = self.model.compute_logits(hidden_states, None, dp_params=new_dp_params)
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+    def propose(
+        self,
+        # [num_tokens]
+        target_token_ids: torch.Tensor,
+        # [num_tokens]
+        target_positions: torch.Tensor,
+        # [num_tokens, hidden_size]
+        target_hidden_states: torch.Tensor,
+        # [num_tokens]
+        target_slot_mapping: torch.Tensor,
+        # [batch_size]
+        next_token_ids: torch.Tensor,
+        # [batch_size + 1] starting with 0
+        cu_num_tokens: torch.Tensor,
+        # [batch_size, max_num_blocks_per_req]
+        block_table: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        main_model_common_metadata: MLUCommonAttentionMetadata,
+        # [batch_size]
+        num_rejected_tokens: torch.Tensor,
+        # [num_tokens]
+        token_indices: torch.Tensor,
+        whole_block_table: torch.Tensor,
+        main_model_dp_params: Optional[DataParallelRuntimeParams] = None,
+        time_markers: List =[],
+    ) -> torch.Tensor:
+        num_tokens = target_token_ids.shape[0]
+        batch_size = next_token_ids.shape[0]
+        last_token_indices = cu_num_tokens[1:] - 1
+        if self.method == "eagle3":
+            assert isinstance(self.model, Eagle3LlamaForCausalLM)
+            target_hidden_states = self.model.combine_hidden_states(
+                target_hidden_states)
+            assert target_hidden_states.shape[-1] == self.hidden_size
+
+        if num_rejected_tokens is None or self.method != "deepseek_mtp":
+            # Shift the input ids by one token.
+            # E.g., [a1, b1, b2, c1, c2, c3] -> [b1, b2, c1, c2, c3, c3]
+            self.input_ids[:num_tokens - 1] = target_token_ids[1:]
+            # Replace the last token with the next token.
+            # E.g., [b1, b2, c1, c2, c3, c3] -> [a2, b2, b3, c2, c3, c4]
+            self.input_ids[last_token_indices] = next_token_ids
+            hidden_states_indices = last_token_indices
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            @brief: prepare_input_ids is pad attn for rejected tokens,
+                    like [a1, b1, b2, c1] -> [a1, 0, b1, b2, c1, 0]
+            =============================
+            '''
+            input_ids, hidden_states_indices = self.prepare_input_ids(
+                target_token_ids=target_token_ids,
+                next_token_ids=next_token_ids,
+                token_indices=token_indices,
+                last_token_indices=last_token_indices,
+                num_rejected_tokens=num_rejected_tokens
+            )
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        # FA requires seq_len to have dtype int32.
+        seq_lens = (target_positions[last_token_indices] + 1).int()
+
+        if self.method in ["eagle", "eagle3"]:
+            # FIXME(woosuk): The below two ops cause synchronization. Optimize.
+            max_seq_len = seq_lens.max().item()
+            max_num_tokens = (cu_num_tokens[1:] -
+                                cu_num_tokens[:-1]).max().item()
+            attn_metadata = FlashAttentionMetadata(
+                num_actual_tokens=num_tokens,
+                max_query_len=max_num_tokens,
+                query_start_loc=cu_num_tokens,
+                max_seq_len=max_seq_len,
+                seq_lens=seq_lens,
+                block_table=block_table,
+                slot_mapping=target_slot_mapping,
+                # TODO(woosuk): Support cascade attention.
+                use_cascade=False,
+                common_prefix_len=0,
+                cu_prefix_query_lens=None,
+                prefix_kv_lens=None,
+                suffix_kv_lens=None,
+            )
+        elif self.method == "deepseek_mtp":
+            query_lens = cu_num_tokens[1:] - cu_num_tokens[:-1]
+            max_query_len = query_lens.max().item()
+
+            assert self.runner is not None
+
+            # FIXME: need to consider multiple kv_cache_groups
+            assert self.kv_cache_group_id is not None, (
+                "kv_cache_group_id is None")
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: 1. common_attn_metadata use main model's common_attn_metadata
+                    2. attn_metadata_builders use kv_cache_group_id
+            '''
+            attn_metadata = self.runner.attn_metadata_builders[self.kv_cache_group_id].build(
+                num_reqs=batch_size,
+                num_actual_tokens=num_tokens,
+                max_query_len=max_query_len,
+                common_prefix_len=0,
+                common_attn_metadata=main_model_common_metadata,
+            )
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+        else:
+            raise ValueError(f"Unsupported method: {self.method}")
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: Use full graph with draft model and pad batch_size for dp
+        '''
+        decode_only = (all(not prefill for prefill in main_model_dp_params.dp_is_prefill)
+                      if enable_data_parallel()
+                      else main_model_common_metadata.is_decode_only)
+        # Determine if we can use full graph
+        all_dp_max_query_lens = (max(main_model_dp_params.token_split_list)
+                                if enable_data_parallel()
+                                else batch_size * (self.num_speculative_tokens + 1))
+        captured_already = (hasattr(self, "draft_graph_runners") and 
+                           (all_dp_max_query_lens <= max(self.draft_graph_runners.keys()) * 
+                           (self.num_speculative_tokens + 1) if enable_data_parallel() else
+                           batch_size in self.draft_graph_runners))
+        use_full_graph = (VLLM_V1_USE_FULL_GRAPH and self.use_cuda_graph
+                            and decode_only and captured_already)
+        if (VLLM_V1_USE_FULL_GRAPH
+                and self.use_cuda_graph and decode_only and not use_full_graph):
+            logger.warning_once(
+                f"Select MLU-V1 Full-MLUGraph mode with drafter, however running in " +
+                f"eager mode: decode_only={decode_only}, captured_already={captured_already}, " +
+                f"num_tokens={num_tokens}."
+            )
+
+        # dp pad batch_size
+        if use_full_graph and enable_data_parallel():
+            paded_batch_size = self.vllm_config.pad_for_cudagraph(all_dp_max_query_lens // (self.num_speculative_tokens + 1))
+            num_input_tokens = paded_batch_size * (self.num_speculative_tokens + 1)
+        else:
+            paded_batch_size = batch_size
+            num_input_tokens = num_tokens
+
+        # change attn metadata num_actual_tokens
+        attn_metadata.num_actual_tokens = num_input_tokens
+
+        attn_metadata_copy = None
+        # pad attn metadata
+        if use_full_graph and enable_data_parallel() and num_input_tokens != num_tokens:
+            assert self.runner is not None
+            # copy attn metadata when k>1 for draft model,
+            # because dp pad batch_size will change attn metadata
+            if self.num_speculative_tokens > 1:
+                attn_metadata_copy = copy.deepcopy(attn_metadata)
+            # Update attention metadata.
+            pad_attn_metadata(
+                attn_metadata,
+                main_model_common_metadata,
+                whole_block_table,
+                self.runner,
+                num_tokens,
+                num_input_tokens,
+                batch_size,
+                paded_batch_size,
+            )
+
+            # Update input ids, pad with 0 if necessary.
+            token_pad_size = num_input_tokens - num_tokens
+            assert token_pad_size >= 0
+            # Update target hidden states, pad with zeros if necessary.
+            if token_pad_size > 0:
+                target_hidden_states = F.pad(
+                    target_hidden_states,
+                    (0, 0, 0, token_pad_size),
+                    value=0.0
+                )
+
+            # Update positions, pad with zeros if necessary.
+            if token_pad_size > 0:
+                target_positions = F.pad(
+                    target_positions,
+                    (0, token_pad_size),
+                    value=0
+                )
+
+        # At this moment, we assume all eagle layers belong to the same KV
+        # cache group, thus using the same attention metadata.
+        per_layer_attn_metadata = {}
+        for layer_name in self.attn_layer_names:
+            per_layer_attn_metadata[layer_name] = attn_metadata
+        per_layer_attn_metadata[COMMON_METADATA_STR] = main_model_common_metadata
+
+        # copy inputs to buffer for cudagraph
+        self.positions[:num_input_tokens] = target_positions
+        self.hidden_states[:num_input_tokens] = target_hidden_states
+
+        kwargs = {} if main_model_dp_params is None else {"dp_params": main_model_dp_params}
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN and not use_full_graph:
+            start = torch.mlu.Event(enable_timing=True)
+            start.record()
+        with set_forward_context(per_layer_attn_metadata,
+                                 self.vllm_config,
+                                 num_tokens=num_input_tokens):
+            if use_full_graph:
+                model_graph = self.draft_graph_runners[paded_batch_size]
+                ret_hidden_states = model_graph(
+                    input_ids=self.input_ids[:num_input_tokens],
+                    positions=self.positions[:num_input_tokens],
+                    previous_hidden_states=self.hidden_states[:num_input_tokens],
+                    intermediate_tensors=None,
+                    inputs_embeds=None,
+                )
+            else:
+                ret_hidden_states = self.model(
+                    input_ids=self.input_ids[:num_input_tokens],
+                    positions=self.positions[:num_input_tokens],
+                    previous_hidden_states=self.hidden_states[:num_input_tokens],
+                    **kwargs,
+                )
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                end = torch.mlu.Event(enable_timing=True)
+                end.record()
+                if use_full_graph:
+                    time_markers.append([model_graph.start, end])
+                else:
+                    time_markers.append([start, end])
+            if self.method == "deepseek_mtp":
+                last_hidden_states = ret_hidden_states
+            else:
+                last_hidden_states, hidden_states = ret_hidden_states
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        if main_model_dp_params is not None:
+            # Ensure main_model_dp_params has required attribute before assignment
+            if hasattr(main_model_dp_params, 'logits_batch_split_list'):
+                main_model_dp_params.logits_batch_split_list = self.get_logits_batch_sizes(batch_size)
+            else:
+                raise AttributeError("dp_params must have 'logits_batch_split_list' attribute")
+
+        sample_hidden_states = last_hidden_states[hidden_states_indices]
+        logits = self.model.compute_logits(sample_hidden_states, None, dp_params=main_model_dp_params)
+
+        draft_token_ids = logits.argmax(dim=-1)
+
+        # Early exit if there is only one draft token to be generated.
+        if self.num_speculative_tokens == 1:
+            # [batch_size, 1]
+            return draft_token_ids.view(-1, 1)
+
+        # TODO: Currently, MTP module released by deepseek only has
+        # one layer. Adapt this code to support multiple layers once
+        # there's a multi-layer MTP module.
+
+        # Generate the remaining draft tokens.
+        draft_token_ids_list = [draft_token_ids]
+        positions = target_positions[last_token_indices]
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        '''
+        hidden_states = last_hidden_states[hidden_states_indices]
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        if self.use_cuda_graph and \
+            batch_size <= self.cudagraph_batch_sizes[-1]:
+            input_batch_size = self.vllm_config.pad_for_cudagraph(batch_size)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: Use full graph and if decode only and no pad behavior.
+                Do not pad inputs if these conditions do not meet.
+            '''
+            if VLLM_V1_USE_FULL_GRAPH:
+                if (not decode_only or input_batch_size != batch_size):
+                    input_batch_size = batch_size
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            input_batch_size = batch_size
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: set num_decode_tokens to batch_size and max_query_len to 1
+        '''
+        if attn_metadata_copy is not None:
+            attn_metadata_draft = attn_metadata_copy
+        else:
+            attn_metadata_draft = attn_metadata
+        attn_metadata_draft.num_actual_tokens = input_batch_size
+        attn_metadata_draft.max_query_len = 1
+        attn_metadata_draft.query_start_loc = self.arange[:input_batch_size + 1]        
+        main_model_common_metadata.max_query_len = 1
+        main_model_common_metadata.num_actual_tokens = input_batch_size
+        main_model_common_metadata.num_input_tokens = input_batch_size
+
+        # set per_layer_attn_metadata_draft
+        per_layer_attn_metadata_draft = {}    
+        if main_model_common_metadata.infer_mode != MLUInferMode.DECODE_ONLY:
+            seq_lens = torch.ones(input_batch_size, dtype=torch.int32,)
+            cu_num_tokens = torch.cumsum(seq_lens, dim=0)
+            query_start_loc = torch.empty(input_batch_size + 1, dtype=torch.int32)
+            query_start_loc[0] = 0
+            query_start_loc[1:] = cu_num_tokens
+            seq_start_loc = self.arange[:input_batch_size + 1]
+            common_attn_metadata_k = MLUCommonAttentionMetadata.build(
+                query_start_loc=query_start_loc.to(self.runner.device),
+                seq_lens=seq_lens.to(self.runner.device),
+                seq_start_loc=seq_start_loc.to(self.runner.device),
+                is_start_loc_match=False, # not prefill
+                max_query_len=1,
+                num_actual_tokens=input_batch_size,
+                num_speculative_tokens=self.num_speculative_tokens,
+                has_prefill_reqs=main_model_common_metadata.infer_mode == MLUInferMode.CHUNKED,
+            )
+            if attn_metadata_draft.num_decodes > 0:
+                self.runner.attn_metadata_builders[self.kv_cache_group_id]._num_decode_tokens = self.runner.attn_metadata_builders[self.kv_cache_group_id]._num_decodes
+                self.runner.attn_metadata_builders[self.kv_cache_group_id].decoder_query_len = 1
+            attn_metadata_k = self.runner.attn_metadata_builders[self.kv_cache_group_id].build(
+                num_reqs=input_batch_size,
+                num_actual_tokens=input_batch_size,
+                max_query_len=1,
+                common_prefix_len=0,
+                common_attn_metadata=common_attn_metadata_k,
+            )
+            if attn_metadata_k.num_decodes > 0:
+                attn_metadata_k.decode.max_query_len = 1
+            for layer_name in self.attn_layer_names:
+                per_layer_attn_metadata_draft[layer_name] = attn_metadata_k
+            per_layer_attn_metadata_draft[COMMON_METADATA_STR] = common_attn_metadata_k
+        else:
+            attn_metadata_draft.num_decode_tokens = input_batch_size
+            attn_metadata_draft.decode.max_query_len = 1
+            # decode only use attn_metadata_draft and main_model_common_metadata for optimize
+            for layer_name in self.attn_layer_names:
+                per_layer_attn_metadata_draft[layer_name] = attn_metadata_draft
+            per_layer_attn_metadata_draft[COMMON_METADATA_STR] = main_model_common_metadata    
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        for _ in range(self.num_speculative_tokens - 1):
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: get dp_params for draft model
+            '''
+            # dp_params for draft model
+            if main_model_dp_params is not None:
+                dp_params = self.runner._get_data_parallel_metadata(
+                    input_batch_size,
+                    input_batch_size,
+                    main_model_common_metadata.is_decode_only,
+                    [1] * input_batch_size
+                )
+            kwargs = {} if main_model_dp_params is None else {"dp_params": dp_params}
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+            # Update the inputs.
+            # cast to int32 is crucial when eagle model is compiled.
+            # tensor.argmax() returns int64 by default.
+            input_ids = draft_token_ids_list[-1].int()
+            positions += 1
+
+            # NOTE(woosuk): We should handle the case where the draft model
+            # generates tokens beyond the max model length. Since it is complex
+            # to remove such requests from the batch, we keep them in the batch
+            # but adjust the position ids and slot mappings to avoid the
+            # out-of-range access during the model execution. The draft tokens
+            # generated with this adjustment should be ignored.
+            exceeds_max_model_len = positions >= self.max_model_len
+            # Mask out the position ids that exceed the max model length.
+            # Otherwise, we may get out-of-range error in RoPE.
+            clamped_positions = torch.where(exceeds_max_model_len, 0,
+                                            positions)
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: deepseek_mtp do not need to update following attn_metadata
+            '''
+            if self.method != "deepseek_mtp":
+                # Increment the sequence lengths.
+                attn_metadata.max_seq_len += 1
+                attn_metadata.seq_lens += 1
+                # Consider max model length.
+                attn_metadata.max_seq_len = min(attn_metadata.max_seq_len,
+                                                self.max_model_len)
+                # For the requests that exceed the max model length, we set the
+                # sequence length to 1 to minimize their overheads in attention.
+                attn_metadata.seq_lens.masked_fill_(exceeds_max_model_len, 1)
+    
+                # Compute the slot mapping.
+                block_numbers = clamped_positions // self.block_size
+                block_ids = block_table.gather(dim=1,
+                                                index=block_numbers.view(-1, 1))
+                block_ids = block_ids.view(-1)
+                attn_metadata.slot_mapping = (block_ids * self.block_size +
+                                                clamped_positions % self.block_size)
+                # Mask out the slot mappings that exceed the max model length.
+                # Otherwise, the KV cache will be inadvertently updated with the
+                # padding tokens.
+                attn_metadata.slot_mapping.masked_fill_(exceeds_max_model_len,
+                                                        PADDING_SLOT_ID)
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+            # copy inputs to buffer for cudagraph
+            self.input_ids[:batch_size] = input_ids
+            self.positions[:batch_size] = clamped_positions
+            self.hidden_states[:batch_size] = hidden_states
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: record latency
+            '''
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                start = torch.mlu.Event(enable_timing=True)
+                start.record()
+            # Run the model.
+            with set_forward_context(per_layer_attn_metadata_draft,
+                                     self.vllm_config,
+                                     num_tokens=input_batch_size):
+                ret_hidden_states = self.model(
+                    input_ids=self.input_ids[:input_batch_size],
+                    positions=self.positions[:input_batch_size],
+                    previous_hidden_states=self.hidden_states[:input_batch_size],
+                    **kwargs,
+                )
+                if self.method == "deepseek_mtp":
+                    last_hidden_states = ret_hidden_states
+                else:
+                    last_hidden_states, hidden_states = ret_hidden_states
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                end = torch.mlu.Event(enable_timing=True)
+                end.record()
+                time_markers.append([start, end])
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+            hidden_states = last_hidden_states[:batch_size]
+            logits = self.model.compute_logits(last_hidden_states[:batch_size],
+                                                None, dp_params=dp_params)
+    
+            # TODO(wenlong): get more than one token for tree attention
+            draft_token_ids = logits.argmax(dim=-1)
+            draft_token_ids_list.append(draft_token_ids)
+    
+        # [batch_size, num_speculative_tokens]
+        draft_token_ids = torch.stack(draft_token_ids_list, dim=1)
+        return draft_token_ids

