diff --git a/vllm_mlu/vllm_mlu/v1/spec_decode/eagle.py b/vllm_mlu/vllm_mlu/v1/spec_decode/eagle.py
new file mode 100644
index 000000000..5635987bd
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/spec_decode/eagle.py
@@ -0,0 +1,705 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+import torch
+import torch.nn as nn
+from typing import List, Optional
+from vllm.attention.layer import Attention
+from vllm.config import (CompilationLevel, VllmConfig,
+                         get_layers_from_vllm_config)
+from vllm.distributed.parallel_state import get_pp_group
+from vllm.forward_context import set_forward_context
+from vllm.logger import init_logger
+from vllm.model_executor.model_loader import get_model
+from vllm.model_executor.models import supports_multimodal
+from vllm.model_executor.models.llama_eagle3 import Eagle3LlamaForCausalLM
+from vllm.v1.attention.backends.flash_attn import (CommonAttentionMetadata,
+                                                   FlashAttentionMetadata)
+from vllm.v1.kv_cache_interface import KVCacheConfig
+from vllm.v1.sample.metadata import SamplingMetadata
+from vllm.v1.spec_decode.utils import prepare_eagle_input_kernel
+
+from vllm.v1.spec_decode.eagle import EagleProposer, PADDING_SLOT_ID, logger
+
+from vllm_mlu.v1.attention.backends.utils import (
+    MLUCommonAttentionMetadata, get_common_metadata_from_attn_metadata,
+    get_common_metadata, COMMON_METADATA_STR)
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+from vllm_mlu._mlu_utils import *
+from vllm_mlu.v1.attention.backends.utils import MLUInferMode
+
+
+class MluEagleProposer(EagleProposer):
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        device: torch.device,
+        runner=None,
+    ):
+        self.vllm_config = vllm_config
+        self.speculative_config = vllm_config.speculative_config
+        self.draft_model_config = self.speculative_config.draft_model_config
+        self.method = self.speculative_config.method
+
+        self.runner = runner
+
+        self.dtype = vllm_config.model_config.dtype
+        self.max_model_len = vllm_config.model_config.max_model_len
+        self.block_size = vllm_config.cache_config.block_size
+        self.num_speculative_tokens = (
+            self.speculative_config.num_speculative_tokens)
+        self.max_num_tokens = (
+            vllm_config.scheduler_config.max_num_batched_tokens)
+        # We need to get the hidden size from the draft model config because
+        # the draft model's hidden size can be different from the target model's
+        # hidden size (e.g., Llama 3.3 70B).
+        self.hidden_size = self.draft_model_config.get_hidden_size()
+
+        self.use_cuda_graph = ((self.vllm_config.compilation_config.level
+                                == CompilationLevel.PIECEWISE
+                                or VLLM_V1_USE_FULL_GRAPH) and
+                                not self.vllm_config.model_config.enforce_eager)
+        self.cudagraph_batch_sizes = list(
+            reversed(
+                self.vllm_config.compilation_config.cudagraph_capture_sizes))
+
+        # persistent buffers for cuda graph
+        self.input_ids = torch.zeros(self.max_num_tokens,
+                                        dtype=torch.int32,
+                                        device=device)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: tmo positions need to be int32
+        '''
+        self.positions = torch.zeros(self.max_num_tokens,
+                                        dtype=torch.int32,
+                                        device=device)
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        self.hidden_states = torch.zeros(
+            (self.max_num_tokens, self.hidden_size),
+            dtype=self.dtype,
+            device=device)
+        # We need +1 here because the arange is used to set query_start_loc,
+        # which has one more element than batch_size.
+        self.arange = torch.arange(vllm_config.scheduler_config.max_num_seqs +
+                                    1,
+                                    device=device,
+                                    dtype=torch.int32)
+        '''
+        =============================
+        Modify by vllm_mlu
+        @brief: Now kv_cache is stored in groups, need to get the corresponding group_id
+        FIXME: need to be removed after update https://github.com/vllm-project/vllm/pull/20022
+        =============================
+        '''
+        self.kv_cache_group_id = None
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+    
+    '''
+    =============================
+    Modify by vllm_mlu
+    @brief: Replenish tokens for rejected token positions
+    =============================
+    '''
+    def prepare_input_ids(
+        self,
+        target_token_ids: torch.Tensor,
+        next_token_ids: torch.Tensor,
+        token_indices: torch.Tensor,
+        last_token_indices: torch.Tensor,
+        num_rejected_tokens: torch.Tensor = None,
+    ) -> torch.Tensor:
+        """Prepare input token IDs by handling token shifting and masking operations.
+
+        Args:
+            target_token_ids: Tensor of shape [num_tokens] containing original token IDs
+            next_token_ids: Tensor of shape [batch_size] containing next token for each batch
+            token_indices: Tensor of shape [num_valid_tokens] indicating indices of valid tokens
+            last_token_indices: Tensor of shape [batch_size] indicating last token index for each batch
+            num_rejected_tokens: Tensor of shape [batch_size] indicating number of rejected tokens per batch
+
+        Returns:
+            torch.Tensor: Processed input tensor of shape [num_tokens] with -1 at invalid positions
+        """
+        num_tokens = target_token_ids.shape[0]
+
+        # Create input tensor filled with 0
+        # input_ids = torch.full_like(target_token_ids, 0)
+        input_ids = self.input_ids[:num_tokens]
+
+        # Create mask for valid tokens
+        valid_mask = torch.zeros(num_tokens, dtype=torch.bool, device=target_token_ids.device)
+        valid_mask[token_indices] = True
+
+        # Calculate shifted IDs
+        shifted_ids = torch.zeros_like(target_token_ids)
+        shifted_ids[:-1] = target_token_ids[1:]  # Shift forward by one position
+        shifted_ids[-1] = target_token_ids[-1]   # Keep last position unchanged
+
+        # Handle rejected tokens
+        if num_rejected_tokens is not None and torch.any(num_rejected_tokens > 0):
+            # Use vectorized operations to calculate replace_idx_list
+            # If num_rejected_tokens > 0, use last_token_indices - num_rejected_tokens
+            # Otherwise use last_token_indices
+            reject_mask = num_rejected_tokens > 0
+            replace_indices = torch.where(
+                reject_mask,
+                last_token_indices - num_rejected_tokens,  # When there are rejected tokens
+                last_token_indices  # When no rejected tokens
+            )
+        else:
+            replace_indices = last_token_indices
+
+        # Set next token for each batch
+        shifted_ids[replace_indices] = next_token_ids
+
+        # Apply mask to keep only valid positions
+        self.input_ids[:num_tokens] = torch.where(valid_mask, shifted_ids, input_ids)
+        return self.input_ids[:num_tokens], replace_indices
+    '''
+    =============================
+    End of MLU Hijack
+    =============================
+    '''
+
+    def propose(
+        self,
+        # [num_tokens]
+        target_token_ids: torch.Tensor,
+        # [num_tokens]
+        target_positions: torch.Tensor,
+        # [num_tokens, hidden_size]
+        target_hidden_states: torch.Tensor,
+        # [num_tokens]
+        target_slot_mapping: torch.Tensor,
+        # [batch_size]
+        next_token_ids: torch.Tensor,
+        # [batch_size + 1] starting with 0
+        cu_num_tokens: torch.Tensor,
+        # [batch_size, max_num_blocks_per_req]
+        block_table: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        main_model_common_metadata: MLUCommonAttentionMetadata,
+        # [batch_size]
+        num_rejected_tokens: torch.Tensor,
+        # [num_tokens]
+        token_indices: torch.Tensor,
+        time_markers: List = [],
+    ) -> torch.Tensor:
+        num_tokens = target_token_ids.shape[0]
+        batch_size = next_token_ids.shape[0]
+        last_token_indices = cu_num_tokens[1:] - 1
+        if self.method == "eagle3":
+            assert isinstance(self.model, Eagle3LlamaForCausalLM)
+            target_hidden_states = self.model.combine_hidden_states(
+                target_hidden_states)
+            assert target_hidden_states.shape[-1] == self.hidden_size
+
+        if num_rejected_tokens is None or self.method != "deepseek_mtp":
+            # Shift the input ids by one token.
+            # E.g., [a1, b1, b2, c1, c2, c3] -> [b1, b2, c1, c2, c3, c3]
+            self.input_ids[:num_tokens - 1] = target_token_ids[1:]
+            # Replace the last token with the next token.
+            # E.g., [b1, b2, c1, c2, c3, c3] -> [a2, b2, b3, c2, c3, c4]
+            self.input_ids[last_token_indices] = next_token_ids
+            hidden_states_indices = last_token_indices
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            '''
+            input_ids, hidden_states_indices = self.prepare_input_ids(
+                target_token_ids=target_token_ids,
+                next_token_ids=next_token_ids,
+                token_indices=token_indices,
+                last_token_indices=last_token_indices,
+                num_rejected_tokens=num_rejected_tokens
+            )
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        # FA requires seq_len to have dtype int32.
+        seq_lens = (target_positions[last_token_indices] + 1).int()
+
+        if self.method in ["eagle", "eagle3"]:
+            # FIXME(woosuk): The below two ops cause synchronization. Optimize.
+            max_seq_len = seq_lens.max().item()
+            max_num_tokens = (cu_num_tokens[1:] -
+                                cu_num_tokens[:-1]).max().item()
+            attn_metadata = FlashAttentionMetadata(
+                num_actual_tokens=num_tokens,
+                max_query_len=max_num_tokens,
+                query_start_loc=cu_num_tokens,
+                max_seq_len=max_seq_len,
+                seq_lens=seq_lens,
+                block_table=block_table,
+                slot_mapping=target_slot_mapping,
+                # TODO(woosuk): Support cascade attention.
+                use_cascade=False,
+                common_prefix_len=0,
+                cu_prefix_query_lens=None,
+                prefix_kv_lens=None,
+                suffix_kv_lens=None,
+            )
+        elif self.method == "deepseek_mtp":
+            query_lens = cu_num_tokens[1:] - cu_num_tokens[:-1]
+            max_query_len = query_lens.max().item()
+
+            assert self.runner is not None
+
+            # FIXME: need to consider multiple kv_cache_groups
+            assert self.kv_cache_group_id is not None, (
+                "kv_cache_group_id is None")
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: 1. common_attn_metadata use main model's common_attn_metadata
+                    2. attn_metadata_builders use kv_cache_group_id
+            '''
+            attn_metadata = self.runner.attn_metadata_builders[self.kv_cache_group_id].build(
+                num_reqs=batch_size,
+                num_actual_tokens=num_tokens,
+                max_query_len=max_query_len,
+                common_prefix_len=0,
+                common_attn_metadata=main_model_common_metadata,
+            )
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+        else:
+            raise ValueError(f"Unsupported method: {self.method}")
+        # At this moment, we assume all eagle layers belong to the same KV
+        # cache group, thus using the same attention metadata.
+        per_layer_attn_metadata = {}
+        for layer_name in self.attn_layer_names:
+            per_layer_attn_metadata[layer_name] = attn_metadata
+        per_layer_attn_metadata[COMMON_METADATA_STR] = main_model_common_metadata
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: Use full graph with draft model
+        '''
+        decode_only = main_model_common_metadata.is_decode_only
+        DECODE_TOKENS_PER_REQ = 1 + self.num_speculative_tokens
+        num_input_batches = num_tokens // DECODE_TOKENS_PER_REQ
+        if self.use_cuda_graph and \
+            num_tokens <= DECODE_TOKENS_PER_REQ * self.cudagraph_batch_sizes[-1]:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: Pad inputs to run into full graph for decode stage.
+            '''
+            dp_size = self.vllm_config.parallel_config.data_parallel_size
+            if VLLM_V1_USE_FULL_GRAPH and decode_only and dp_size == 1:
+                num_input_batches = self.vllm_config.pad_for_cudagraph(num_input_batches)
+                num_input_tokens = num_input_batches * DECODE_TOKENS_PER_REQ
+            else:
+                num_input_tokens = num_tokens
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+        else:
+            num_input_tokens = num_tokens
+        captured_already = hasattr(self, "draft_graph_runners") and \
+            (num_input_tokens // DECODE_TOKENS_PER_REQ in self.draft_graph_runners)
+        use_full_graph = (VLLM_V1_USE_FULL_GRAPH and self.use_cuda_graph
+                            and decode_only and captured_already)
+        if (VLLM_V1_USE_FULL_GRAPH
+                and self.use_cuda_graph and decode_only and not use_full_graph):
+            logger.warning_once(
+                f"Select MLU-V1 Full-MLUGraph mode with drafter, however running in " +
+                f"eager mode: captured_already={captured_already}, " +
+                f"num_tokens={num_tokens}."
+            )
+
+        # copy inputs to buffer for cudagraph
+        self.positions[:num_tokens] = target_positions
+        self.hidden_states[:num_tokens] = target_hidden_states
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN and not use_full_graph:
+            start = torch.mlu.Event(enable_timing=True)
+            start.record()
+        with set_forward_context(per_layer_attn_metadata,
+                                 self.vllm_config,
+                                 num_tokens=num_input_tokens):
+            if use_full_graph:
+                model_graph = self.draft_graph_runners[num_input_batches]
+                ret_hidden_states = model_graph(
+                    input_ids=self.input_ids[:num_input_tokens],
+                    positions=self.positions[:num_input_tokens],
+                    previous_hidden_states=self.hidden_states[:num_input_tokens],
+                    intermediate_tensors=None,
+                    inputs_embeds=None,
+                )
+            else:
+                ret_hidden_states = self.model(
+                    input_ids=self.input_ids[:num_input_tokens],
+                    positions=self.positions[:num_input_tokens],
+                    previous_hidden_states=self.hidden_states[:num_input_tokens],
+                )
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                end = torch.mlu.Event(enable_timing=True)
+                end.record()
+                if use_full_graph:
+                    time_markers.append([model_graph.start, end])
+                else:
+                    time_markers.append([start, end])
+            if self.method == "deepseek_mtp":
+                last_hidden_states = ret_hidden_states
+            else:
+                last_hidden_states, hidden_states = ret_hidden_states
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        sample_hidden_states = last_hidden_states[hidden_states_indices]
+        logits = self.model.compute_logits(sample_hidden_states, None)
+        draft_token_ids = logits.argmax(dim=-1)
+
+        # Early exit if there is only one draft token to be generated.
+        if self.num_speculative_tokens == 1:
+            # [batch_size, 1]
+            return draft_token_ids.view(-1, 1)
+
+        # TODO: Currently, MTP module released by deepseek only has
+        # one layer. Adapt this code to support multiple layers once
+        # there's a multi-layer MTP module.
+
+        # Generate the remaining draft tokens.
+        draft_token_ids_list = [draft_token_ids]
+
+        positions = target_positions[last_token_indices]
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        '''
+        hidden_states = sample_hidden_states
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        if self.use_cuda_graph and \
+            batch_size <= self.cudagraph_batch_sizes[-1]:
+            input_batch_size = self.vllm_config.pad_for_cudagraph(batch_size)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: Use full graph and if decode only and no pad behavior.
+              Do not pad inputs if these conditions do not meet.
+            '''
+            if VLLM_V1_USE_FULL_GRAPH:
+                if (not decode_only or input_batch_size != batch_size):
+                    input_batch_size = batch_size
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            input_batch_size = batch_size
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: set num_decode_tokens to batch_size and max_query_len to 1
+        '''
+        attn_metadata_draft = attn_metadata
+        attn_metadata_draft.num_actual_tokens = input_batch_size
+        attn_metadata_draft.max_query_len = 1
+        attn_metadata_draft.query_start_loc = self.arange[:input_batch_size + 1]        
+        main_model_common_metadata.max_query_len = 1
+        main_model_common_metadata.num_actual_tokens = input_batch_size
+        main_model_common_metadata.num_input_tokens = input_batch_size
+        # set per_layer_attn_metadata_draft
+        per_layer_attn_metadata_draft = {}    
+        if main_model_common_metadata.infer_mode != MLUInferMode.DECODE_ONLY:
+            seq_lens = torch.ones(input_batch_size, dtype=torch.int32,)
+            cu_num_tokens = torch.cumsum(seq_lens, dim=0)
+            query_start_loc = torch.empty(input_batch_size + 1, dtype=torch.int32)
+            query_start_loc[0] = 0
+            query_start_loc[1:] = cu_num_tokens
+            seq_start_loc = self.arange[:input_batch_size + 1]
+            common_attn_metadata_k = MLUCommonAttentionMetadata.build(
+                query_start_loc=query_start_loc.to(self.runner.device),
+                seq_lens=seq_lens.to(self.runner.device),
+                seq_start_loc=seq_start_loc.to(self.runner.device),
+                is_start_loc_match=False, # not prefill
+                max_query_len=1,
+                num_actual_tokens=input_batch_size,
+                num_speculative_tokens=self.num_speculative_tokens,
+                has_prefill_reqs=main_model_common_metadata.infer_mode == MLUInferMode.CHUNKED,
+            )
+            if attn_metadata_draft.num_decodes > 0:
+                self.runner.attn_metadata_builders[self.kv_cache_group_id]._num_decode_tokens = self.runner.attn_metadata_builders[self.kv_cache_group_id]._num_decodes
+                self.runner.attn_metadata_builders[self.kv_cache_group_id].decoder_query_len = 1
+            attn_metadata_k = self.runner.attn_metadata_builders[self.kv_cache_group_id].build(
+                num_reqs=input_batch_size,
+                num_actual_tokens=input_batch_size,
+                max_query_len=1,
+                common_prefix_len=0,
+                common_attn_metadata=common_attn_metadata_k,
+            )
+            if attn_metadata_k.num_decodes > 0:
+                attn_metadata_k.decode.max_query_len = 1
+            for layer_name in self.attn_layer_names:
+                per_layer_attn_metadata_draft[layer_name] = attn_metadata_k
+            per_layer_attn_metadata_draft[COMMON_METADATA_STR] = common_attn_metadata_k
+        else:
+            attn_metadata_draft.num_decode_tokens = input_batch_size
+            attn_metadata_draft.decode.max_query_len = 1
+            # decode only use attn_metadata_draft and main_model_common_metadata for optimize
+            for layer_name in self.attn_layer_names:
+                per_layer_attn_metadata_draft[layer_name] = attn_metadata_draft
+            per_layer_attn_metadata_draft[COMMON_METADATA_STR] = main_model_common_metadata    
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+        for _ in range(self.num_speculative_tokens - 1):
+            # Update the inputs.
+            # cast to int32 is crucial when eagle model is compiled.
+            # tensor.argmax() returns int64 by default.
+            input_ids = draft_token_ids_list[-1].int()
+            positions += 1
+
+            # NOTE(woosuk): We should handle the case where the draft model
+            # generates tokens beyond the max model length. Since it is complex
+            # to remove such requests from the batch, we keep them in the batch
+            # but adjust the position ids and slot mappings to avoid the
+            # out-of-range access during the model execution. The draft tokens
+            # generated with this adjustment should be ignored.
+            exceeds_max_model_len = positions >= self.max_model_len
+            # Mask out the position ids that exceed the max model length.
+            # Otherwise, we may get out-of-range error in RoPE.
+            clamped_positions = torch.where(exceeds_max_model_len, 0,
+                                            positions)
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: deepseek_mtp do not need to update following attn_metadata
+            '''
+            if self.method != "deepseek_mtp":
+                # Increment the sequence lengths.
+                attn_metadata.max_seq_len += 1
+                attn_metadata.seq_lens += 1
+                # Consider max model length.
+                attn_metadata.max_seq_len = min(attn_metadata.max_seq_len,
+                                                self.max_model_len)
+                # For the requests that exceed the max model length, we set the
+                # sequence length to 1 to minimize their overheads in attention.
+                attn_metadata.seq_lens.masked_fill_(exceeds_max_model_len, 1)
+
+                # Compute the slot mapping.
+                block_numbers = clamped_positions // self.block_size
+                block_ids = block_table.gather(dim=1,
+                                                index=block_numbers.view(-1, 1))
+                block_ids = block_ids.view(-1)
+                attn_metadata.slot_mapping = (block_ids * self.block_size +
+                                                clamped_positions % self.block_size)
+                # Mask out the slot mappings that exceed the max model length.
+                # Otherwise, the KV cache will be inadvertently updated with the
+                # padding tokens.
+                attn_metadata.slot_mapping.masked_fill_(exceeds_max_model_len,
+                                                        PADDING_SLOT_ID)
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+
+            # copy inputs to buffer for cudagraph
+            self.input_ids[:batch_size] = input_ids
+            self.positions[:batch_size] = clamped_positions
+            self.hidden_states[:batch_size] = hidden_states
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: record latency
+            '''
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                start = torch.mlu.Event(enable_timing=True)
+                start.record()
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+            # Run the model.
+            # Note: per_layer_attn_metadata_draft is used here for draft model attention.
+            with set_forward_context(per_layer_attn_metadata_draft,
+                                        self.vllm_config,
+                                        num_tokens=input_batch_size):
+                ret_hidden_states = self.model(
+                    input_ids=self.input_ids[:input_batch_size],
+                    positions=self.positions[:input_batch_size],
+                    previous_hidden_states=self.hidden_states[:input_batch_size],
+                )
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: adapt to different methods
+                '''
+                if self.method == "deepseek_mtp":
+                    last_hidden_states = ret_hidden_states
+                else:
+                    last_hidden_states, hidden_states = ret_hidden_states
+                '''
+                =============================
+                End of MLU Hijack
+                =============================
+                '''
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: record latency
+            '''
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                end = torch.mlu.Event(enable_timing=True)
+                end.record()
+                time_markers.append([start, end])
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+            hidden_states = hidden_states[:batch_size]
+            logits = self.model.compute_logits(last_hidden_states[:batch_size],
+                                                None)
+
+            # TODO(wenlong): get more than one token for tree attention
+            draft_token_ids = logits.argmax(dim=-1)
+            draft_token_ids_list.append(draft_token_ids)
+
+        # [batch_size, num_speculative_tokens]
+        draft_token_ids = torch.stack(draft_token_ids_list, dim=1)
+        return draft_token_ids
+
+    def load_model(
+        self, target_model: nn.Module) -> None:
+        draft_model_config = \
+            self.vllm_config.speculative_config.draft_model_config
+        target_attn_layer_names = set(
+            get_layers_from_vllm_config(self.vllm_config, Attention).keys())
+
+        self.model = get_model(vllm_config=self.vllm_config,
+                                model_config=draft_model_config)
+
+        draft_attn_layer_names = (
+            get_layers_from_vllm_config(self.vllm_config, Attention).keys() -
+            target_attn_layer_names)
+
+        self.attn_layer_names = list(draft_attn_layer_names)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: only eagle and eagle3 need to share embed_tokens with the target model
+        '''
+        if self.method in ["eagle", "eagle3"]:
+            # share embed_tokens with the target model if needed
+            if get_pp_group().world_size == 1 \
+                and self.model.model.embed_tokens.weight.shape \
+                    == target_model.model.embed_tokens.weight.shape:
+                logger.info(
+                    "Assuming the EAGLE head shares the same vocab embedding" \
+                    " with the target model."
+                )
+                del self.model.model.embed_tokens
+                self.model.model.embed_tokens = target_model.model.embed_tokens
+            else:
+                logger.info(
+                    "The EAGLE head's vocab embedding will be loaded separately" \
+                    " from the target model."
+                )
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        # share lm_head with the target model if needed
+        # some model definition do not define lm_head explicitly
+        # and reuse embed_tokens for lm_head, e.g., CohereForCausalLM
+        if self.vllm_config.speculative_config.method != "eagle3" and \
+                hasattr(target_model, "lm_head"):
+            logger.info("Loading EAGLE LM head weights from the target model.")
+            if supports_multimodal(target_model):
+                self.model.lm_head = target_model.get_language_model().lm_head
+            else:
+                self.model.lm_head = target_model.lm_head
+
+    @torch.inference_mode()
+    def dummy_run(
+        self,
+        num_tokens: int,
+    ) -> None:
+        with set_forward_context(None, self.vllm_config,
+                                 num_tokens=num_tokens):
+            self.model(
+                input_ids=self.input_ids[:num_tokens],
+                positions=self.positions[:num_tokens],
+                previous_hidden_states=self.hidden_states[:num_tokens],
+            )
+
+    def validate_same_kv_cache_group(
+        self,
+        kv_cache_config: KVCacheConfig) -> None:
+        """
+        Validate that all eagle layers belong to the same KVCacheGroup.
+        Need this assumption to ensure all eagle layers can use the
+        same AttentionMetadata.
+        May extend to multiple AttentionMetadata in the future.
+        """
+        kv_cache_groups: dict[str, int] = {}
+        for id, kv_cache_group in enumerate(kv_cache_config.kv_cache_groups):
+            for layer_name in kv_cache_group.layer_names:
+                kv_cache_groups[layer_name] = id
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: get kv_cache_group_id and filter kv_cache_groups
+        '''
+        eagle_cache_groups = set(kv_cache_groups[layer_name] 
+                            for layer_name in self.attn_layer_names 
+                            if layer_name in kv_cache_groups)
+
+        assert len(eagle_cache_groups) == 1, (
+            "All eagle layers should belong to the same kv cache group")
+        self.kv_cache_group_id = next(iter(eagle_cache_groups))
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''

