diff --git a/vllm_mlu/vllm_mlu/v1/attention/backends/flash_mla.py b/vllm_mlu/vllm_mlu/v1/attention/backends/flash_mla.py
new file mode 100644
index 000000000..bd6c3bd21
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/attention/backends/flash_mla.py
@@ -0,0 +1,619 @@
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, Any, Optional, Tuple
+
+import torch
+
+from vllm.utils import cdiv, round_down
+from vllm.attention.backends.abstract import AttentionType
+from vllm.logger import init_logger
+
+from vllm_mlu.v1.attention.backends.flash_attn import (FlashAttentionBackend,
+                                                       FlashAttentionMetadata,
+                                                       FlashAttentionMetadataBuilder,
+                                                       FlashAttentionImpl)
+from vllm_mlu.v1.attention.backends.utils import MLUInferMode
+
+if TYPE_CHECKING:
+    from vllm.v1.core.sched.output import SchedulerOutput
+    from vllm.v1.worker.gpu_input_batch import InputBatch
+    from vllm_mlu.v1.worker.gpu_model_runner import MLUModelRunner
+
+logger = init_logger(__name__)
+
+
+class MLAFlashAttentionBackend(FlashAttentionBackend):
+
+    @staticmethod
+    def get_name() -> str:
+        return "MLA_FLASH_ATTN_VLLM_V1"
+
+    @staticmethod
+    def get_impl_cls() -> type["MLAFlashAttentionImpl"]:
+        return MLAFlashAttentionImpl
+
+    @staticmethod
+    def get_metadata_cls() -> type["MLAFlashAttentionMetadata"]:
+        return MLAFlashAttentionMetadata
+
+    @staticmethod
+    def get_builder_cls() -> type["MLAFlashAttentionMetadataBuilder"]:
+        return MLAFlashAttentionMetadataBuilder
+
+    @staticmethod
+    def get_kv_cache_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> tuple[int, ...]:
+        return (1, num_blocks, num_kv_heads, block_size, head_size)
+
+    @staticmethod
+    def get_kv_cache_scale_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+    ) -> tuple[int, ...]:
+        return (1, num_blocks, num_kv_heads, block_size)
+
+
+@dataclass
+class MLAFlashAttentionCommonMetadata:
+    # used for trace
+    parent_attn_metadata: FlashAttentionMetadata
+
+    # Input positions for rotrary embeddings since for MLA the rotary
+    # position embeddings are applied inside the attention backend
+    input_positions: torch.Tensor
+
+    # FlashAttentionImpl.forward used attn_metadata members
+    num_actual_tokens: int  # Number of tokens excluding padding.
+    slot_mapping: torch.Tensor
+
+    query_start_loc: torch.Tensor
+    seq_start_loc: torch.Tensor
+    seq_lens: torch.Tensor
+    max_query_len: int
+    max_seq_len: int
+    block_table: torch.Tensor
+
+    local_attn_metadata: Optional[FlashAttentionMetadata.LocalAttentionMetadata] = None
+
+    # For cascade attention.
+    use_cascade: bool = False
+
+    # For mlu infer
+    compute_type: MLUInferMode = None
+
+    @property
+    def is_prefill_only(self):
+        return self.compute_type == MLUInferMode.PREFILL_ONLY
+
+    @property
+    def is_decode_only(self):
+        return self.compute_type == MLUInferMode.DECODE_ONLY
+
+    @property
+    def is_chunked(self):
+        return self.compute_type == MLUInferMode.CHUNKED
+
+    @property
+    def is_dummy_run(self):
+        return self.parent_attn_metadata.is_dummy_run
+
+    @property
+    def compute_dtype(self):
+        return self.parent_attn_metadata.compute_dtype
+
+    @property
+    def block_tables(self):
+        return self.block_table
+
+
+@dataclass
+class MLAFlashAttentionPrefillMetadata(MLAFlashAttentionCommonMetadata):
+
+    @dataclass
+    class ChunkedContextMetadata:
+        # New for MLA (compared to FlashAttention)
+        # For handling chunked prefill
+        cu_seq_lens: torch.Tensor
+        starts: torch.Tensor
+        seq_tot: list[int]
+        max_seq_lens: list[int]
+        workspace: torch.Tensor
+
+    num_prefills: int = 0
+    chunked_context: Optional[ChunkedContextMetadata] = None
+
+
+    @property
+    def context_chunk_cu_seq_lens(self):
+        if self.chunked_context is None:
+            return None
+        return self.chunked_context.cu_seq_lens
+
+    @property
+    def context_chunk_starts(self):
+        if self.chunked_context is None:
+            return None
+        return self.chunked_context.starts
+
+    @property
+    def context_chunk_seq_tot(self):
+        if self.chunked_context is None:
+            return None
+        return self.chunked_context.seq_tot
+
+    @property
+    def context_chunk_max_seq_lens(self):
+        if self.chunked_context is None:
+            return None
+        return self.chunked_context.max_seq_lens
+
+    @property
+    def context_chunk_workspace(self):
+        if self.chunked_context is None:
+            return None
+        return self.chunked_context.workspace
+
+@dataclass
+class MLAFlashAttentionDecoderMetadata(MLAFlashAttentionCommonMetadata):
+    pass
+
+
+@dataclass
+class MLAFlashAttentionMetadata(FlashAttentionMetadata):
+    # New for MLA Chunked prefill(compared to FlashAttention)
+    # For handling prefill decode split
+    num_decodes: int = 0
+    num_decode_tokens: int = 0
+    num_prefills: int = 0
+
+    prefill_metadata: Optional[MLAFlashAttentionPrefillMetadata] = None
+    decode_metadata: Optional[MLAFlashAttentionDecoderMetadata] = None
+
+
+class MLAFlashAttentionMetadataBuilder(FlashAttentionMetadataBuilder):
+
+    def __init__(self, runner: "MLUModelRunner"):
+        super().__init__(runner=runner)
+
+        scheduler_config = runner.scheduler_config
+        model_config = runner.model_config
+        cache_config = runner.cache_config
+        if runner.speculative_config is not None:
+            self.num_speculative_tokens = runner.speculative_config.num_speculative_tokens
+        else:
+            self.num_speculative_tokens = 0
+
+        # mlu v1 mtp forces query_len = 1 + k for decoder, so we should judge decoder
+        # by query_len = 1 + k
+        self.decoder_query_len = 1 + self.num_speculative_tokens
+
+        self.page_size = self.runner.block_size
+
+        # To dummy run, no vaild reqs to deal and reorder_batch isn't called
+        self._num_decodes = 0
+        self._num_prefills = 0
+        self._num_decode_tokens = 0
+        self._num_prefill_tokens = 0
+
+        if self.chunked_prefill_enabled:
+            # from vllm_mlu.v1.engine.core import EngineCore_MluHiack, the EngineCore_MluHiack's
+            # __init__ func has the logic that when mlu_envs.VLLM_V1_USE_UNCHUNK_SCHED and
+            # not enable_chunked_prefill, use V1UnchunkScheduler schedule type. So chunked prefill
+            # use V1Scheduler, here should assert the condition
+            # assert not mlu_envs.VLLM_V1_USE_UNCHUNK_SCHED, (
+            #     "mla chunked prefill only supports V1 Scheduler, not V1UnchunkScheduler, "
+            #     "please export VLLM_V1_USE_UNCHUNK_SCHED=0 or disable chunked prefill."
+            # )
+            self.chunked_prefill_workspace_size = min(
+                # Max sure there is enough for 1 full length request or at least
+                # 4 pages of cache per request
+                max(
+                    1 * model_config.max_model_len, 4 *
+                    scheduler_config.max_num_seqs * cache_config.block_size),
+                # For long-context models try not to over-allocate limiting
+                # kv-cache space, limiting it to 64k tokens,
+                # which would result in the workspace being:
+                #   2*(576)*(64*1024) = 144mb
+                # (assuming 576 MLA head dim, and fp16)
+                # which would result in up-projected context being
+                #   2*(192*128)*(64*1024) = 3gb
+                # (assuming 192 QK head dim, 128 heads, and fp16)
+                128 * 1024)
+            assert self.chunked_prefill_workspace_size >= \
+                scheduler_config.max_num_seqs * cache_config.block_size
+            self.chunked_prefill_workspace = torch.empty(
+                (self.chunked_prefill_workspace_size,
+                 model_config.get_head_size()),
+                dtype=model_config.dtype,
+                device=runner.device,
+            )
+
+    def reorder_batch(self, input_batch: "InputBatch",
+                      scheduler_output: "SchedulerOutput") -> bool:
+        '''
+        Same as vllm/v1/attention/backends/mla/common.py:MLACommonMetadataBuilder:reorder_batch
+        '''
+        # If chunked prefill is disabled, Call pararent FlashAttentionMetadataBuilder
+        if not self.chunked_prefill_enabled:
+            return super().reorder_batch(input_batch=input_batch, scheduler_output=scheduler_output)
+
+        # We now want to reorder the batch so that the "decode" requests are at
+        # the front and the "prefill" requests are at the using the least amount
+        # swaps possible. (NOTE for now we loosely use "decode" to mean requests
+        # where attention is likely memory-bound and "prefill" to mean requests
+        # where attention is likely compute-bound, TODO(lucas): figure out a
+        # better naming here)
+        decodes = []
+        prefills = []
+        num_decode_tokens = 0
+        num_prefill_tokens = 0
+
+        for i, req_id in enumerate(input_batch.req_ids):
+            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
+            # for now treat 1 scheduled token as "decode" even if its not,
+            # we should update this to something like < 8 in the future but
+            # currently the TritonMLA._forward_decode only supports
+            # num_tokens = 1
+            # for vllm mlu, judge decoder by num_tolens <= self.decoder_query_len
+            if num_tokens <= self.decoder_query_len:
+                decodes.append(i)
+                num_decode_tokens += num_tokens
+            else:
+                prefills.append(i)
+                num_prefill_tokens += num_tokens
+
+        # We hope that this is fairly minimal since decodes
+        # should be around for a number of iterations so hopefully they are
+        # relatively stationary (and new request are generally appended to the
+        # persistent batch so already should be at the back)
+        # To achieve this we loop over the decodes in descending order and
+        # the prefills in ascending order. We swap decodes from the  "back"
+        # i.e. past where the last decode should be in the reodorered with
+        # prefills from the front of the batch.
+        # `decodes` and `prefills` are already in ascending order just based on
+        # the above loop
+        num_decodes = len(decodes)
+        num_prefills = len(prefills)
+        modified_batch = False
+
+        for i in range(1, min(num_decodes, num_prefills) + 1):
+            # If the decode is at the "back" of the batch, i, we can swap it
+            # with the prefill closest to the front of the batch
+            decode_idx = decodes[num_decodes - i]
+            if decode_idx < num_decodes:
+                break
+
+            input_batch.swap_states(prefills[i - 1], decode_idx)
+            modified_batch = True
+
+        # Save for next `build` call
+        # TODO(lucas): this is a bit of a hack, we should probably have a
+        # better way of doing this
+        self._num_decodes = num_decodes
+        self._num_prefills = num_prefills
+        self._num_decode_tokens = num_decode_tokens
+        self._num_prefill_tokens = num_prefill_tokens
+
+        return modified_batch
+
+    def build_for_chunked_prefill(
+        self,
+        parent_attn_metadata: FlashAttentionMetadata,
+        num_reqs: int,
+        num_actual_tokens: int,
+        max_query_len: int,
+        num_decodes: int,
+        num_prefills: int,
+        num_decode_tokens: int,
+        num_prefill_tokens: int,
+    ) -> Tuple[MLAFlashAttentionPrefillMetadata, MLAFlashAttentionDecoderMetadata]:
+        '''
+        Same as vllm/v1/attention/backends/mla/common.py:MLACommonMetadataBuilder:build
+        '''
+        # Note(simon): be careful about the CPU <> GPU memory movement in this
+        # function. We should avoid GPU -> CPU sync as much as possible because
+        # it blocks on all previous kernels.
+        device = self.runner.device
+        block_table = (
+            self.runner.input_batch.block_table.get_device_tensor()[:num_reqs])
+        query_start_loc = self.runner.query_start_loc_cpu[:num_reqs + 1].to(
+            device, non_blocking=True)
+        slot_mapping = self.runner.slot_mapping_cpu[:num_actual_tokens].to(
+            device, non_blocking=True)
+        input_positions = self.runner.positions_cpu[:num_actual_tokens].to(
+            device, non_blocking=True)
+
+        seq_lens_cpu = self.runner.seq_lens_cpu[:num_reqs]
+        seq_lens = seq_lens_cpu.to(device, non_blocking=True)
+
+        prefill_metadata = None
+        if num_prefills > 0:
+            reqs_start = num_decodes  # prefill_start
+            tokens_start = num_decode_tokens
+
+            context_lens_cpu = self.runner.input_batch.\
+                num_computed_tokens_cpu_tensor[reqs_start:num_reqs]
+            max_context_len_cpu = context_lens_cpu.max().item()
+            num_prefills_with_context_cpu = (context_lens_cpu > 0).sum().item()
+            prefill_query_start_loc = query_start_loc[
+                reqs_start:] - query_start_loc[reqs_start]
+            prefill_seq_start_loc = parent_attn_metadata.seq_start_loc[
+                reqs_start:] - parent_attn_metadata.seq_start_loc[reqs_start]
+
+            chunked_context_metadata = None
+            if self.chunked_prefill_enabled and num_prefills > 0 \
+                and max_context_len_cpu > 0:
+                # NOTE: it is recommend you read the `Chunked Prefill` section
+                # in the comment at the top of the file before trying to
+                # understand the following code
+
+                # currently we allocate an equal amount of workspace for each
+                # prefill in the batch, we could probably use a more advanced
+                # algorithm here and allocate more workspace to prefills with
+                # longer context lengths
+                max_context_chunk = (self.chunked_prefill_workspace_size //
+                                     num_prefills_with_context_cpu)
+
+                # align max_context_chunk to page_size by rounding down,
+                # currently the `gather_cache` kernel cannot handle
+                # `context_chunk_starts` that are not aligned to page_size
+                max_context_chunk = round_down(max_context_chunk,
+                                               self.page_size)
+
+                assert max_context_chunk > 0
+                num_chunks = cdiv(max_context_len_cpu, max_context_chunk)
+                logger.debug(f"{num_chunks=}, {num_prefills_with_context_cpu=}, \
+                    {max_context_len_cpu=}, {num_prefills=}, {num_decodes=}, \
+                    {self.chunked_prefill_workspace_size=}")
+
+                # if `max_context_chunk = 256`, `num_chunks = 3`, and
+                #   `num_prefills_with_context = 4`, create a tensor that looks
+                # like
+                #  [[0, 0, 0, 0], [256, 256, 256, 256], [512, 512, 512, 512]]
+                # Note(simon): this is done in CPU because of downstream's
+                # of `to_list`.
+                chunk_starts = \
+                    torch.arange(num_chunks, dtype=torch.int32) \
+                    .unsqueeze(1).expand(-1, num_prefills) \
+                    * max_context_chunk
+                chunk_ends = torch.min(context_lens_cpu.unsqueeze(0),
+                                       chunk_starts + max_context_chunk)
+                chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)
+
+                cu_seq_lens_cpu = torch.zeros(num_chunks,
+                                              num_prefills + 1,
+                                              dtype=torch.int32,
+                                              pin_memory=True)
+                torch.cumsum(chunk_seq_lens,
+                             dim=1,
+                             out=cu_seq_lens_cpu[:, 1:],
+                             dtype=torch.int32)
+
+                chunked_context_metadata = \
+                    MLAFlashAttentionPrefillMetadata.ChunkedContextMetadata(
+                    cu_seq_lens=cu_seq_lens_cpu.to(device, non_blocking=True),
+                    starts=chunk_starts.to(device, non_blocking=True),
+                    seq_tot=chunk_seq_lens.sum(dim=1).tolist(),
+                    max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),
+                    workspace=self.chunked_prefill_workspace,
+                )
+
+                assert max(chunked_context_metadata.max_seq_lens) <= \
+                    self.chunked_prefill_workspace_size
+
+            prefill_metadata = MLAFlashAttentionPrefillMetadata(
+                parent_attn_metadata = parent_attn_metadata,
+                input_positions=input_positions[tokens_start:],
+                num_actual_tokens=num_prefill_tokens,
+                slot_mapping=slot_mapping[tokens_start:],
+                query_start_loc=prefill_query_start_loc,
+                seq_start_loc=prefill_seq_start_loc,
+                seq_lens=seq_lens[num_decodes:],
+                max_query_len=max_query_len,
+                max_seq_len=seq_lens_cpu[reqs_start:].max(),
+                block_table=block_table[reqs_start:, ...],
+                compute_type=MLUInferMode.PREFILL_ONLY,
+                num_prefills=num_prefills,
+                chunked_context=chunked_context_metadata,
+            )
+
+        decode_metadata = None
+        if num_decodes > 0:
+            decode_metadata = MLAFlashAttentionDecoderMetadata(
+                parent_attn_metadata = parent_attn_metadata,
+                input_positions=input_positions[:num_decode_tokens],
+                num_actual_tokens=num_decode_tokens,
+                slot_mapping=slot_mapping[:num_decode_tokens],
+                query_start_loc=query_start_loc[:num_decodes + 1],
+                seq_start_loc=parent_attn_metadata.seq_start_loc[:num_decodes + 1],
+                seq_lens=seq_lens[:num_decodes],
+                max_query_len=self.decoder_query_len,
+                max_seq_len=seq_lens_cpu[:num_decodes].max(),
+                block_table=block_table[:num_decodes, ...],
+                compute_type=MLUInferMode.DECODE_ONLY,
+            )
+        return prefill_metadata, decode_metadata
+
+    def build(self,
+              num_reqs: int,
+              num_actual_tokens: int,
+              max_query_len: int,
+              common_prefix_len: int,
+              num_speculative_tokens: int = 0,
+              is_dummy_run: bool = False) -> MLAFlashAttentionMetadata:
+        parent_attn_metadata = super().build(num_reqs=num_reqs,
+                                             num_actual_tokens=num_actual_tokens,
+                                             max_query_len=max_query_len,
+                                             common_prefix_len=common_prefix_len,
+                                             num_speculative_tokens=num_speculative_tokens,
+                                             is_dummy_run=is_dummy_run)
+
+        prefill_metadata = None
+        decode_metadata = None
+
+        if not parent_attn_metadata.chunked_prefill_enabled or parent_attn_metadata.is_dummy_run:
+            chunked_prefill_enabled = False
+            num_decodes = 0
+            num_prefills = 0
+            num_decode_tokens = 0
+            num_prefill_tokens = 0
+        else:
+            chunked_prefill_enabled = parent_attn_metadata.chunked_prefill_enabled
+            num_decodes = self._num_decodes
+            num_prefills = self._num_prefills
+            num_decode_tokens = self._num_decode_tokens
+            num_prefill_tokens = self._num_prefill_tokens
+
+        if chunked_prefill_enabled:
+            assert num_decodes + num_prefills == num_reqs
+            prefill_metadata, decode_metadata = self.build_for_chunked_prefill(
+                parent_attn_metadata=parent_attn_metadata,
+                num_reqs=num_reqs,
+                num_actual_tokens=num_actual_tokens,
+                max_query_len=max_query_len,
+                num_decodes=num_decodes,
+                num_prefills=num_prefills,
+                num_decode_tokens=num_decode_tokens,
+                num_prefill_tokens=num_prefill_tokens)
+
+        return MLAFlashAttentionMetadata(
+            # FlashAttentionMetadata specfic
+            num_actual_tokens=parent_attn_metadata.num_actual_tokens,
+            max_query_len=parent_attn_metadata.max_query_len,
+            query_start_loc=parent_attn_metadata.query_start_loc,
+            seq_start_loc=parent_attn_metadata.seq_start_loc,
+            max_seq_len=parent_attn_metadata.max_seq_len,
+            seq_lens=parent_attn_metadata.seq_lens,
+            block_table=parent_attn_metadata.block_table,
+            slot_mapping=parent_attn_metadata.slot_mapping,
+            # For cascade attention
+            use_cascade=parent_attn_metadata.use_cascade,
+            common_prefix_len=parent_attn_metadata.common_prefix_len,
+            cu_prefix_query_lens=parent_attn_metadata.cu_prefix_query_lens,
+            prefix_kv_lens=parent_attn_metadata.prefix_kv_lens,
+            suffix_kv_lens=parent_attn_metadata.suffix_kv_lens,
+            # For logging.
+            num_input_tokens=parent_attn_metadata.num_input_tokens,
+            # For mlu infer
+            is_dummy_run=parent_attn_metadata.is_dummy_run,
+            compute_type=parent_attn_metadata.compute_type,
+            compute_dtype=parent_attn_metadata.compute_dtype,
+            chunked_prefill_enabled=chunked_prefill_enabled,
+            # for local attention
+            local_attn_metadata=parent_attn_metadata.local_attn_metadata,
+            # MLACommonMetadata Chunk prefill specific
+            num_decodes=num_decodes,
+            num_decode_tokens=num_decode_tokens,
+            num_prefills=num_prefills,
+            prefill_metadata=prefill_metadata,
+            decode_metadata=decode_metadata,
+        )
+
+    def use_cascade_attention(self, *args, **kwargs) -> bool:
+        return False
+
+class MLAFlashAttentionImpl(FlashAttentionImpl):
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: int,
+        alibi_slopes: Optional[list[float]],
+        sliding_window: Optional[int],
+        kv_cache_dtype: str,
+        blocksparse_params: Optional[dict[str, Any]] = None,
+        logits_soft_cap: Optional[float] = None,
+        attn_type: AttentionType = AttentionType.DECODER,
+        use_irope: bool = False,
+        **extra_impl_args,
+    ) -> None:
+        super().__init__(num_heads=num_heads,
+                         head_size=head_size,
+                         scale=scale,
+                         num_kv_heads=num_kv_heads,
+                         alibi_slopes=alibi_slopes,
+                         sliding_window=sliding_window,
+                         kv_cache_dtype=kv_cache_dtype,
+                         blocksparse_params=blocksparse_params,
+                         logits_soft_cap=logits_soft_cap,
+                         attn_type=attn_type,
+                         use_irope=use_irope,
+                         **extra_impl_args)
+
+
+    def forward(
+        self,
+        layer: torch.nn.Module,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: MLAFlashAttentionMetadata,
+        output: Optional[torch.Tensor] = None,
+        kwargs: Optional[dict[str, Any]] = {},
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+        """Forward pass with FlashAttention.
+
+        Args:
+            query: shape = [num_tokens, num_heads, head_size]
+            key: shape = [num_tokens, num_kv_heads, head_size]
+            value: shape = [num_tokens, num_kv_heads, head_size]
+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+            attn_metadata: Metadata for attention.
+        Returns:
+            shape = [num_tokens, num_heads * head_size]
+        NOTE: FP8 quantization, flash-attn expect the size of
+              {q,k,v}_descale to be (num_sequences, num_kv_heads).
+              We use torch's .expand() to avoid duplicating values
+        """
+
+        if not attn_metadata.chunked_prefill_enabled:
+            return super().forward(layer=layer,
+                                   query=query,
+                                   key=key,
+                                   value=value,
+                                   kv_cache=kv_cache,
+                                   attn_metadata=attn_metadata,
+                                   output=output,
+                                   kwargs=kwargs)
+
+        assert output is not None, "Output tensor must be provided."
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: for mlu v1, attn_metadata is always required by global rope,
+                use is_dummy_run to distinguish dummy and real inference.
+        '''
+        if attn_metadata.is_dummy_run:
+            # Profiling run.
+            return output
+        '''
+        ==================
+        End of MLU Hijack
+        =====
+        '''
+        only_prefill = kwargs.get("only_prefill", False)
+        only_decode = kwargs.get("only_decode", False)
+        if only_prefill:
+            metadata = attn_metadata.prefill_metadata
+        elif only_decode:
+            metadata = attn_metadata.decode_metadata
+        else:
+            metadata = attn_metadata
+
+        return super().forward(layer=layer,
+                               query=query,
+                               key=key,
+                               value=value,
+                               kv_cache=kv_cache,
+                               attn_metadata=metadata,
+                               output=output,
+                               kwargs=kwargs)

