diff --git a/vllm_mlu/vllm_mlu/v1/worker/dp_gpu_model_runner.py b/vllm_mlu/vllm_mlu/v1/worker/dp_gpu_model_runner.py
new file mode 100644
index 000000000..d9917ba27
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/worker/dp_gpu_model_runner.py
@@ -0,0 +1,713 @@
+from typing import TYPE_CHECKING, Dict, Optional, List, Union, Any
+
+import torch
+import numpy as np
+
+from vllm.distributed.parallel_state import (
+    get_tp_group, get_pp_group)
+from vllm.distributed.kv_transfer import (
+    get_kv_transfer_group, has_kv_transfer_group)
+from vllm.distributed import (
+    divide, get_moe_expert_parallel_world_size
+)
+from vllm.config import VllmConfig, LoadFormat
+from vllm.forward_context import set_forward_context
+from vllm.logger import init_logger
+from vllm.sequence import IntermediateTensors
+from vllm.utils import LazyLoader, async_tensor_h2d, get_dtype_size
+from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, ModelRunnerOutput)
+from vllm.v1.spec_decode.eagle import EagleProposer
+from vllm.v1.spec_decode.ngram_proposer import NgramProposer
+from vllm.v1.spec_decode.medusa import MedusaProposer
+
+if TYPE_CHECKING:
+    import xgrammar as xgr
+    from vllm.v1.core.sched.output import SchedulerOutput
+else:
+    xgr = LazyLoader("xgr", globals(), "xgrammar")
+
+from vllm_mlu.v1.attention.backends.flash_attn import pad_attn_metadata
+from vllm_mlu.v1.attention.backends.utils import (
+    MLUCommonAttentionMetadata, get_common_metadata_from_attn_metadata)
+from vllm_mlu.v1.worker.gpu_model_runner import MLUModelRunner
+from vllm_mlu.mlu_forward_context import MLUDPMetadata
+from vllm_mlu.model_executor.models.dp_utils import (
+    enable_emb_logits_custom_parallel,
+    get_runtime_infos_per_dp_group,
+    get_deepseek_layer_split_list,
+)
+from vllm_mlu._mlu_utils import (
+    VLLM_V1_USE_FULL_GRAPH, VLLM_LATENCY_DEBUG_WITH_DEVICE_EN)
+from vllm_mlu.model_executor.models.deepseek_v2 import MLUDeepseekV2MoE
+from vllm_mlu.distributed.parallel_state import (
+    init_cnclep, get_cnclep
+)
+
+logger = init_logger(__name__)
+
+
+class DPMLUModelRunner(MLUModelRunner):
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        device: torch.device,
+    ):
+        vllm_config.mlu_config.enable_custom_data_parallel_opt = True
+        super().__init__(vllm_config, device)
+        if self.use_cuda_graph:
+            assert VLLM_V1_USE_FULL_GRAPH, (
+                "DPMLUModelRunner can only run with full graph"
+            )
+        self.use_all2all = self.mlu_config.decode_dispatch_combine_use_all2all
+        if self.use_all2all:
+            assert get_moe_expert_parallel_world_size() > 1, (
+                "all2all requires that expert parallel is enabled")
+            kwargs = self.make_cnclep_kwargs()
+            init_cnclep(**kwargs)
+
+    def _get_data_parallel_metadata(
+            self,
+            num_tokens: int,
+            num_reqs: int,
+            is_decode_only: bool,
+            query_len_per_batch: Optional[List[int]],
+    ) -> "MLUDPMetadata":
+        (dp_query_lens, dp_group_bs, dp_is_prefill,
+         seq_len_per_batch) = get_runtime_infos_per_dp_group(
+            num_tokens,
+            num_reqs,
+            not is_decode_only,
+            query_len_per_batch,
+            self.device,
+            self.vllm_config,
+        )
+        (emb_query_lens, logits_batch_sizes,
+         dense_attn_token_split_list) = get_deepseek_layer_split_list(
+            dp_query_lens,
+            dp_group_bs,
+        )
+        return MLUDPMetadata.make_oot(
+            self.parallel_config.data_parallel_rank,
+            self.parallel_config.data_parallel_size,
+            self.parallel_config.tensor_parallel_size,
+            dp_query_lens,
+            dp_is_prefill,
+            self.vllm_config.mlu_config.prefill_dispatch_use_RS_AG,
+            seq_lens=(seq_len_per_batch if all(dp_is_prefill) else None),
+            batch_sizes=dp_group_bs,
+            emb_query_lens=emb_query_lens,
+            logits_batch_sizes=logits_batch_sizes,
+            dense_attn_token_split_list=dense_attn_token_split_list,
+        )
+
+    def _get_dp_graph_info(self,
+                           K: int,
+                           num_scheduled_tokens: int,
+                           dp_metadata: "MLUDPMetadata"):
+        """
+        Check if the DeepSeek model can enter graph mode and retrieve input
+        tokens and batch.
+
+        This function also applies to other eligible MoE models with DP enabled,
+        reusing the same graph mode compatibility logic.
+
+        Returns:
+            tuple: Contains three elements:
+                num_input_tokens: Retrieved input token
+                num_input_batchs: Retrieved input batch
+                use_graph: Whether the model can use graph mode
+        """
+        if (self.use_cuda_graph
+            and all(not prefill for prefill in dp_metadata.dp_is_prefill)
+            and all(token_num <= self.cudagraph_batch_sizes[-1] * (1 + K)
+                    for token_num in dp_metadata.token_split_list)):
+            num_input_batchs = self.vllm_config.pad_for_cudagraph(
+                    max(dp_metadata.token_split_list) // (1 + K))
+            num_input_tokens = num_input_batchs * (1 + K)
+            use_graph = True
+        else:
+            num_input_batchs = self.input_batch.num_reqs
+            num_input_tokens = num_scheduled_tokens
+            use_graph = False
+        return num_input_tokens, num_input_batchs, use_graph
+
+    @torch.inference_mode()
+    def moe_dp_execute_dummy_batch(
+        self, num_tokens: int, skip_attn: bool = True,
+    ) -> torch.Tensor:
+        max_num_reqs = self.scheduler_config.max_num_seqs
+        num_reqs = min(num_tokens, max_num_reqs)
+        min_tokens_per_req = num_tokens // num_reqs
+        num_scheduled_tokens_list = [min_tokens_per_req] * num_reqs
+        num_scheduled_tokens_list[-1] += num_tokens % num_reqs
+        assert sum(num_scheduled_tokens_list) == num_tokens
+        assert len(num_scheduled_tokens_list) == num_reqs
+        num_scheduled_tokens = np.array(num_scheduled_tokens_list,
+                                        dtype=np.int32)
+
+        # MUST do comm across dp group first when enable data parallel.
+        # Here we set dummy run state as prefill only to prevent other dp
+        # group use graph.
+        dp_metadata = self._get_data_parallel_metadata(
+            num_tokens, num_reqs, False, [num_tokens // num_reqs] * num_reqs
+        )
+
+        # always skip attn compute
+        attn_metadata: Optional[Dict[str, Any]] = None
+
+        input_ids = self.input_ids[:num_tokens]
+        with self.maybe_randomize_inputs(input_ids), set_forward_context(
+                attn_metadata,
+                self.vllm_config,
+                num_tokens=num_tokens):
+            hidden_states = self.model(
+                input_ids=input_ids,
+                positions=self.positions[:num_tokens],
+                intermediate_tensors=None,
+                inputs_embeds=None,
+                dp_params=dp_metadata,
+            )
+
+        kwargs = ({"dp_params": dp_metadata}
+                  if enable_emb_logits_custom_parallel() else {})
+        self.model.compute_logits(
+            hidden_states[:num_tokens], None, **kwargs)
+
+        if self.speculative_config and self.speculative_config.use_eagle():
+            assert isinstance(self.drafter, EagleProposer)
+            target_token_ids = self.input_ids[:num_tokens]
+            target_positions = self.positions[:num_tokens]
+            # hidden_states no need to be sliced
+            target_hidden_states = hidden_states
+            self.drafter.propose_ds_execute_dummy_batch(
+                target_token_ids=target_token_ids,
+                target_positions=target_positions,
+                target_hidden_states=target_hidden_states,
+                dp_params=dp_metadata)
+        logit_indices = np.cumsum(num_scheduled_tokens) - 1
+        return hidden_states[logit_indices]
+
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        scheduler_output: "SchedulerOutput",
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+    ) -> Union[ModelRunnerOutput, IntermediateTensors]:
+
+        self._update_states(scheduler_output)
+        if not scheduler_output.total_num_scheduled_tokens:
+            if not has_kv_transfer_group():
+                # Return empty ModelRunnerOutput if there's no work to do.
+                return EMPTY_MODEL_RUNNER_OUTPUT
+
+            return self.kv_connector_no_forward(scheduler_output)
+
+        # Prepare the decoder inputs.
+        attn_metadata, logits_indices, spec_decode_metadata = (
+            self._prepare_inputs(scheduler_output))
+        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+        common_metadata: MLUCommonAttentionMetadata = \
+            get_common_metadata_from_attn_metadata(attn_metadata)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: get dp_metadata and graph infos for moe data parallel
+        '''
+        # all layers share the same attn_metadata
+        assert len(self.kv_cache_config.kv_cache_groups) == 1
+        attn_metadata_val = next(iter(attn_metadata.values()))
+
+        K = self.drafter.num_speculative_tokens if hasattr(self, "drafter") else 0
+        cur_num_reqs = (
+            self.input_batch.num_reqs if common_metadata.is_prefill_only
+            else self.input_batch.num_reqs * (1 + K)
+        )
+        query_len_per_batch = None
+        if self.mlu_config.is_dpsk_mcc_enabled:
+            # avoid d2h here when not mcc
+            query_len_per_batch = (
+                attn_metadata_val.query_start_loc[1:] -
+                attn_metadata_val.query_start_loc[:-1]
+            ).tolist()
+        dp_metadata = self._get_data_parallel_metadata(
+            num_scheduled_tokens, cur_num_reqs,
+            common_metadata.is_decode_only, query_len_per_batch
+        )
+
+        dp_can_use_graph = False
+        if self.use_cuda_graph:
+            num_input_tokens, num_paded_reqs, dp_can_use_graph = (
+                self._get_dp_graph_info(K, num_scheduled_tokens, dp_metadata)
+            )
+            if dp_can_use_graph:
+                block_table = self.input_batch.block_table[0]
+                pad_attn_metadata(
+                    attn_metadata_val, common_metadata, block_table, self,
+                    num_scheduled_tokens, num_input_tokens,
+                    self.input_batch.num_reqs, num_paded_reqs)
+        else:
+            # Eager mode.
+            # Pad tokens to multiple of tensor_parallel_size when
+            # enabled collective fusion for SP
+            tp_size = self.vllm_config.parallel_config.tensor_parallel_size
+            if self.vllm_config.compilation_config.pass_config. \
+                enable_sequence_parallelism and tp_size > 1:
+                from vllm.utils import round_up
+                num_input_tokens = round_up(num_scheduled_tokens, tp_size)
+            else:
+                num_input_tokens = num_scheduled_tokens
+
+        # Padding for DP
+        # num_pad, num_tokens_across_dp = self.get_dp_padding(num_input_tokens)
+        # num_input_tokens += num_pad
+        num_pad, num_tokens_across_dp = 0, None
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        # _prepare_inputs may reorder the batch, so we must gather multi
+        # modal outputs after that to ensure the correct order
+        if self.is_multimodal_model:
+            # Run the multimodal encoder if any.
+            self._execute_mm_encoder(scheduler_output)
+            mm_embeds = self._gather_mm_embeddings(scheduler_output)
+        else:
+            mm_embeds = []
+
+        if self.is_multimodal_model and get_pp_group().is_first_rank:
+            # NOTE(woosuk): To unify token ids and soft tokens (vision
+            # embeddings), we always use embeddings (rather than token ids)
+            # as input to the multimodal model, even when the input is text.
+            input_ids = self.input_ids[:num_scheduled_tokens]
+            if mm_embeds:
+                inputs_embeds = self.model.get_input_embeddings(
+                    input_ids, mm_embeds)
+            else:
+                inputs_embeds = self.model.get_input_embeddings(input_ids)
+            # TODO(woosuk): Avoid the copy. Optimize.
+            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)
+            inputs_embeds = self.inputs_embeds[:num_input_tokens]
+            input_ids = None
+        else:
+            # For text-only models, we use token ids as input.
+            # While it is possible to use embeddings as input just like the
+            # multimodal models, it is not desirable for performance since
+            # then the embedding layer is not included in the CUDA graph.
+            input_ids = self.input_ids[:num_input_tokens]
+            inputs_embeds = None
+        if self.uses_mrope:
+            positions = self.mrope_positions[:, :num_input_tokens]
+        else:
+            positions = self.positions[:num_input_tokens]
+
+        if get_pp_group().is_first_rank:
+            intermediate_tensors = None
+        else:
+            intermediate_tensors = self.sync_and_slice_intermediate_tensors(
+                num_input_tokens, intermediate_tensors, True)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: v1 support full graph, offline benchmark
+        @brief: add dp_params when it is not None for eager mode
+        '''
+        use_full_graph = VLLM_V1_USE_FULL_GRAPH and dp_can_use_graph
+
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN and not use_full_graph:
+            start = torch.mlu.Event(enable_timing=True)
+            start.record()
+
+        # Run the decoder.
+        # Use persistent buffers for CUDA graphs.
+        with set_forward_context(attn_metadata,
+                                 self.vllm_config,
+                                 num_tokens=num_input_tokens,
+                                 num_tokens_across_dp=num_tokens_across_dp):
+            self.maybe_setup_kv_connector(scheduler_output)
+
+            if use_full_graph:
+                # Here we use batch size as index.
+                batch_size = num_input_tokens
+                if hasattr(self, "drafter"):
+                    assert num_input_tokens % (1 + self.drafter.num_speculative_tokens) == 0, \
+                        "num_input_tokens should be multiple of (1 + num_speculative_tokens)."
+                    batch_size = num_input_tokens // (1 + self.drafter.num_speculative_tokens)
+                model_graph = self.graph_runners[batch_size]
+                model_output = model_graph(
+                    input_ids=input_ids,
+                    positions=positions,
+                    intermediate_tensors=intermediate_tensors,
+                    inputs_embeds=inputs_embeds,
+                )
+            else:
+                model_output = self.model(
+                    input_ids=input_ids,
+                    positions=positions,
+                    intermediate_tensors=intermediate_tensors,
+                    inputs_embeds=inputs_embeds,
+                    dp_params=dp_metadata,
+                )
+
+            self.maybe_wait_for_kv_save()
+            finished_sending, finished_recving = (
+                self.get_finished_kv_transfers(scheduler_output))
+
+        if self.use_aux_hidden_state_outputs:
+            hidden_states, aux_hidden_states = model_output
+        else:
+            hidden_states = model_output
+        # Broadcast PP output for external_launcher (torchrun)
+        # to make sure we are synced across pp ranks
+        # TODO: Support overlapping mirco-batches
+        # https://github.com/vllm-project/vllm/issues/18019
+        broadcast_pp_output = \
+            self.parallel_config.distributed_executor_backend \
+            == "external_launcher" and len(get_pp_group().ranks) > 0
+        if not get_pp_group().is_last_rank:
+            # For mid-pipeline stages, return the hidden states.
+            if not broadcast_pp_output:
+                return hidden_states
+            assert isinstance(hidden_states, IntermediateTensors)
+            get_pp_group().send_tensor_dict(hidden_states.tensors,
+                                            all_gather_group=get_tp_group())
+            logits = None
+        else:
+            sample_hidden_states = hidden_states[logits_indices]
+            kwargs = ({"dp_params": dp_metadata}
+                      if enable_emb_logits_custom_parallel() else {})
+            logits = self.model.compute_logits(sample_hidden_states, None, **kwargs)
+        if broadcast_pp_output:
+            model_output_broadcast_data = {
+                "logits": logits.contiguous(),
+            } if logits is not None else {}
+            model_output_broadcast_data = get_pp_group().broadcast_tensor_dict(
+                model_output_broadcast_data, src=len(get_pp_group().ranks) - 1)
+            assert model_output_broadcast_data is not None
+            logits = model_output_broadcast_data["logits"]
+
+        self.time_markers = []
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+            end = torch.mlu.Event(enable_timing=True)
+            end.record()
+            if use_full_graph:
+                self.time_markers.append([model_graph.start, end])
+            else:
+                self.time_markers.append([start, end])
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        # Apply structured output bitmasks if present
+        if scheduler_output.grammar_bitmask is not None:
+            self.apply_grammar_bitmask(scheduler_output, logits)
+
+        # Sample the next token and get logprobs if needed.
+        sampling_metadata = self.input_batch.sampling_metadata
+        if spec_decode_metadata is None:
+            sampler_output = self.sampler(
+                logits=logits,
+                sampling_metadata=sampling_metadata,
+            )
+        else:
+            # When indexing with a tensor (bonus_logits_indices), PyTorch
+            # creates a new tensor with separate storage from the original
+            # logits tensor. This means any in-place operations on bonus_logits
+            # won't affect the original logits tensor.
+            assert logits is not None
+            bonus_logits = logits[spec_decode_metadata.bonus_logits_indices]
+            sampler_output = self.sampler(
+                logits=bonus_logits,
+                sampling_metadata=sampling_metadata,
+            )
+            bonus_token_ids = sampler_output.sampled_token_ids
+
+            # Just like `bonus_logits`, `target_logits` is a new tensor with
+            # separate storage from the original `logits` tensor. Therefore,
+            # it is safe to update `target_logits` in place.
+            target_logits = logits[spec_decode_metadata.target_logits_indices]
+            output_token_ids = self.rejection_sampler(
+                spec_decode_metadata,
+                None,  # draft_probs
+                target_logits,
+                bonus_token_ids,
+                sampling_metadata,
+            )
+            sampler_output.sampled_token_ids = output_token_ids
+
+        # TODO(woosuk): The following loop can be slow since it iterates over
+        # the requests one by one. Optimize.
+        discard_sampled_tokens_req_indices = []
+        for i, req_id in enumerate(self.input_batch.req_ids):
+            req_state = self.requests[req_id]
+            seq_len = (req_state.num_computed_tokens +
+                       scheduler_output.num_scheduled_tokens[req_id])
+            if seq_len < req_state.num_tokens:
+                # Ignore the sampled token for partial prefills.
+                # Rewind the generator state as if the token was not sampled.
+                # This relies on cuda-specific torch-internal impl details
+                generator = self.input_batch.generators.get(i)
+                if generator is not None:
+                    generator.set_offset(generator.get_offset() - 4)
+                # Record the index of the request that should not be sampled,
+                # so that we could clear the sampled tokens before returning.
+                discard_sampled_tokens_req_indices.append(i)
+
+        # NOTE: GPU -> CPU Sync happens here.
+        # Move as many CPU operations as possible before this sync point.
+        logprobs_tensors = sampler_output.logprobs_tensors
+        logprobs_lists = logprobs_tensors.tolists() \
+            if logprobs_tensors is not None else None
+
+        # Compute prompt logprobs if needed.
+        prompt_logprobs_dict = self._get_prompt_logprobs_dict(
+            hidden_states[:num_scheduled_tokens],
+            scheduler_output,
+        )
+
+        # Get the valid generated tokens.
+        sampled_token_ids = sampler_output.sampled_token_ids
+        max_gen_len = sampled_token_ids.shape[-1]
+        if max_gen_len == 1:
+            # No spec decode tokens.
+            valid_sampled_token_ids = sampled_token_ids.tolist()
+        else:
+            # Includes spec decode tokens.
+            valid_sampled_token_ids = self.rejection_sampler.parse_output(
+                sampled_token_ids,
+                self.input_batch.vocab_size,
+            )
+        # Mask out the sampled tokens that should not be sampled.
+        for i in discard_sampled_tokens_req_indices:
+            valid_sampled_token_ids[i].clear()
+
+        if not self.speculative_config:
+            # Speculative decoding is not enabled.
+            spec_token_ids = None
+        elif self.speculative_config.method == "ngram":
+            assert isinstance(self.drafter, NgramProposer)
+            spec_token_ids = self.generate_draft_token_ids(
+                valid_sampled_token_ids, sampling_metadata)
+        elif self.speculative_config.method == "medusa":
+            assert isinstance(self.drafter, MedusaProposer)
+            if max_gen_len == 1:
+                hidden_states = sample_hidden_states
+            else:
+                indices = []
+                offset = 0
+                for num_draft, tokens in zip(
+                        spec_decode_metadata.num_draft_tokens,
+                        valid_sampled_token_ids):
+                    indices.append(offset + len(tokens) - 1)
+                    offset += num_draft + 1
+
+                indices = torch.tensor(indices,
+                                       device=sample_hidden_states.device)
+                hidden_states = sample_hidden_states[indices]
+
+            spec_token_ids = self.drafter.propose(
+                target_hidden_states=hidden_states,
+                sampling_metadata=sampling_metadata,
+            )
+        elif self.speculative_config.use_eagle():
+            assert isinstance(self.drafter, EagleProposer)
+            # TODO(woosuk): Refactor the loop.
+            next_token_ids: list[int] = []
+            for i, token_ids in enumerate(valid_sampled_token_ids):
+                if token_ids:
+                    # Common case.
+                    next_token_id = token_ids[-1]
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: cache output token for spec_decode,
+                        the final token will cache in next step _update_states
+                    '''
+                    req_id = self.input_batch.req_ids[i]
+                    req_state = self.requests[req_id]
+                    req_state.output_token_ids.extend(token_ids[:-1])
+                    '''
+                    ==================
+                    End of MLU Hijack
+                    ==================
+                    '''
+                else:
+                    # Partial prefill (rare case).
+                    # Get the next token id from the request state.
+                    req_id = self.input_batch.req_ids[i]
+                    req_state = self.requests[req_id]
+                    seq_len = (req_state.num_computed_tokens +
+                               scheduler_output.num_scheduled_tokens[req_id])
+                    next_token_id = req_state.get_token_id(seq_len)
+                next_token_ids.append(next_token_id)
+            next_token_ids = torch.tensor(next_token_ids,
+                                          dtype=torch.int32,
+                                          device=self.device)
+            # At this moment, we assume all eagle layers belong to the same KV
+            # cache group, thus using the same attention metadata.
+            eagle_attn_metadata = attn_metadata[
+                self.drafter.attn_layer_names[0]]
+
+            # NOTE: deepseek_mtp uses MLA which does not have `block_table`
+            if hasattr(eagle_attn_metadata, "block_table"):
+                block_table = eagle_attn_metadata.block_table
+            else:
+                block_table = None
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            '''
+            num_rejected_tokens_tensor = None
+            token_indices = None
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            if spec_decode_metadata is None:
+                # input_ids can be None for multimodal models.
+                target_token_ids = self.input_ids[:num_scheduled_tokens]
+                target_positions = positions[:num_scheduled_tokens]
+                if self.use_aux_hidden_state_outputs:
+                    target_hidden_states = torch.cat(
+                        [h[:num_scheduled_tokens] for h in aux_hidden_states],
+                        dim=-1)
+                else:
+                    target_hidden_states = hidden_states[:num_scheduled_tokens]
+                target_slot_mapping = eagle_attn_metadata.slot_mapping
+                cu_num_tokens = eagle_attn_metadata.query_start_loc
+            else:
+                # TODO(woosuk): Refactor this.
+                num_draft_tokens = spec_decode_metadata.num_draft_tokens
+                num_rejected_tokens = [
+                    n + 1 - len(valid_sampled_token_ids[i]) if n > 0 else 0
+                    for i, n in enumerate(num_draft_tokens)
+                ]
+                num_rejected_tokens_tensor = async_tensor_h2d(
+                    num_rejected_tokens,
+                    dtype=torch.int32,
+                    target_device=self.device,
+                    pin_memory=True)
+                num_tokens = num_scheduled_tokens - sum(num_rejected_tokens)
+                cu_num_tokens, token_indices = self.drafter.prepare_inputs(
+                    eagle_attn_metadata.query_start_loc[:self.input_batch.num_reqs+1],
+                    num_rejected_tokens_tensor,
+                    num_tokens,
+                )
+                cu_num_tokens = eagle_attn_metadata.query_start_loc[:self.input_batch.num_reqs+1]
+                target_token_ids = self.input_ids[token_indices]
+                target_positions = positions[token_indices]
+                if self.use_aux_hidden_state_outputs:
+                    target_hidden_states = torch.cat(
+                        [h[token_indices] for h in aux_hidden_states], dim=-1)
+                else:
+                    target_hidden_states = hidden_states[token_indices]
+                target_slot_mapping = eagle_attn_metadata.slot_mapping[
+                    token_indices]
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            '''
+            target_token_ids = self.input_ids[:num_scheduled_tokens]
+            target_positions = positions[:num_scheduled_tokens]
+            # hidden_states no need to be sliced
+            target_hidden_states = hidden_states[:num_scheduled_tokens]
+            target_slot_mapping = eagle_attn_metadata.slot_mapping[:num_scheduled_tokens]
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            draft_token_ids = self.drafter.propose(
+                target_token_ids=target_token_ids,
+                target_positions=target_positions,
+                target_hidden_states=target_hidden_states,
+                target_slot_mapping=target_slot_mapping,
+                next_token_ids=next_token_ids,
+                cu_num_tokens=cu_num_tokens,
+                block_table=block_table,
+                sampling_metadata=sampling_metadata,
+                main_model_common_metadata=common_metadata,
+                num_rejected_tokens=num_rejected_tokens_tensor,
+                token_indices=token_indices,
+                whole_block_table=self.input_batch.block_table[0],
+                main_model_dp_params=dp_metadata,
+                time_markers=self.time_markers
+            )
+            spec_token_ids = draft_token_ids.tolist()
+
+        # Clear KVConnector state after all KVs are generated.
+        if has_kv_transfer_group():
+            get_kv_transfer_group().clear_connector_metadata()
+
+        return ModelRunnerOutput(
+            req_ids=self.input_batch.req_ids,
+            req_id_to_index=self.input_batch.req_id_to_index,
+            sampled_token_ids=valid_sampled_token_ids,
+            spec_token_ids=spec_token_ids,
+            logprobs=logprobs_lists,
+            prompt_logprobs_dict=prompt_logprobs_dict,
+            finished_sending=finished_sending,
+            finished_recving=finished_recving,
+        )
+    
+    def make_cnclep_kwargs(self) -> dict[Any, Any]:
+
+        K = (self.drafter.num_speculative_tokens 
+            if hasattr(self, "drafter") and isinstance(self.drafter, EagleProposer)
+            else 0)
+        seq_len = K + 1
+        config = self.model_config.hf_config
+        topk = config.num_experts_per_tok
+        hidden_size = config.hidden_size
+        dispatch_token_size = hidden_size * get_dtype_size(torch.int8) + get_dtype_size(torch.float32)
+        combine_token_size = hidden_size * get_dtype_size(self.dtype)
+
+        max_num_seqs_per_dp = self.scheduler_config.max_num_seqs
+        max_num_tokens_per_rank = divide(max_num_seqs_per_dp * seq_len * topk, 
+                                         self.parallel_config.tensor_parallel_size)
+
+        return dict(dispatch_token_size=dispatch_token_size,
+                    combine_token_size=combine_token_size,
+                    max_num_tokens_per_rank=max_num_tokens_per_rank,
+                    num_global_experts=config.n_routed_experts)
+
+    def prepare_all2all_buffer_for_model(
+        self, model: torch.nn.Module) -> None:
+        """
+        Prepare all2all buffer for the model.
+        """
+        if not self.use_all2all:
+            return
+
+        moe_modules = [
+            module for module in self.model.modules()
+            if isinstance(module, MLUDeepseekV2MoE)
+        ]
+        if hasattr(self, "drafter") and isinstance(self.drafter, EagleProposer):
+            draft_moes = [
+                module for module in self.drafter.model.modules()
+                if isinstance(module, MLUDeepseekV2MoE)
+            ]
+            moe_modules.extend(draft_moes)
+        for module in moe_modules:
+            if self.load_config.load_format == LoadFormat.DUMMY:
+                module.pack_params()
+                module.pack_params_after_loading()
+            module.prepare_for_cnclep(get_cnclep())
+    
+    def load_model(self) -> None:
+        super().load_model()
+        if self.use_all2all:
+            self.prepare_all2all_buffer_for_model(self.model)
\ No newline at end of file

