diff --git a/vllm_mlu/vllm_mlu/v1/worker/gpu_model_runner.py b/vllm_mlu/vllm_mlu/v1/worker/gpu_model_runner.py
new file mode 100644
index 000000000..5a0b57989
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/worker/gpu_model_runner.py
@@ -0,0 +1,2744 @@
+# SPDX-License-Identifier: Apache-2.0
+
+import gc
+import time
+import weakref
+from typing import TYPE_CHECKING, Optional, Union, Dict, List, Tuple, Any
+
+import numpy as np
+import torch
+import torch.distributed
+import torch.nn as nn
+from contextlib import contextmanager
+from dataclasses import dataclass
+from tqdm import tqdm
+
+from vllm.attention import AttentionType, get_attn_backend
+from vllm.attention.backends.abstract import (AttentionBackend,
+                                              AttentionMetadataBuilder)
+from vllm.attention.layer import Attention
+from vllm.attention.utils.fa_utils import get_flash_attn_version
+from vllm.config import (CompilationLevel, VllmConfig,
+                         get_layers_from_vllm_config)
+from vllm.distributed.kv_transfer import (get_kv_transfer_group,
+                                          has_kv_transfer_group)
+from vllm.distributed.parallel_state import (
+    get_pp_group, get_tp_group, prepare_communication_buffer_for_model)
+from vllm.forward_context import get_forward_context, set_forward_context
+from vllm.logger import init_logger
+from vllm.lora.layers import LoRAMapping
+from vllm.lora.request import LoRARequest
+from vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding
+from vllm.model_executor.model_loader import get_model, get_model_loader
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import MultiModalKwargs, PlaceholderRange
+from vllm.multimodal.utils import group_mm_inputs_by_modality
+from vllm.sequence import IntermediateTensors
+from vllm.sampling_params import SamplingType
+from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,
+                        GiB_bytes, LazyLoader, async_tensor_h2d, cdiv,
+                        check_use_alibi, is_pin_memory_available,
+                        weak_ref_tensor)
+from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
+from vllm.v1.kv_cache_interface import (AttentionSpec, FullAttentionSpec,
+                                        KVCacheConfig, KVCacheSpec,
+                                        SlidingWindowSpec)
+from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsTensors,
+                             ModelRunnerOutput)
+from vllm.v1.sample.metadata import SamplingMetadata
+from vllm.v1.sample.rejection_sampler import RejectionSampler
+from vllm.v1.sample.sampler import Sampler
+from vllm.v1.spec_decode.eagle import EagleProposer
+from vllm.v1.spec_decode.medusa import MedusaProposer
+from vllm.v1.spec_decode.metadata import SpecDecodeMetadata
+from vllm.v1.spec_decode.ngram_proposer import NgramProposer
+from vllm.v1.spec_decode.utils import is_spec_decode_supported
+from vllm.v1.utils import bind_kv_cache
+from vllm.v1.worker.block_table import BlockTable
+from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
+
+from vllm.v1.worker.utils import (initialize_kv_cache_for_kv_sharing,
+                                  sanity_check_mm_encoder_outputs,
+                                  scatter_mm_placeholders)
+
+if TYPE_CHECKING:
+    import xgrammar as xgr
+    from vllm.v1.core.sched.output import SchedulerOutput
+else:
+    xgr = LazyLoader("xgr", globals(), "xgrammar")
+
+
+from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+from vllm.model_executor.layers.linear import RowParallelLinear
+
+from vllm_mlu.v1.attention.backends.flash_attn import FlashAttentionMetadata, pad_attn_metadata
+from vllm_mlu.v1.attention.backends.mla.flashmla import FlashMLAMetadataBuilder
+from vllm_mlu.v1.attention.backends.utils import (
+    MLUCommonAttentionMetadata, get_common_metadata_from_attn_metadata,
+    get_common_metadata, COMMON_METADATA_STR)
+from vllm_mlu.v1.kv_cache_interface import (MLUFullAttentionSpec,
+                                            MLUSlidingWindowSpec)
+from vllm_mlu.model_executor.layers.rotary_embedding import MLURotaryEmbedding
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp
+from vllm_mlu._mlu_utils import *
+from vllm_mlu.v1.spec_decode.eagle import MluEagleProposer
+from vllm_mlu.v1.spec_decode.dp_eagle import DPMluEagleProposer
+from vllm_mlu.v1.sample.sampler import MluSampler
+
+logger = init_logger(__name__)
+
+
+_NUM_WARMUP_ITERS = 2
+
+# --- FLAGSCALE MODIFICATION BEG ---
+# Know more about FlagGems: https://github.com/FlagOpen/FlagGems
+import os
+if os.getenv("USE_FLAGGEMS", "false").lower() in ("1", "true", "yes"):
+   try:
+       print("Try to using FLAGGEMS...")
+       import flag_gems
+       flag_gems.enable(record=True, path="/tmp/gems_oplist.log.txt")
+       flag_gems.apply_gems_patches_to_vllm(verbose=True)         # [Experimental] Optional, not required now. Will be officially released later.
+       logger.info("Successfully enabled flag_gems as default ops implementation.")
+   except ImportError as e:
+       # Throw an exception directly if failure occurs
+       raise ImportError("Failed to import 'flag_gems'. Please install flag_gems or set USE_FLAGGEMS=false to disable it.") from e
+   except Exception as e:
+       # Throw an exception directly if failure occurs
+       raise RuntimeError(f"Failed to enable 'flag_gems': {e}. Please check your flag_gems installation or set USE_FLAGGEMS=false to disable it.") from e
+# --- FLAGSCALE MODIFICATION END ---
+
+
+@dataclass
+class MLUGraphCaptureContext:
+    stream: torch.mlu.Stream
+
+
+@contextmanager
+def mlu_graph_capture(device: torch.device):
+    """
+    `graph_capture` is a context manager which should surround the code that
+    is capturing the CUDA graph. Its main purpose is to ensure that the
+    some operations will be run after the graph is captured, before the graph
+    is replayed. It returns a `GraphCaptureContext` object which contains the
+    necessary data for the graph capture. Currently, it only contains the
+    stream that the graph capture is running on. This stream is set to the
+    current CUDA stream when the context manager is entered and reset to the
+    default stream when the context manager is exited. This is to ensure that
+    the graph capture is running on a separate stream from the default stream,
+    in order to explicitly distinguish the kernels to capture
+    from other kernels possibly launched on background in the default stream.
+    """
+    context = MLUGraphCaptureContext(torch.mlu.Stream(device=device))
+    with get_tp_group().graph_capture(context), get_pp_group().graph_capture(
+            context):
+        yield context
+
+
+def _model_forward_pre_hook(self, args, kwargs):
+    '''
+    This hook function will be called before model.forward
+    '''
+    assert len(args) == 0 and len(kwargs) > 0, \
+        f"The pre-forward's expected inputs are not passed by kwargs. " + \
+        f"Expected len(args)=0, len(kwargs)>0, " + \
+        f"now, len(args)={len(args)}, len(kwargs)={len(kwargs)}."
+
+    common_metadata: MLUCommonAttentionMetadata = get_common_metadata()
+
+    if common_metadata:
+        # Prepare attributes for all rope in model
+        MLURotaryEmbedding.set_mlu_var_v1(common_metadata=common_metadata)
+        is_prompt = common_metadata.is_prefill_only
+        RowParallelLinear.is_prompt = is_prompt
+        FeedForward.is_prompt = is_prompt
+        if is_prompt:
+            set_is_prompt(True)
+        else:
+            set_is_prompt(False)
+
+    return (args, kwargs)
+
+
+class MLUModelRunner(GPUModelRunner):
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        device: torch.device,
+    ):
+        self.vllm_config = vllm_config
+        self.model_config = vllm_config.model_config
+        self.cache_config = vllm_config.cache_config
+        self.lora_config = vllm_config.lora_config
+        self.load_config = vllm_config.load_config
+        self.parallel_config = vllm_config.parallel_config
+        self.scheduler_config = vllm_config.scheduler_config
+        self.speculative_config = vllm_config.speculative_config
+        self.prompt_adapter_config = vllm_config.prompt_adapter_config
+        self.observability_config = vllm_config.observability_config
+        self.mlu_config = vllm_config.mlu_config
+
+        from vllm.model_executor.models.utils import set_cpu_offload_max_bytes
+        set_cpu_offload_max_bytes(
+            int(self.cache_config.cpu_offload_gb * 1024**3))
+
+        model_config = self.model_config
+        cache_config = self.cache_config
+        scheduler_config = self.scheduler_config
+        parallel_config = self.parallel_config
+        self.device = device
+        self.pin_memory = is_pin_memory_available()
+        self.dtype = self.model_config.dtype
+        if cache_config.cache_dtype == "auto":
+            self.kv_cache_dtype = self.dtype
+        else:
+            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[
+                cache_config.cache_dtype]
+
+        self.is_multimodal_model = model_config.is_multimodal_model
+        self.max_model_len = model_config.max_model_len
+        self.max_num_tokens = scheduler_config.max_num_batched_tokens
+        self.max_num_reqs = scheduler_config.max_num_seqs
+
+        # Model-related.
+        self.num_query_heads = model_config.get_num_attention_heads(
+            parallel_config)
+        self.hidden_size = model_config.get_hidden_size()
+        self.attention_chunk_size = model_config.attention_chunk_size
+
+        self.cascade_attn_enabled = not self.model_config.disable_cascade_attn
+
+        # Multi-modal data support
+        self.mm_registry = MULTIMODAL_REGISTRY
+        self.uses_mrope = model_config.uses_mrope
+
+        encoder_compute_budget, encoder_cache_size = compute_encoder_budget(
+            model_config=model_config,
+            scheduler_config=scheduler_config,
+            mm_registry=self.mm_registry,
+        )
+        self.max_num_encoder_input_tokens = encoder_compute_budget
+        self.encoder_cache_size = encoder_cache_size
+
+        # Sampler
+        """
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use tmo topk_topp_sampler to sample.
+        """
+        sampler_cls = MluSampler if self.model_config.is_deepseek_mla else Sampler
+        self.sampler = sampler_cls()
+        """
+        =================
+        End of MLU Hijack
+        =================
+        """
+
+        # Lazy initializations
+        # self.model: nn.Module  # Set after load_model
+        # Initialize in initialize_kv_cache
+        self.kv_caches: list[torch.Tensor] = []
+        self.attn_metadata_builders: list[AttentionMetadataBuilder] = []
+        self.attn_backends: list[type[AttentionBackend]] = []
+        # self.kv_cache_config: KVCacheConfig
+
+        # req_id -> (input_id -> encoder_output)
+        self.encoder_cache: dict[str, dict[int, torch.Tensor]] = {}
+
+        self.use_aux_hidden_state_outputs = False
+        # Set up speculative decoding.
+        # NOTE(Jiayi): currently we put the entire draft model on
+        # the last PP rank. This is not ideal if there are many
+        # layers in the draft model.
+        if self.speculative_config and get_pp_group().is_last_rank:
+            if self.speculative_config.method == "ngram":
+                self.drafter = NgramProposer(self.vllm_config)
+            elif self.speculative_config.use_eagle():
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: Use MluEagleProposer instead of EagleProposer
+                '''
+                if vllm_config.mlu_config.enable_custom_data_parallel_opt:
+                    proposer_cls = DPMluEagleProposer
+                else:
+                    proposer_cls = MluEagleProposer
+                self.drafter = proposer_cls(self.vllm_config, self.device,
+                                            self)  # type: ignore
+                self.previous_hidden_states = torch.zeros(
+                    (self.max_num_tokens, self.hidden_size),
+                    dtype=self.dtype,
+                    device=self.device)
+                '''
+                =============================
+                End of MLU Hijack
+                =============================
+                '''
+                if self.speculative_config.method == "eagle3":
+                    self.use_aux_hidden_state_outputs = True
+            elif self.speculative_config.method == "medusa":
+                self.drafter = MedusaProposer(
+                    vllm_config=self.vllm_config,
+                    device=self.device)  # type: ignore
+            else:
+                raise ValueError("Unknown speculative decoding method: "
+                                 f"{self.speculative_config.method}")
+            self.rejection_sampler = RejectionSampler()
+
+        # Request states.
+        self.requests: dict[str, CachedRequestState] = {}
+
+        # Input Batch
+        # NOTE(Chen): Ideally, we should initialize the input batch inside
+        # `initialize_kv_cache` based on the kv cache config. However, as in
+        # https://github.com/vllm-project/vllm/pull/18298, due to some unknown
+        # reasons, we have to initialize the input batch before `load_model`,
+        # quantization + weight offloading will fail otherwise. As a temporary
+        # solution, we initialize the input batch here, and re-initialize it
+        # in `initialize_kv_cache` if the block_sizes here is different from
+        # the block_sizes in the kv cache config.
+        self.input_batch = InputBatch(
+            max_num_reqs=self.max_num_reqs,
+            max_model_len=self.max_model_len,
+            max_num_batched_tokens=self.max_num_tokens,
+            device=self.device,
+            pin_memory=self.pin_memory,
+            vocab_size=self.model_config.get_vocab_size(),
+            block_sizes=[self.cache_config.block_size],
+        )
+
+        self.use_cuda_graph = ((self.vllm_config.compilation_config.level
+                                == CompilationLevel.PIECEWISE
+                                or VLLM_V1_USE_FULL_GRAPH
+                                ) and not self.model_config.enforce_eager)
+        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.
+        # The convention is different.
+        # self.cudagraph_batch_sizes sorts in ascending order.
+        # The batch sizes in the config are in descending order.
+        self.cudagraph_batch_sizes = list(
+            reversed(
+                self.vllm_config.compilation_config.cudagraph_capture_sizes))
+
+        # Cache the device properties.
+        self._init_device_properties()
+
+        # Persistent buffers for CUDA graphs.
+        self.input_ids = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int32,
+                                     device=self.device)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: change postions/slot_mapping dtype from int64 to int32
+        '''
+        self.positions = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int32,
+                                     device=self.device)
+        self.query_start_loc = torch.zeros(self.max_num_reqs + 1,
+                                           dtype=torch.int32,
+                                           device=self.device)
+        self.seq_lens = torch.zeros(self.max_num_reqs,
+                                    dtype=torch.int32,
+                                    device=self.device)
+        self.slot_mapping = torch.zeros(self.max_num_tokens,
+                                        dtype=torch.int32,
+                                        device=self.device)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        # None in the first PP rank. The rest are set after load_model.
+        self.intermediate_tensors: Optional[IntermediateTensors] = None
+
+        # Only relevant for models using M-RoPE (e.g, Qwen2-VL)
+        if self.uses_mrope:
+            # NOTE: `mrope_positions` is implemented with one additional dummy
+            # position on purpose to make it non-contiguous so that it can work
+            # with torch compile.
+            # See detailed explanation in https://github.com/vllm-project/vllm/pull/12128#discussion_r1926431923
+
+            # NOTE: When M-RoPE is enabled, position ids are 3D regardless of
+            # the modality of inputs. For text-only inputs, each dimension has
+            # identical position IDs, making M-RoPE functionally equivalent to
+            # 1D-RoPE.
+            # See page 5 of https://arxiv.org/abs/2409.12191
+            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+                                               dtype=torch.int32,
+                                               device=self.device)
+            self.mrope_positions_cpu = torch.zeros(
+                (3, self.max_num_tokens + 1),
+                dtype=torch.int32,
+                device="cpu",
+                pin_memory=self.pin_memory)
+
+        # Only relevant for models using ALiBi (e.g, MPT)
+        self.use_alibi = check_use_alibi(model_config)
+
+        self.inputs_embeds = torch.zeros(
+            (self.max_num_tokens, self.hidden_size),
+            dtype=self.dtype,
+            device=self.device)
+
+        # OPTIMIZATION: Cache the tensors rather than creating them every step.
+        # Keep in int64 to avoid overflow with long context
+        self.arange_np = np.arange(max(self.max_num_reqs + 1,
+                                       self.max_model_len,
+                                       self.max_num_tokens),
+                                   dtype=np.int64)
+        # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
+        # a faster version of creating a new tensor every time. Thus, we should
+        # not make any assumptions about the values in these tensors.
+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int32,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.positions_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int32,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.positions_np = self.positions_cpu.numpy()
+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                               dtype=torch.int32,
+                                               device="cpu",
+                                               pin_memory=self.pin_memory)
+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()
+        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+                                        dtype=torch.int32,
+                                        device="cpu",
+                                        pin_memory=self.pin_memory)
+        self.seq_lens_np = self.seq_lens_cpu.numpy()
+
+        # Layer pairings for cross-layer KV sharing.
+        # If an Attention layer `layer_name` is in the keys of this dict, it
+        # means this layer will perform attention using the keys and values
+        # from the KV cache of `shared_kv_cache_layers[layer_name]`.
+        self.shared_kv_cache_layers: dict[str, str] = {}
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add seq_start_loc for chunk fa
+        @brief: support full graph capture
+        @brief: support context mlugraph
+        '''
+        self.seq_start_loc = torch.zeros(self.max_num_reqs + 1,
+                                         dtype=torch.int32,
+                                         device=self.device)
+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                             dtype=torch.int32,
+                                             device="cpu",
+                                             pin_memory=self.pin_memory)
+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
+
+        self.graph_runners: dict[int, MLUGraphRunner_V1] = {}
+        self.graph_memory_pool: Optional[tuple[
+            int, int]] = None  # Set during graph capture.
+
+        self.context_graph_runner: MLUGraphRunner_V1 = None
+        self.prefill_enable_mlugraph = self.mlu_config.prefill_enable_mlugraph
+        self.prefill_mlugraph_batch_size = self.mlu_config.prefill_mlugraph_batch_size
+        self.prefill_mlugraph_seq_len = self.mlu_config.prefill_mlugraph_seq_len
+
+        self.num_speculative_tokens = 0
+        if self.speculative_config is not None:
+            self.num_speculative_tokens = self.speculative_config.num_speculative_tokens
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    # Note: used for model runner override.
+    def _init_device_properties(self) -> None:
+        """Initialize attributes from torch.mlu.get_device_properties
+        """
+        self.device_properties = torch.mlu.get_device_properties(self.device)
+        self.num_sms = self.device_properties.multi_processor_count
+
+    # Note: used for model runner override.
+    def _sync_device(self) -> None:
+        torch.mlu.synchronize()
+
+    def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
+        """Update the cached states and the persistent batch with the scheduler
+        output.
+
+        The updated states are used by the `_prepare_inputs` function to create
+        the input GPU tensors for the model.
+
+        The SamplingMetadata is updated and copied to the GPU if there is a
+        new/resumed/paused/finished request in the batch.
+        """
+        # Remove finished requests from the cached states.
+        for req_id in scheduler_output.finished_req_ids:
+            self.requests.pop(req_id, None)
+            self.encoder_cache.pop(req_id, None)
+        # Remove the finished requests from the persistent batch.
+        # NOTE(woosuk): There could be an edge case where finished_req_ids and
+        # scheduled_req_ids overlap. This happens when a request is aborted and
+        # then resubmitted with the same ID. In this case, we treat them as two
+        # distinct requests - clearing the cached states for the first request
+        # and handling the second as a new request.
+        removed_req_indices: list[int] = []
+        for req_id in scheduler_output.finished_req_ids:
+            req_index = self.input_batch.remove_request(req_id)
+            if req_index is not None:
+                removed_req_indices.append(req_index)
+
+        # Free the cached encoder outputs.
+        for req_id, input_id in scheduler_output.free_encoder_input_ids:
+            encoder_outputs = self.encoder_cache.get(req_id)
+            if encoder_outputs is not None:
+                encoder_outputs.pop(input_id, None)
+                if not encoder_outputs:
+                    self.encoder_cache.pop(req_id, None)
+
+        # Remove the unscheduled requests from the persistent batch.
+        # NOTE(woosuk): The unscheduled requests are either preempted requests
+        # or running requests that are not scheduled in this step. We remove
+        # them from the persistent batch but keep their cached states since
+        # they will be scheduled again sometime in the future.
+        scheduled_req_ids = scheduler_output.num_scheduled_tokens.keys()
+        cached_req_ids = self.input_batch.req_id_to_index.keys()
+        unscheduled_req_ids = cached_req_ids - scheduled_req_ids
+        # NOTE(woosuk): The persistent batch optimization assumes that
+        # consecutive batches contain mostly the same requests. If batches
+        # have low request overlap (e.g., alternating between two distinct
+        # sets of requests), this optimization becomes very inefficient.
+        for req_id in unscheduled_req_ids:
+            req_index = self.input_batch.remove_request(req_id)
+            assert req_index is not None
+            removed_req_indices.append(req_index)
+
+        req_ids_to_add: list[str] = []
+        # Add new requests to the cached states.
+        for new_req_data in scheduler_output.scheduled_new_reqs:
+            req_id = new_req_data.req_id
+            sampling_params = new_req_data.sampling_params
+            if sampling_params.sampling_type == SamplingType.RANDOM_SEED:
+                generator = torch.Generator(device=self.device)
+                generator.manual_seed(sampling_params.seed)
+            else:
+                generator = None
+
+            self.requests[req_id] = CachedRequestState(
+                req_id=req_id,
+                prompt_token_ids=new_req_data.prompt_token_ids,
+                mm_inputs=new_req_data.mm_inputs,
+                mm_positions=new_req_data.mm_positions,
+                sampling_params=sampling_params,
+                generator=generator,
+                block_ids=new_req_data.block_ids,
+                num_computed_tokens=new_req_data.num_computed_tokens,
+                output_token_ids=[],
+                lora_request=new_req_data.lora_request,
+            )
+
+            # Only relevant for models using M-RoPE (e.g, Qwen2-VL)
+            if self.uses_mrope:
+                image_grid_thw = []
+                video_grid_thw = []
+                second_per_grid_ts = []
+                audio_feature_lengths = []
+                use_audio_in_video = False
+                for mm_input in self.requests[req_id].mm_inputs:
+                    if mm_input.get("image_grid_thw") is not None:
+                        image_grid_thw.extend(
+                            mm_input["image_grid_thw"].tolist())
+                    if mm_input.get("video_grid_thw") is not None:
+                        video_grid_thw.extend(
+                            mm_input["video_grid_thw"].tolist())
+                    if mm_input.get("second_per_grid_ts") is not None:
+                        second_per_grid_ts.extend(
+                            mm_input["second_per_grid_ts"])
+                    if mm_input.get("audio_feature_lengths") is not None:
+                        audio_feature_lengths.extend(
+                            mm_input["audio_feature_lengths"])
+                    if mm_input.get("use_audio_in_video") is True:
+                        use_audio_in_video = True
+
+                hf_config = self.model_config.hf_config
+
+                self.requests[req_id].mrope_positions, \
+                    self.requests[req_id].mrope_position_delta = \
+                    MRotaryEmbedding.get_input_positions_tensor(
+                        self.requests[req_id].prompt_token_ids,
+                        hf_config=hf_config,
+                        image_grid_thw=image_grid_thw,
+                        video_grid_thw=video_grid_thw,
+                        second_per_grid_ts=second_per_grid_ts,
+                        audio_feature_lengths=audio_feature_lengths,
+                        use_audio_in_video=use_audio_in_video,
+                    )
+
+            req_ids_to_add.append(req_id)
+
+        # Update the states of the running/resumed requests.
+        for req_data in scheduler_output.scheduled_cached_reqs:
+            req_id = req_data.req_id
+            req_state = self.requests[req_id]
+
+            # Update the cached states.
+            num_computed_tokens = req_data.num_computed_tokens
+            req_state.num_computed_tokens = num_computed_tokens
+            # Add the sampled token(s) from the previous step (if any).
+            # This doesn't include "unverified" tokens like spec decode tokens.
+            num_new_tokens = (num_computed_tokens +
+                              len(req_data.new_token_ids) -
+                              req_state.num_tokens)
+            if num_new_tokens == 1:
+                # Avoid slicing list in most common case.
+                req_state.output_token_ids.append(req_data.new_token_ids[-1])
+            elif num_new_tokens > 0:
+                req_state.output_token_ids.extend(
+                    req_data.new_token_ids[-num_new_tokens:])
+            # Update the block IDs.
+            if not req_data.resumed_from_preemption:
+                # Append the new blocks to the existing block IDs.
+                for block_ids, new_block_ids in zip(  # type: ignore[call-overload]
+                        req_state.block_ids,
+                        req_data.new_block_ids,
+                        strict=True):
+                    block_ids.extend(new_block_ids)
+            else:
+                # The request is resumed from preemption.
+                # Replace the existing block IDs with the new ones.
+                req_state.block_ids = req_data.new_block_ids
+
+            req_index = self.input_batch.req_id_to_index.get(req_id)
+            if req_index is None:
+                # The request is not in the persistent batch.
+                # The request was either preempted and resumed later, or was not
+                # scheduled in the previous step and needs to be added again.
+                req_ids_to_add.append(req_id)
+                continue
+
+            # Update the persistent batch.
+            self.input_batch.num_computed_tokens_cpu[req_index] = (
+                num_computed_tokens)
+            self.input_batch.block_table.append_row(req_data.new_block_ids,
+                                                    req_index)
+            # Add new_token_ids to token_ids_cpu.
+            start_token_index = num_computed_tokens
+            end_token_index = num_computed_tokens + len(req_data.new_token_ids)
+            self.input_batch.token_ids_cpu[
+                req_index,
+                start_token_index:end_token_index] = req_data.new_token_ids
+            self.input_batch.num_tokens_no_spec[req_index] = end_token_index
+            # Add spec_token_ids to token_ids_cpu.
+            spec_token_ids = scheduler_output.scheduled_spec_decode_tokens.get(
+                req_id, ())
+            if spec_token_ids:
+                start_index = end_token_index
+                end_token_index += len(spec_token_ids)
+                self.input_batch.token_ids_cpu[
+                    req_index, start_index:end_token_index] = spec_token_ids
+            # NOTE(woosuk): `num_tokens` here may include spec decode tokens.
+            self.input_batch.num_tokens[req_index] = end_token_index
+
+        # Check if the batch has changed. If not, we can skip copying the
+        # sampling metadata from CPU to GPU.
+        batch_changed = len(removed_req_indices) > 0 or len(req_ids_to_add) > 0
+
+        # Add the new or resumed requests to the persistent batch.
+        # The smaller empty indices are filled first.
+        removed_req_indices.sort(reverse=True)
+        for req_id in req_ids_to_add:
+            req_state = self.requests[req_id]
+            if removed_req_indices:
+                # Fill the empty index.
+                req_index = removed_req_indices.pop()
+            else:
+                # Append to the end.
+                req_index = None
+            self.input_batch.add_request(req_state, req_index)
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: cache the unverified spec_decode token
+            '''
+            req_index = self.input_batch.req_id_to_index.get(req_id)
+            assert req_index is not None
+            num_tokens = self.input_batch.num_tokens[req_index]
+            end_token_index = num_tokens
+            spec_token_ids = scheduler_output.scheduled_spec_decode_tokens.get(
+                req_id, ())
+            if spec_token_ids:
+                start_index = end_token_index
+                end_token_index += len(spec_token_ids)
+                self.input_batch.token_ids_cpu[
+                    req_index, start_index:end_token_index] = spec_token_ids
+            # NOTE(woosuk): `num_tokens` here may include spec decode tokens.
+            self.input_batch.num_tokens[req_index] = end_token_index
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+        # Condense the batched states if there are empty indices.
+        if removed_req_indices:
+            self.input_batch.condense(removed_req_indices)
+
+        batch_reordered = self._may_reorder_batch(scheduler_output)
+
+        if batch_changed or batch_reordered:
+            self.input_batch.refresh_sampling_metadata()
+
+    def _prepare_inputs(
+        self,
+        scheduler_output: "SchedulerOutput",
+    ) -> tuple[dict[str, Any], torch.Tensor, Optional[SpecDecodeMetadata]]:
+        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+        assert total_num_scheduled_tokens > 0
+        num_reqs = self.input_batch.num_reqs
+        assert num_reqs > 0
+
+        # OPTIMIZATION: Start copying the block table first.
+        # This way, we can overlap the copy with the following CPU operations.
+        self.input_batch.block_table.commit(num_reqs)
+
+        # Get the number of scheduled tokens for each request.
+        req_ids = self.input_batch.req_ids
+        tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]
+        num_scheduled_tokens = np.array(tokens, dtype=np.int32)
+        max_num_scheduled_tokens = max(tokens)
+
+        # Get request indices.
+        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]
+        req_indices = np.repeat(self.arange_np[:num_reqs],
+                                num_scheduled_tokens)
+
+        # cu_num_tokens: [2, 5, 3] -> [2, 7, 10]
+        # arange: [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+        cu_num_tokens, arange = self._get_cumsum_and_arange(
+            num_scheduled_tokens)
+
+        # Get positions.
+        positions_np = self.positions_np[:total_num_scheduled_tokens]
+        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],
+               arange,
+               out=positions_np)
+
+        # Calculate M-RoPE positions.
+        # Only relevant for models using M-RoPE (e.g, Qwen2-VL)
+        if self.uses_mrope:
+            self._calc_mrope_positions(scheduler_output)
+
+        # Get token indices.
+        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]
+        # where M is the max_model_len.
+        token_indices = (positions_np +
+                         req_indices * self.input_batch.token_ids_cpu.shape[1])
+
+        # NOTE(woosuk): We use torch.index_select instead of np.take here
+        # because torch.index_select is much faster than np.take for large
+        # tensors.
+        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),
+                           0,
+                           torch.from_numpy(token_indices),
+                           out=self.input_ids_cpu[:total_num_scheduled_tokens])
+
+        # Calculate the slot mapping for each KV cache group.
+        for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                self.kv_cache_config.kv_cache_groups):
+            block_size = kv_cache_group_spec.kv_cache_spec.block_size
+            block_table: BlockTable = self.input_batch.block_table[
+                kv_cache_group_id]
+            # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+            # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]
+            # where K is the max_num_blocks_per_req and the block size is 2.
+            # NOTE(woosuk): We can't simply use `token_indices // block_size`
+            # here because M (max_model_len) is not necessarily divisible by
+            # block_size.
+            block_table_indices = (
+                req_indices * block_table.max_num_blocks_per_req +
+                positions_np // block_size)
+            block_table_cpu = block_table.get_cpu_tensor()
+            block_numbers = block_table_cpu.flatten(
+            )[block_table_indices].numpy()
+            block_offsets = positions_np % block_size
+            np.add(
+                block_numbers * block_size,
+                block_offsets,
+                out=block_table.slot_mapping_np[:total_num_scheduled_tokens])
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add seq_start_loc for chunk fa
+        '''
+        # Prepare the attention metadata.
+        self.query_start_loc_np[0] = 0
+        self.query_start_loc_np[1:num_reqs + 1] = cu_num_tokens
+
+        self.seq_lens_np[:num_reqs] = (
+            self.input_batch.num_computed_tokens_cpu[:num_reqs] +
+            num_scheduled_tokens)
+
+        self.seq_start_loc_np[0] = 0
+        self.seq_start_loc_np[1:num_reqs + 1] = np.cumsum(self.seq_lens_np[:num_reqs])
+
+        # Copy the tensors to the GPU.
+        self.input_ids[:total_num_scheduled_tokens].copy_(
+            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)
+        if self.uses_mrope:
+            # Only relevant for models using M-RoPE (e.g, Qwen2-VL)
+            self.mrope_positions[:, :total_num_scheduled_tokens].copy_(
+                self.mrope_positions_cpu[:, :total_num_scheduled_tokens],
+                non_blocking=True)
+        else:
+            # Common case (1D positions)
+            self.positions[:total_num_scheduled_tokens].copy_(
+                self.positions_cpu[:total_num_scheduled_tokens],
+                non_blocking=True)
+
+        self.query_start_loc[:num_reqs + 1].copy_(
+            self.query_start_loc_cpu[:num_reqs + 1], non_blocking=True)
+        self.seq_lens[:num_reqs].copy_(self.seq_lens_cpu[:num_reqs],
+                                       non_blocking=True)
+        self.seq_start_loc[:num_reqs + 1].copy_(
+            self.seq_start_loc_cpu[:num_reqs + 1], non_blocking=True)
+
+        # Fill unused with -1. Needed for reshape_and_cache
+        self.seq_lens[num_reqs:].fill_(0)
+        # Note: pad query_start_loc to be non-decreasing, as kernels
+        # like FlashAttention requires that
+        self.query_start_loc[num_reqs + 1:].fill_(
+            self.query_start_loc_cpu[num_reqs].item())
+        self.seq_start_loc[num_reqs + 1:].fill_(-1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        query_start_loc = self.query_start_loc[:num_reqs + 1]
+        seq_lens = self.seq_lens[:num_reqs]
+        seq_start_loc = self.seq_start_loc[:num_reqs + 1]
+        is_start_loc_match = (self.query_start_loc_cpu[num_reqs].item()
+                              == self.seq_start_loc_cpu[num_reqs].item())
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add num_speculative_tokens
+        '''
+        num_speculative_tokens = 0
+        if self.speculative_config is not None:
+            num_speculative_tokens = self.speculative_config.num_speculative_tokens
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        has_prefill_reqs = False
+        for req_id in self.input_batch.req_ids:
+            req_index = self.input_batch.req_id_to_index.get(req_id)
+            num_prompt_tokens = self.input_batch.num_prompt_tokens[req_index]
+            num_computed_tokens = self.input_batch.num_computed_tokens_cpu[req_index]
+            if num_computed_tokens < num_prompt_tokens:
+                has_prefill_reqs = True
+                break
+        common_attn_metadata = MLUCommonAttentionMetadata.build(
+            query_start_loc=query_start_loc,
+            seq_lens=seq_lens,
+            seq_start_loc=seq_start_loc,
+            is_start_loc_match=is_start_loc_match,
+            max_query_len=max_num_scheduled_tokens,
+            num_actual_tokens=total_num_scheduled_tokens,
+            num_speculative_tokens=num_speculative_tokens,
+            has_prefill_reqs=has_prefill_reqs,
+        )
+
+        attn_metadata: dict[str, Any] = {}
+        # Prepare the attention metadata for each KV cache group and make layers
+        # in the same group share the same metadata.
+        for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                self.kv_cache_config.kv_cache_groups):
+
+            # Prepare for cascade attention if enabled & beneficial.
+            common_prefix_len = 0
+            if self.cascade_attn_enabled:
+                common_prefix_len = self._compute_cascade_attn_prefix_len(
+                    num_scheduled_tokens,
+                    scheduler_output.
+                    num_common_prefix_blocks[kv_cache_group_id],
+                    kv_cache_group_spec.kv_cache_spec,
+                    self.attn_metadata_builders[kv_cache_group_id],
+                )
+
+            attn_metadata_i = (
+                self.attn_metadata_builders[kv_cache_group_id].build(
+                    num_reqs=num_reqs,
+                    num_actual_tokens=total_num_scheduled_tokens,
+                    max_query_len=max_num_scheduled_tokens,
+                    common_prefix_len=common_prefix_len,
+                    common_attn_metadata=common_attn_metadata))
+            for layer_name in kv_cache_group_spec.layer_names:
+                attn_metadata[layer_name] = attn_metadata_i
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: bind decode_attn metadata to prefill_attn
+                '''
+                if (self.model_config.is_deepseek_mla
+                        and layer_name.endswith("self_attn.mla_attn")):
+                    prefill_attn_name = layer_name.replace(
+                        "self_attn.mla_attn", "self_attn.attn")
+                    attn_metadata[prefill_attn_name] = attn_metadata_i
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+        attn_metadata[COMMON_METADATA_STR] = common_attn_metadata
+
+        use_spec_decode = len(
+            scheduler_output.scheduled_spec_decode_tokens) > 0
+        if not use_spec_decode:
+            # NOTE(woosuk): Due to chunked prefills, the batch may contain
+            # partial requests. While we should not sample any token
+            # from these partial requests, we do so for simplicity.
+            # We will ignore the sampled tokens from the partial requests.
+            # TODO: Support prompt logprobs.
+            logits_indices = query_start_loc[1:] - 1
+            spec_decode_metadata = None
+        else:
+            # Get the number of draft tokens for each request.
+            # Iterate over the dictionary rather than all requests since not all
+            # requests have draft tokens.
+            num_draft_tokens = np.zeros(num_reqs, dtype=np.int32)
+            for req_id, draft_token_ids in (
+                    scheduler_output.scheduled_spec_decode_tokens.items()):
+                req_idx = self.input_batch.req_id_to_index[req_id]
+                num_draft_tokens[req_idx] = len(draft_token_ids)
+
+            spec_decode_metadata = self._calc_spec_decode_metadata(
+                num_draft_tokens, cu_num_tokens)
+            logits_indices = spec_decode_metadata.logits_indices
+
+        # Hot-Swap lora model
+        if self.lora_config:
+            self.set_active_loras(self.input_batch, num_scheduled_tokens)
+
+        return attn_metadata, logits_indices, spec_decode_metadata
+
+    def _execute_mm_encoder(self, scheduler_output: "SchedulerOutput"):
+        scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs
+        if not scheduled_encoder_inputs:
+            return
+
+        # Batch the multi-modal inputs.
+        mm_inputs = list[MultiModalKwargs]()
+        req_ids_pos = list[tuple[str, int, PlaceholderRange]]()
+        for req_id, encoder_input_ids in scheduled_encoder_inputs.items():
+            req_state = self.requests[req_id]
+
+            for mm_input_id in encoder_input_ids:
+                mm_inputs.append(req_state.mm_inputs[mm_input_id])
+                req_ids_pos.append(
+                    (req_id, mm_input_id, req_state.mm_positions[mm_input_id]))
+
+        # Batch mm inputs as much as we can: if a request in the batch has
+        # multiple modalities or a different modality than the previous one,
+        # we process it separately to preserve item order.
+        # FIXME(ywang96): This is a hacky way to deal with multiple modalities
+        # in the same batch while still being able to benefit from batching
+        # multimodal inputs. The proper solution should be reordering the
+        # encoder outputs.
+        grouped_mm_inputs_list = group_mm_inputs_by_modality(mm_inputs)
+
+        encoder_outputs = []
+        for grouped_mm_inputs in grouped_mm_inputs_list:
+            batched_mm_inputs = MultiModalKwargs.batch(
+                grouped_mm_inputs, pin_memory=self.pin_memory)
+            batched_mm_inputs = MultiModalKwargs.as_kwargs(
+                batched_mm_inputs,
+                device=self.device,
+            )
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: v1 offline benchmark
+            '''
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                mm_start = torch.mlu.Event(enable_timing=True)
+                mm_start.record()
+            # Run the encoder.
+            # `curr_group_outputs` is either of the following:
+            # 1. A tensor of shape (num_items, feature_size, hidden_size)
+            # in case feature_size is fixed across all multimodal items.
+            # 2. A list or tuple (length: num_items) of tensors, each of shape
+            # (feature_size, hidden_size) in case the feature size is dynamic
+            # depending on the input multimodal items.
+            curr_group_outputs = self.model.get_multimodal_embeddings(
+                **batched_mm_inputs)
+            if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+                mm_end = torch.mlu.Event(enable_timing=True)
+                mm_end.record()
+                self.mm_time_markers.append([mm_start, mm_end])
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+            sanity_check_mm_encoder_outputs(
+                curr_group_outputs,
+                expected_num_items=len(grouped_mm_inputs),
+            )
+
+            for output in curr_group_outputs:
+                encoder_outputs.append(output)
+
+        # Cache the encoder outputs.
+        for (req_id, input_id, pos_info), output in zip(
+                req_ids_pos,
+                encoder_outputs,
+        ):
+            if req_id not in self.encoder_cache:
+                self.encoder_cache[req_id] = {}
+
+            self.encoder_cache[req_id][input_id] = scatter_mm_placeholders(
+                output,
+                is_embed=pos_info.is_embed,
+            )
+
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        scheduler_output: "SchedulerOutput",
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+    ) -> Union[ModelRunnerOutput, IntermediateTensors]:
+
+        self._update_states(scheduler_output)
+        if not scheduler_output.total_num_scheduled_tokens:
+            if not has_kv_transfer_group():
+                # Return empty ModelRunnerOutput if there's no work to do.
+                return EMPTY_MODEL_RUNNER_OUTPUT
+
+            return self.kv_connector_no_forward(scheduler_output)
+
+        # Prepare the decoder inputs.
+        attn_metadata, logits_indices, spec_decode_metadata = (
+            self._prepare_inputs(scheduler_output))
+
+        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+        common_metadata: MLUCommonAttentionMetadata = \
+            get_common_metadata_from_attn_metadata(attn_metadata)
+        K = self.drafter.num_speculative_tokens if hasattr(self, "drafter") else 0
+        if (self.use_cuda_graph
+                and num_scheduled_tokens // (K + 1) <= self.cudagraph_batch_sizes[-1]):
+            # Use piecewise CUDA graphs.
+            # Add padding to the batch size.
+            num_input_tokens = self.vllm_config.pad_for_cudagraph(
+                num_scheduled_tokens // (K + 1)) * (K + 1)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: When disable dp and in decode stage, we pad input tokens to use full graph.
+            Otherwise, we use full graph if and only if no pad behavior.
+            '''
+            dp_size = self.vllm_config.parallel_config.data_parallel_size
+            if VLLM_V1_USE_FULL_GRAPH:
+                if common_metadata.is_decode_only and dp_size == 1:
+                    assert num_scheduled_tokens % (1 + K) == 0, (
+                        f"scheduled token number {num_scheduled_tokens} must be multiple of "
+                        f"one plus speculative tokens {K} in decoder only scenarios."
+                    )
+                    num_input_batches = num_scheduled_tokens // (1 + K)
+                    num_input_batches = self.vllm_config.pad_for_cudagraph(num_input_batches)
+                    num_input_tokens = num_input_batches * (1 + K)
+                else:
+                    num_input_tokens = num_scheduled_tokens
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            # Eager mode.
+            # Pad tokens to multiple of tensor_parallel_size when
+            # enabled collective fusion for SP
+            tp_size = self.vllm_config.parallel_config.tensor_parallel_size
+            if self.vllm_config.compilation_config.pass_config. \
+                enable_sequence_parallelism and tp_size > 1:
+                from vllm.utils import round_up
+                num_input_tokens = round_up(num_scheduled_tokens, tp_size)
+            else:
+                num_input_tokens = num_scheduled_tokens
+
+        # Padding for DP
+        num_pad, num_tokens_across_dp = self.get_dp_padding(num_input_tokens)
+        num_input_tokens += num_pad
+
+        # _prepare_inputs may reorder the batch, so we must gather multi
+        # modal outputs after that to ensure the correct order
+        self.mm_time_markers = []
+        if self.is_multimodal_model:
+            # Run the multimodal encoder if any.
+            self._execute_mm_encoder(scheduler_output)
+            mm_embeds = self._gather_mm_embeddings(scheduler_output)
+        else:
+            mm_embeds = []
+
+        if self.is_multimodal_model and get_pp_group().is_first_rank:
+            # NOTE(woosuk): To unify token ids and soft tokens (vision
+            # embeddings), we always use embeddings (rather than token ids)
+            # as input to the multimodal model, even when the input is text.
+            input_ids = self.input_ids[:num_scheduled_tokens]
+            if mm_embeds:
+                inputs_embeds = self.model.get_input_embeddings(
+                    input_ids, mm_embeds)
+            else:
+                inputs_embeds = self.model.get_input_embeddings(input_ids)
+            # TODO(woosuk): Avoid the copy. Optimize.
+            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)
+            inputs_embeds = self.inputs_embeds[:num_input_tokens]
+            input_ids = None
+        else:
+            # For text-only models, we use token ids as input.
+            # While it is possible to use embeddings as input just like the
+            # multimodal models, it is not desirable for performance since
+            # then the embedding layer is not included in the CUDA graph.
+            input_ids = self.input_ids[:num_input_tokens]
+            inputs_embeds = None
+        if self.uses_mrope:
+            positions = self.mrope_positions[:, :num_input_tokens]
+        else:
+            positions = self.positions[:num_input_tokens]
+
+        if get_pp_group().is_first_rank:
+            intermediate_tensors = None
+        else:
+            intermediate_tensors = self.sync_and_slice_intermediate_tensors(
+                num_input_tokens, intermediate_tensors, True)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: v1 support full graph, offline benchmark
+        '''
+        # Only the following conditions are met, use decode full graph.
+        # 1. decode only
+        # 2. decode actual token in batch capture list
+        # 3. no pad behavior
+        decode_only = common_metadata.is_decode_only
+        captured_already = (num_input_tokens // (K + 1)) in self.graph_runners
+        input_tokens = (inputs_embeds.shape[0] if self.is_multimodal_model
+                        else input_ids.shape[0])
+        # no_pad = num_scheduled_tokens == input_tokens
+        use_full_graph = VLLM_V1_USE_FULL_GRAPH and self.use_cuda_graph
+        if decode_only:
+            # If the model is decode only, we can use full graph.
+            use_full_graph = use_full_graph and captured_already
+            # Update attn_metadata for full graph.
+            if use_full_graph and num_input_tokens != num_scheduled_tokens:
+                common_metadata = get_common_metadata_from_attn_metadata(attn_metadata)
+                for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                        self.kv_cache_config.kv_cache_groups):
+                    block_table = self.input_batch.block_table[kv_cache_group_id]
+                    first_layer_name = kv_cache_group_spec.layer_names[0]
+                    attn_metadata_i = attn_metadata[first_layer_name]
+                    num_reqs = self.input_batch.num_reqs
+                    num_padded_reqs = self.vllm_config.pad_for_cudagraph(num_reqs)
+                    pad_attn_metadata(
+                        attn_metadata_i, common_metadata, block_table,
+                        self, num_scheduled_tokens, num_input_tokens,
+                        num_reqs, num_padded_reqs,
+                    )
+        elif common_metadata.is_prefill_only:
+            num_reqs = self.input_batch.num_reqs
+            batch_size_equal = num_reqs == self.prefill_mlugraph_batch_size
+            seq_lens_equal = np.all(
+                self.seq_lens_np[:num_reqs] == self.prefill_mlugraph_seq_len)
+            use_full_graph = (use_full_graph and self.prefill_enable_mlugraph
+                              and batch_size_equal and seq_lens_equal)
+        else:
+            # can not use full graph when chunked
+            use_full_graph = False
+
+        if (VLLM_GRAPH_DEBUG and VLLM_V1_USE_FULL_GRAPH
+                and self.use_cuda_graph and decode_only and not use_full_graph):
+            logger.warning_once(
+                f"Select MLU-V1 Full-MLUGraph mode, however running in eager mode.\n"
+                f"- The scheduled batch number {num_scheduled_tokens // (K + 1)} is "
+                f"larger than the max capturer size {self.cudagraph_batch_sizes[-1]}.\n"
+                f"- Please adapt batch_capture_list by "
+                f"'export MLU_GRAPH_CAPTURE_LIST={num_scheduled_tokens},...'.\n"
+                f"Disable this warning by 'export MLU_GRAPH_DEBUG=0'."
+            )
+
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN and not use_full_graph:
+            start = torch.mlu.Event(enable_timing=True)
+            start.record()
+
+        # Run the decoder.
+        # Use persistent buffers for CUDA graphs.
+        with set_forward_context(attn_metadata,
+                                 self.vllm_config,
+                                 num_tokens=num_input_tokens,
+                                 num_tokens_across_dp=num_tokens_across_dp):
+            self.maybe_setup_kv_connector(scheduler_output)
+
+            if use_full_graph:
+                batch_size = input_tokens
+                if hasattr(self, "drafter"):
+                    assert input_tokens % (1 + self.num_speculative_tokens) == 0, \
+                        "input_tokens should be multiple of (1 + num_speculative_tokens)."
+                    batch_size = input_tokens // (1 + self.num_speculative_tokens)
+                model_graph = (self.graph_runners[batch_size]
+                               if decode_only else self.context_graph_runner)
+                model_output = model_graph(
+                    input_ids=input_ids,
+                    positions=positions,
+                    intermediate_tensors=intermediate_tensors,
+                    inputs_embeds=inputs_embeds,
+                )
+            else:
+                model_output = self.model(
+                    input_ids=input_ids,
+                    positions=positions,
+                    intermediate_tensors=intermediate_tensors,
+                    inputs_embeds=inputs_embeds,
+                )
+
+            self.maybe_wait_for_kv_save()
+            finished_sending, finished_recving = (
+                self.get_finished_kv_transfers(scheduler_output))
+
+        if self.use_aux_hidden_state_outputs:
+            hidden_states, aux_hidden_states = model_output
+        else:
+            hidden_states = model_output
+        # Broadcast PP output for external_launcher (torchrun)
+        # to make sure we are synced across pp ranks
+        # TODO: Support overlapping mirco-batches
+        # https://github.com/vllm-project/vllm/issues/18019
+        broadcast_pp_output = \
+            self.parallel_config.distributed_executor_backend \
+            == "external_launcher" and len(get_pp_group().ranks) > 0
+        if not get_pp_group().is_last_rank:
+            # For mid-pipeline stages, return the hidden states.
+            if not broadcast_pp_output:
+                return hidden_states
+            assert isinstance(hidden_states, IntermediateTensors)
+            get_pp_group().send_tensor_dict(hidden_states.tensors,
+                                            all_gather_group=get_tp_group())
+            logits = None
+        else:
+            sample_hidden_states = hidden_states[logits_indices]
+            logits = self.model.compute_logits(sample_hidden_states, None)
+        if broadcast_pp_output:
+            model_output_broadcast_data = {
+                "logits": logits.contiguous(),
+            } if logits is not None else {}
+            model_output_broadcast_data = get_pp_group().broadcast_tensor_dict(
+                model_output_broadcast_data, src=len(get_pp_group().ranks) - 1)
+            assert model_output_broadcast_data is not None
+            logits = model_output_broadcast_data["logits"]
+
+        self.time_markers = []
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+            end = torch.mlu.Event(enable_timing=True)
+            end.record()
+            if use_full_graph:
+                self.time_markers.append([model_graph.start, end])
+            else:
+                self.time_markers.append([start, end])
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        # Apply structured output bitmasks if present
+        if scheduler_output.grammar_bitmask is not None:
+            self.apply_grammar_bitmask(scheduler_output, logits)
+
+        # Sample the next token and get logprobs if needed.
+        sampling_metadata = self.input_batch.sampling_metadata
+        if spec_decode_metadata is None:
+            sampler_output = self.sampler(
+                logits=logits,
+                sampling_metadata=sampling_metadata,
+            )
+        else:
+            # When indexing with a tensor (bonus_logits_indices), PyTorch
+            # creates a new tensor with separate storage from the original
+            # logits tensor. This means any in-place operations on bonus_logits
+            # won't affect the original logits tensor.
+            assert logits is not None
+            bonus_logits = logits[spec_decode_metadata.bonus_logits_indices]
+            sampler_output = self.sampler(
+                logits=bonus_logits,
+                sampling_metadata=sampling_metadata,
+            )
+            bonus_token_ids = sampler_output.sampled_token_ids
+
+            # Just like `bonus_logits`, `target_logits` is a new tensor with
+            # separate storage from the original `logits` tensor. Therefore,
+            # it is safe to update `target_logits` in place.
+            target_logits = logits[spec_decode_metadata.target_logits_indices]
+            output_token_ids = self.rejection_sampler(
+                spec_decode_metadata,
+                None,  # draft_probs
+                target_logits,
+                bonus_token_ids,
+                sampling_metadata,
+            )
+            sampler_output.sampled_token_ids = output_token_ids
+
+        # TODO(woosuk): The following loop can be slow since it iterates over
+        # the requests one by one. Optimize.
+        discard_sampled_tokens_req_indices = []
+        for i, req_id in enumerate(self.input_batch.req_ids):
+            req_state = self.requests[req_id]
+            seq_len = (req_state.num_computed_tokens +
+                       scheduler_output.num_scheduled_tokens[req_id])
+            if seq_len < req_state.num_tokens:
+                # Ignore the sampled token for partial prefills.
+                # Rewind the generator state as if the token was not sampled.
+                # This relies on cuda-specific torch-internal impl details
+                generator = self.input_batch.generators.get(i)
+                if generator is not None:
+                    generator.set_offset(generator.get_offset() - 4)
+                # Record the index of the request that should not be sampled,
+                # so that we could clear the sampled tokens before returning.
+                discard_sampled_tokens_req_indices.append(i)
+
+        # NOTE: GPU -> CPU Sync happens here.
+        # Move as many CPU operations as possible before this sync point.
+        logprobs_tensors = sampler_output.logprobs_tensors
+        logprobs_lists = logprobs_tensors.tolists() \
+            if logprobs_tensors is not None else None
+
+        # Compute prompt logprobs if needed.
+        prompt_logprobs_dict = self._get_prompt_logprobs_dict(
+            hidden_states[:num_scheduled_tokens],
+            scheduler_output,
+        )
+
+        # Get the valid generated tokens.
+        sampled_token_ids = sampler_output.sampled_token_ids
+        max_gen_len = sampled_token_ids.shape[-1]
+        if max_gen_len == 1:
+            # No spec decode tokens.
+            valid_sampled_token_ids = sampled_token_ids.tolist()
+        else:
+            # Includes spec decode tokens.
+            valid_sampled_token_ids = self.rejection_sampler.parse_output(
+                sampled_token_ids,
+                self.input_batch.vocab_size,
+            )
+        # Mask out the sampled tokens that should not be sampled.
+        for i in discard_sampled_tokens_req_indices:
+            valid_sampled_token_ids[i].clear()
+
+        if not self.speculative_config:
+            # Speculative decoding is not enabled.
+            spec_token_ids = None
+        elif self.speculative_config.method == "ngram":
+            assert isinstance(self.drafter, NgramProposer)
+            spec_token_ids = self.generate_draft_token_ids(
+                valid_sampled_token_ids, sampling_metadata)
+        elif self.speculative_config.method == "medusa":
+            assert isinstance(self.drafter, MedusaProposer)
+            if max_gen_len == 1:
+                hidden_states = sample_hidden_states
+            else:
+                indices = []
+                offset = 0
+                for num_draft, tokens in zip(
+                        spec_decode_metadata.num_draft_tokens,
+                        valid_sampled_token_ids):
+                    indices.append(offset + len(tokens) - 1)
+                    offset += num_draft + 1
+
+                indices = torch.tensor(indices,
+                                       device=sample_hidden_states.device)
+                hidden_states = sample_hidden_states[indices]
+
+            spec_token_ids = self.drafter.propose(
+                target_hidden_states=hidden_states,
+                sampling_metadata=sampling_metadata,
+            )
+        elif self.speculative_config.use_eagle():
+            assert isinstance(self.drafter, EagleProposer)
+            # TODO(woosuk): Refactor the loop.
+            next_token_ids: list[int] = []
+            for i, token_ids in enumerate(valid_sampled_token_ids):
+                if token_ids:
+                    # Common case.
+                    next_token_id = token_ids[-1]
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: cache output token for spec_decode,
+                        the final token will cache in next step _update_states
+                    '''
+                    req_id = self.input_batch.req_ids[i]
+                    req_state = self.requests[req_id]
+                    req_state.output_token_ids.extend(token_ids[:-1])
+                    '''
+                    ==================
+                    End of MLU Hijack
+                    ==================
+                    '''
+                else:
+                    # Partial prefill (rare case).
+                    # Get the next token id from the request state.
+                    req_id = self.input_batch.req_ids[i]
+                    req_state = self.requests[req_id]
+                    seq_len = (req_state.num_computed_tokens +
+                               scheduler_output.num_scheduled_tokens[req_id])
+                    next_token_id = req_state.get_token_id(seq_len)
+                next_token_ids.append(next_token_id)
+            next_token_ids = torch.tensor(next_token_ids,
+                                          dtype=torch.int32,
+                                          device=self.device)
+            # At this moment, we assume all eagle layers belong to the same KV
+            # cache group, thus using the same attention metadata.
+            eagle_attn_metadata = attn_metadata[
+                self.drafter.attn_layer_names[0]]
+
+            # NOTE: deepseek_mtp uses MLA which does not have `block_table`
+            if hasattr(eagle_attn_metadata, "block_table"):
+                block_table = eagle_attn_metadata.block_table
+            else:
+                block_table = None
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            '''
+            num_rejected_tokens_tensor = None
+            token_indices = None
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            if spec_decode_metadata is None:
+                # input_ids can be None for multimodal models.
+                target_token_ids = self.input_ids[:num_scheduled_tokens]
+                target_positions = positions[:num_scheduled_tokens]
+                if self.use_aux_hidden_state_outputs:
+                    target_hidden_states = torch.cat(
+                        [h[:num_scheduled_tokens] for h in aux_hidden_states],
+                        dim=-1)
+                else:
+                    target_hidden_states = hidden_states[:num_scheduled_tokens]
+                target_slot_mapping = eagle_attn_metadata.slot_mapping
+                cu_num_tokens = eagle_attn_metadata.query_start_loc
+            else:
+                # TODO(woosuk): Refactor this.
+                num_draft_tokens = spec_decode_metadata.num_draft_tokens
+                num_rejected_tokens = [
+                    n + 1 - len(valid_sampled_token_ids[i]) if n > 0 else 0
+                    for i, n in enumerate(num_draft_tokens)
+                ]
+                num_rejected_tokens_tensor = async_tensor_h2d(
+                    num_rejected_tokens,
+                    dtype=torch.int32,
+                    target_device=self.device,
+                    pin_memory=True)
+                num_tokens = num_scheduled_tokens - sum(num_rejected_tokens)
+                batch_size = self.input_batch.num_reqs
+                cu_num_tokens, token_indices = self.drafter.prepare_inputs(
+                    eagle_attn_metadata.query_start_loc[:batch_size + 1],
+                    num_rejected_tokens_tensor,
+                    num_tokens,
+                )
+                cu_num_tokens = eagle_attn_metadata.query_start_loc[:batch_size + 1]
+                target_token_ids = self.input_ids[token_indices]
+                target_positions = positions[token_indices]
+                if self.use_aux_hidden_state_outputs:
+                    target_hidden_states = torch.cat(
+                        [h[token_indices] for h in aux_hidden_states], dim=-1)
+                else:
+                    target_hidden_states = hidden_states[token_indices]
+                target_slot_mapping = eagle_attn_metadata.slot_mapping[
+                    token_indices]
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            '''
+            target_token_ids = self.input_ids[:num_scheduled_tokens]
+            target_positions = positions[:num_scheduled_tokens]
+            # hidden_states no need to be sliced
+            target_hidden_states = hidden_states[:num_scheduled_tokens]
+            target_slot_mapping = eagle_attn_metadata.slot_mapping[:num_scheduled_tokens]
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            draft_token_ids = self.drafter.propose(
+                target_token_ids=target_token_ids,
+                target_positions=target_positions,
+                target_hidden_states=target_hidden_states,
+                target_slot_mapping=target_slot_mapping,
+                next_token_ids=next_token_ids,
+                cu_num_tokens=cu_num_tokens,
+                block_table=block_table,
+                sampling_metadata=sampling_metadata,
+                main_model_common_metadata=common_metadata,
+                num_rejected_tokens=num_rejected_tokens_tensor,
+                token_indices=token_indices,
+                time_markers=self.time_markers,
+            )
+            spec_token_ids = draft_token_ids.tolist()
+
+        # Clear KVConnector state after all KVs are generated.
+        if has_kv_transfer_group():
+            get_kv_transfer_group().clear_connector_metadata()
+
+        return ModelRunnerOutput(
+            req_ids=self.input_batch.req_ids,
+            req_id_to_index=self.input_batch.req_id_to_index,
+            sampled_token_ids=valid_sampled_token_ids,
+            spec_token_ids=spec_token_ids,
+            logprobs=logprobs_lists,
+            prompt_logprobs_dict=prompt_logprobs_dict,
+            finished_sending=finished_sending,
+            finished_recving=finished_recving,
+        )
+
+    def apply_grammar_bitmask(
+        self,
+        scheduler_output: "SchedulerOutput",
+        logits: torch.Tensor,
+    ):
+        grammar_bitmask = scheduler_output.grammar_bitmask
+        if grammar_bitmask is None:
+            return
+
+        # We receive the structured output bitmask from the scheduler,
+        # compacted to contain bitmasks only for structured output requests.
+        # The order of the requests in the bitmask is not guaranteed to be the
+        # same as the order of the requests in the gpu runner's batch. We need
+        # to sort the bitmask to match the order of the requests used here.
+
+        # Get the batch indices of the structured output requests.
+        # Keep track of the number of speculative tokens scheduled for every
+        # request in the batch, as the logit indices are offset by this amount.
+        struct_out_req_batch_indices: dict[str, int] = {}
+        cumulative_offset = 0
+        seq = sorted(self.input_batch.req_id_to_index.items(),
+                     key=lambda x: x[1])
+        for req_id, batch_index in seq:
+            logit_index = batch_index + cumulative_offset
+            cumulative_offset += len(
+                scheduler_output.scheduled_spec_decode_tokens.get(req_id, []))
+            if req_id in scheduler_output.structured_output_request_ids:
+                struct_out_req_batch_indices[req_id] = logit_index
+
+        out_indices = []
+
+        # Reorder the bitmask to match the order of the requests in the batch.
+        sorted_bitmask = np.zeros_like(grammar_bitmask,
+                                       shape=(logits.shape[0],
+                                              grammar_bitmask.shape[1]))
+        cumulative_index = 0
+        seq = sorted(scheduler_output.structured_output_request_ids.items(),
+                     key=lambda x: x[1])
+        for req_id, _ in seq:
+            logit_index = struct_out_req_batch_indices[req_id]
+            num_spec_tokens = len(
+                scheduler_output.scheduled_spec_decode_tokens.get(req_id, []))
+            for i in range(1 + num_spec_tokens):
+                sorted_bitmask[logit_index + i] = \
+                    grammar_bitmask[cumulative_index + i]
+                out_indices.append(logit_index + i)
+            cumulative_index += 1 + num_spec_tokens
+        grammar_bitmask = sorted_bitmask
+
+        # Serialization of np.ndarray is much more efficient than a tensor,
+        # so we receive it in that format.
+        grammar_bitmask = torch.from_numpy(grammar_bitmask)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: remove index_put_ from inductor lowering denylist to
+        avoid torch.compile error when using xgrammar
+        '''
+        from torch_mlu._inductor import remove_from_lowering_denylist
+        remove_from_lowering_denylist([torch.ops.aten.index_put_])
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        
+        xgr.apply_token_bitmask_inplace(
+            logits,
+            grammar_bitmask.to(self.device, non_blocking=True),
+            indices=out_indices,
+        )
+
+    def generate_draft_token_ids(
+        self,
+        sampled_token_ids: list[list[int]],
+        sampling_metadata: SamplingMetadata,
+    ) -> list[list[int]]:
+        # TODO(woosuk): Optimize.
+        draft_token_ids: list[list[int]] = []
+        for i, sampled_ids in enumerate(sampled_token_ids):
+            num_sampled_ids = len(sampled_ids)
+            if not num_sampled_ids:
+                # Skip speculative decoding.
+                draft_token_ids.append([])
+                continue
+
+            # Skip requests that require sampling parameters that are not
+            # supported with speculative decoding.
+            req_id = self.input_batch.req_ids[i]
+            if not is_spec_decode_supported(req_id, self.input_batch):
+                draft_token_ids.append([])
+                continue
+
+            # Add sampled_token_ids to token_ids_cpu.
+            start_idx = self.input_batch.num_tokens_no_spec[i]
+            end_idx = start_idx + num_sampled_ids
+            if end_idx >= self.max_model_len:
+                # Skip requests that have already reached the max model length.
+                draft_token_ids.append([])
+                continue
+
+            self.input_batch.token_ids_cpu[i, start_idx:end_idx] = sampled_ids
+            drafter_output = self.drafter.propose(
+                self.input_batch.token_ids_cpu[i, :end_idx])
+            if drafter_output is None or len(drafter_output) == 0:
+                draft_token_ids.append([])
+            else:
+                draft_token_ids.append(drafter_output.tolist())
+        return draft_token_ids
+
+    def load_model(self) -> None:
+        logger.info("Starting to load model %s...", self.model_config.model)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: 
+        1. Set max_batched_token for SparseMoeMlp when enable avg moe.
+        2. modify rope's max_position_embeddings to max_model_len.
+        Those MUST be set before init model.
+        '''
+        if VLLM_AVG_MOE_EN:
+            logger.warning("Inference with Moe avg dispatch, "
+                "it's only for deepseek-v3/r1 model's performance test,"
+                " and will result in precision anomalies. Be careful!")
+            SparseMoeMlp.max_batched_token = max(self.model_config.max_model_len,
+                                                 self.scheduler_config.max_num_batched_tokens)
+        MLURotaryEmbedding.max_seq_len = self.model_config.max_model_len
+        MLURotaryEmbedding.max_model_len = self.model_config.max_model_len
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        with DeviceMemoryProfiler() as m:  # noqa: SIM117
+            time_before_load = time.perf_counter()
+            model_loader = get_model_loader(self.load_config)
+            if not hasattr(self, "model"):
+                logger.info("Loading model from scratch...")
+                self.model = model_loader.load_model(
+                    vllm_config=self.vllm_config,
+                    model_config=self.model_config)
+            else:
+                logger.info(
+                    "Model was already initialized. Loading weights inplace..."
+                )
+                model_loader.load_weights(self.model,
+                                          model_config=self.model_config)
+            if self.lora_config:
+                self.model = self.load_lora_model(self.model,
+                                                  self.model_config,
+                                                  self.scheduler_config,
+                                                  self.lora_config,
+                                                  self.device)
+            if hasattr(self, "drafter"):
+                logger.info("Loading drafter model...")
+                self.drafter.load_model(self.model)
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: Apply forward prehook to draft model.
+                '''
+                self.drafter.model.register_forward_pre_hook(_model_forward_pre_hook, with_kwargs=True)
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+            if self.use_aux_hidden_state_outputs:
+                self.model.set_aux_hidden_state_layers(
+                    self.model.get_eagle3_aux_hidden_state_layers())
+            time_after_load = time.perf_counter()
+        self.model_memory_usage = m.consumed_memory
+        logger.info("Model loading took %.4f GiB and %.6f seconds",
+                    self.model_memory_usage / GiB_bytes,
+                    time_after_load - time_before_load)
+        prepare_communication_buffer_for_model(self.model)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: register model pre forward for rope optimization
+        '''
+        self.model.register_forward_pre_hook(_model_forward_pre_hook, with_kwargs=True)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def _get_prompt_logprobs_dict(
+        self,
+        hidden_states: torch.Tensor,
+        scheduler_output: "SchedulerOutput",
+    ) -> dict[str, Optional[LogprobsTensors]]:
+        num_prompt_logprobs_dict = self.input_batch.num_prompt_logprobs
+        if not num_prompt_logprobs_dict:
+            return {}
+
+        in_progress_dict = self.input_batch.in_progress_prompt_logprobs_cpu
+        prompt_logprobs_dict: dict[str, Optional[LogprobsTensors]] = {}
+
+        # Since prompt logprobs are a rare feature, prioritize simple,
+        # maintainable loop over optimal performance.
+        completed_prefill_reqs = []
+        for req_id, num_prompt_logprobs in num_prompt_logprobs_dict.items():
+
+            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
+
+            # Get metadata for this request.
+            request = self.requests[req_id]
+            num_prompt_tokens = len(request.prompt_token_ids)
+            prompt_token_ids = torch.tensor(request.prompt_token_ids).to(
+                self.device, non_blocking=True)
+
+            # Set up target LogprobsTensors object.
+            logprobs_tensors = in_progress_dict.get(req_id)
+            if not logprobs_tensors:
+                # Create empty logprobs CPU tensors for the entire prompt.
+                # If chunked, we'll copy in slice by slice.
+                logprobs_tensors = LogprobsTensors.empty_cpu(
+                    num_prompt_tokens - 1, num_prompt_logprobs + 1)
+                in_progress_dict[req_id] = logprobs_tensors
+
+            # Determine number of logits to retrieve.
+            start_idx = request.num_computed_tokens
+            start_tok = start_idx + 1
+            num_remaining_tokens = num_prompt_tokens - start_tok
+            if num_tokens <= num_remaining_tokens:
+                # This is a chunk, more tokens remain.
+                # In the == case, there are no more prompt logprobs to produce
+                # but we want to defer returning them to the next step where we
+                # have new generated tokens to return.
+                num_logits = num_tokens
+            else:
+                # This is the last chunk of prompt tokens to return.
+                num_logits = num_remaining_tokens
+                completed_prefill_reqs.append(req_id)
+                prompt_logprobs_dict[req_id] = logprobs_tensors
+
+            if num_logits <= 0:
+                # This can happen for the final chunk if we prefilled exactly
+                # (num_prompt_tokens - 1) tokens for this request in the prior
+                # step. There are no more prompt logprobs to produce.
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: remove the prompt_logprobs for final chunk request
+                '''
+                del prompt_logprobs_dict[req_id]
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                continue
+
+            # Get the logits corresponding to this req's prompt tokens.
+            # If this is a partial request (i.e. chunked prefill),
+            # then there is prompt logprob generated for each index.
+            req_idx = self.input_batch.req_id_to_index[req_id]
+            offset = self.query_start_loc_np[req_idx].item()
+            prompt_hidden_states = hidden_states[offset:offset + num_logits]
+            logits = self.model.compute_logits(prompt_hidden_states, None)
+
+            # Get the "target" tokens for each index. For prompt at index i,
+            # the token at prompt index i+1 is the "sampled" token we want
+            # to gather the logprob for.
+            tgt_token_ids = prompt_token_ids[start_tok:start_tok + num_logits]
+
+            # Compute prompt logprobs.
+            logprobs = self.sampler.compute_logprobs(logits)
+            token_ids, logprobs, ranks = self.sampler.gather_logprobs(
+                logprobs, num_prompt_logprobs, tgt_token_ids)
+
+            # Transfer GPU->CPU async.
+            chunk_slice = slice(start_idx, start_idx + num_logits)
+            logprobs_tensors.logprob_token_ids[chunk_slice].copy_(
+                token_ids, non_blocking=True)
+            logprobs_tensors.logprobs[chunk_slice].copy_(logprobs,
+                                                         non_blocking=True)
+            logprobs_tensors.selected_token_ranks[chunk_slice].copy_(
+                ranks, non_blocking=True)
+
+        # Remove requests that have completed prefill from the batch
+        # num_prompt_logprobs_dict.
+        for req_id in completed_prefill_reqs:
+            del num_prompt_logprobs_dict[req_id]
+            del in_progress_dict[req_id]
+
+        # Must synchronize the non-blocking GPU->CPU transfers.
+        if prompt_logprobs_dict:
+            self._sync_device()
+
+        return prompt_logprobs_dict
+
+    @torch.inference_mode()
+    def _dummy_run(
+        self,
+        num_tokens: int,
+        skip_attn: bool = True,
+    ) -> torch.Tensor:
+
+        # Padding for DP
+        num_pad, num_tokens_across_dp = self.get_dp_padding(num_tokens)
+        num_tokens += num_pad
+
+        # Set num_scheduled_tokens based on num_tokens and max_num_seqs
+        # for dummy run with LoRA so that the num_reqs collectively
+        # has num_tokens in total.
+        assert num_tokens <= self.scheduler_config.max_num_batched_tokens
+        max_num_reqs = self.scheduler_config.max_num_seqs
+        num_reqs = min(num_tokens, max_num_reqs)
+        min_tokens_per_req = num_tokens // num_reqs
+        num_scheduled_tokens_list = [min_tokens_per_req] * num_reqs
+        num_scheduled_tokens_list[-1] += num_tokens % num_reqs
+        assert sum(num_scheduled_tokens_list) == num_tokens
+        assert len(num_scheduled_tokens_list) == num_reqs
+        num_scheduled_tokens = np.array(num_scheduled_tokens_list,
+                                        dtype=np.int32)
+
+        if skip_attn:
+            attn_metadata: Optional[dict[str, Any]] = None
+        else:
+            query_start_loc = self.query_start_loc[:num_reqs + 1]
+            # Make sure max_model_len is used at the graph capture time.
+            self.seq_lens_np[:num_reqs] = self.max_model_len
+            self.seq_lens_np[num_reqs:] = 0
+            self.seq_lens[:num_reqs].copy_(self.seq_lens_cpu[:num_reqs],
+                                           non_blocking=True)
+            seq_lens = self.seq_lens[:num_reqs]
+            seq_start_loc = self.seq_start_loc[:num_reqs + 1]
+
+            is_start_loc_match = (self.query_start_loc_cpu[num_reqs].item()
+                                  == self.seq_start_loc_cpu[num_reqs].item())
+            common_attn_metadata = MLUCommonAttentionMetadata.build(
+                query_start_loc=query_start_loc,
+                seq_lens=seq_lens,
+                seq_start_loc=seq_start_loc,
+                is_start_loc_match=is_start_loc_match,
+                max_query_len=num_tokens,
+                num_actual_tokens=num_tokens,
+                num_speculative_tokens=self.num_speculative_tokens,
+            )
+
+            attn_metadata = {}
+            for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                    self.kv_cache_config.kv_cache_groups):
+                attn_metadata_i = (
+                    self.attn_metadata_builders[kv_cache_group_id].build(
+                        num_reqs=num_reqs,
+                        num_actual_tokens=num_tokens,
+                        max_query_len=num_tokens,
+                        common_prefix_len=0,
+                        common_attn_metadata=common_attn_metadata,
+                    ))
+                for layer_name in kv_cache_group_spec.layer_names:
+                    attn_metadata[layer_name] = attn_metadata_i
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: bind decode_attn metadata to prefill_attn
+                    '''
+                    if (self.model_config.is_deepseek_mla
+                            and layer_name.endswith("self_attn.mla_attn")):
+                        prefill_attn_name = layer_name.replace(
+                            "self_attn.mla_attn", "self_attn.attn")
+                        attn_metadata[prefill_attn_name] = attn_metadata_i
+                    '''
+                    ==================
+                    End of MLU Hijack
+                    ==================
+                    '''
+            attn_metadata[COMMON_METADATA_STR] = common_attn_metadata
+
+        with self.maybe_dummy_run_with_lora(self.lora_config,
+                                            num_scheduled_tokens):
+            model = self.model
+            if self.is_multimodal_model:
+                input_ids = None
+                inputs_embeds = self.inputs_embeds[:num_tokens]
+            else:
+                input_ids = self.input_ids[:num_tokens]
+                inputs_embeds = None
+            if self.uses_mrope:
+                positions = self.mrope_positions[:, :num_tokens]
+            else:
+                positions = self.positions[:num_tokens]
+
+            if get_pp_group().is_first_rank:
+                intermediate_tensors = None
+            else:
+                if self.intermediate_tensors is None:
+                    self.intermediate_tensors = (
+                        self.model.make_empty_intermediate_tensors(
+                            batch_size=self.max_num_tokens,
+                            dtype=self.model_config.dtype,
+                            device=self.device))
+
+                intermediate_tensors = self.sync_and_slice_intermediate_tensors(
+                    num_tokens, None, False)
+
+            with self.maybe_randomize_inputs(input_ids), set_forward_context(
+                    attn_metadata,
+                    self.vllm_config,
+                    num_tokens=num_tokens,
+                    num_tokens_across_dp=num_tokens_across_dp):
+                outputs = model(
+                    input_ids=input_ids,
+                    positions=positions,
+                    intermediate_tensors=intermediate_tensors,
+                    inputs_embeds=inputs_embeds,
+                )
+            if self.use_aux_hidden_state_outputs:
+                hidden_states, _ = outputs
+            else:
+                hidden_states = outputs
+
+            if self.speculative_config and self.speculative_config.use_eagle():
+                assert isinstance(self.drafter, EagleProposer)
+                self.drafter.dummy_run(num_tokens)
+
+        logit_indices = np.cumsum(num_scheduled_tokens) - 1
+        return hidden_states[logit_indices]
+
+    def profile_run(self) -> None:
+        # Profile with multimodal encoder & encoder cache.
+        # TODO: handle encoder-decoder models once we support them.
+        if (self.is_multimodal_model and self.max_num_encoder_input_tokens > 0
+                and self.encoder_cache_size > 0):
+
+            # NOTE: Currently model is profiled with a single non-text
+            # modality with the max possible input tokens even when
+            # it supports multiple.
+            max_tokens_by_modality_dict = self.mm_registry \
+                .get_max_tokens_per_item_by_nonzero_modality(self.model_config)
+            dummy_data_modality, max_tokens_per_mm_item = max(
+                max_tokens_by_modality_dict.items(), key=lambda item: item[1])
+
+            # Check how many items of this modality can be supported by
+            # the encoder budget.
+            encoder_budget = min(self.max_num_encoder_input_tokens,
+                                 self.encoder_cache_size)
+
+            max_num_mm_items_encoder_budget = cdiv(encoder_budget,
+                                                   max_tokens_per_mm_item)
+
+            # Check how many items of this modality can be supported by
+            # the decoder budget.
+            max_mm_items_per_req = self.mm_registry.get_mm_limits_per_prompt(
+                self.model_config)[dummy_data_modality]
+
+            # NOTE: We do not consider max_num_batched_tokens on purpose
+            # because the multimodal embeddings can be generated in advance
+            # and chunked prefilled.
+            max_num_mm_items_decoder_budget = self.max_num_reqs * \
+                max_mm_items_per_req
+
+            max_num_mm_items = min(max_num_mm_items_encoder_budget,
+                                   max_num_mm_items_decoder_budget)
+
+            logger.info(
+                "Encoder cache will be initialized with a budget of %s tokens,"
+                " and profiled with %s %s items of the maximum feature size.",
+                encoder_budget, max_num_mm_items, dummy_data_modality)
+
+            # Create dummy batch of multimodal inputs.
+            dummy_mm_kwargs = self.mm_registry.get_decoder_dummy_data(
+                model_config=self.model_config,
+                seq_len=self.max_num_tokens,
+                mm_counts={
+                    dummy_data_modality: 1
+                },
+            ).multi_modal_data
+
+            batched_dummy_mm_inputs = MultiModalKwargs.batch(
+                [dummy_mm_kwargs] * max_num_mm_items,
+                pin_memory=self.pin_memory)
+            batched_dummy_mm_inputs = MultiModalKwargs.as_kwargs(
+                batched_dummy_mm_inputs,
+                device=self.device,
+            )
+
+            # Run multimodal encoder.
+            dummy_encoder_outputs = self.model.get_multimodal_embeddings(
+                **batched_dummy_mm_inputs)
+
+            sanity_check_mm_encoder_outputs(
+                dummy_encoder_outputs,
+                expected_num_items=max_num_mm_items,
+            )
+
+            # Cache the dummy encoder outputs.
+            self.encoder_cache["tmp"] = dict(enumerate(dummy_encoder_outputs))
+
+        hidden_states = self._dummy_run(self.max_num_tokens)
+        if get_pp_group().is_last_rank:
+            sampler_output = self._dummy_sampler_run(hidden_states)
+        else:
+            sampler_output = None
+        self._sync_device()
+        del hidden_states, sampler_output
+        self.encoder_cache.clear()
+        gc.collect()
+
+    @contextmanager
+    def _maybe_capture_context_mlugraph(self, is_prefill: bool):
+        if not self.prefill_enable_mlugraph or not is_prefill:
+            yield
+        else:
+            # __enter__ code
+            batch_size = self.prefill_mlugraph_batch_size
+            seq_lens_np = self.seq_lens_np[:batch_size]
+            seq_lens_cpu = self.seq_lens_cpu[:batch_size]
+            seq_lens = self.seq_lens[:batch_size]
+            query_start_loc = self.query_start_loc[:batch_size + 1]
+            seq_start_loc = self.seq_start_loc[:batch_size + 1]
+
+            # must make these tensors correct to avoid core dump
+            # for flash attn kernel
+            seq_lens_np.fill(self.prefill_mlugraph_seq_len)
+            seq_lens_cpu.fill_(self.prefill_mlugraph_seq_len)
+            seq_lens.fill_(self.prefill_mlugraph_seq_len)
+            torch.cumsum(seq_lens,
+                         dim=0,
+                         dtype=query_start_loc.dtype,
+                         out=query_start_loc[1:])
+            torch.cumsum(seq_lens,
+                         dim=0,
+                         dtype=seq_start_loc.dtype,
+                         out=seq_start_loc[1:])
+
+            yield
+
+            # __exit__ code
+            seq_lens_np.fill(0)
+            seq_lens_cpu.fill_(0)
+            seq_lens.fill_(0)
+            query_start_loc.fill_(0)
+            seq_start_loc.fill_(0)
+
+    def _capture_full_graph(
+        self,
+        is_prefill: bool,
+        is_drafter: bool,
+        batch_size: int,
+        num_tokens: int,
+        stream: torch.mlu.Stream,
+        graph_runner: "MLUGraphRunner_V1",
+        dummy_lora_id: Optional[int] = None,
+        dummy_lora_request: Optional[LoRARequest] = None,
+    ) -> None:
+        with self._maybe_capture_context_mlugraph(is_prefill):
+            if self.is_multimodal_model:
+                input_ids = None
+                inputs_embeds = self.inputs_embeds[:num_tokens]
+            else:
+                input_ids = self.input_ids[:num_tokens]
+                inputs_embeds = None
+            if self.uses_mrope:
+                positions = self.mrope_positions[:, :num_tokens]
+            else:
+                positions = self.positions[:num_tokens]
+
+            if get_pp_group().is_first_rank:
+                intermediate_tensors = None
+            else:
+                if self.intermediate_tensors is None:
+                    self.intermediate_tensors = (
+                        self.model.make_empty_intermediate_tensors(
+                            batch_size=self.max_num_tokens,
+                            dtype=self.model_config.dtype,
+                            device=self.device))
+                intermediate_tensors = self.sync_and_slice_intermediate_tensors(
+                    num_tokens, None, False)
+
+            if self.lora_config:
+                lora_mapping = LoRAMapping(
+                    **dict(index_mapping=[dummy_lora_id] * num_tokens,
+                           prompt_mapping=[dummy_lora_id] * batch_size,
+                           is_prefill=is_prefill))
+                self.lora_manager.set_active_adapters(
+                    set([dummy_lora_request]), lora_mapping)
+
+            capture_inputs = {
+                "input_ids": input_ids,
+                "positions": positions,
+                "inputs_embeds": inputs_embeds,
+                "intermediate_tensors": intermediate_tensors,
+                "kv_caches": self.kv_caches,
+                "memory_pool": self.graph_memory_pool,
+                "stream": stream,
+            }
+            if is_drafter:
+                capture_inputs.update({
+                    "previous_hidden_states": self.previous_hidden_states[:num_tokens]
+                })
+
+            max_query_len = (self.prefill_mlugraph_seq_len
+                             if is_prefill else 1 + self.num_speculative_tokens)
+            self.seq_lens_np[:batch_size] = self.model_config.max_model_len
+            common_attn_metadata = MLUCommonAttentionMetadata.build(
+                query_start_loc=self.query_start_loc[:batch_size + 1],
+                seq_lens=self.seq_lens[:batch_size],
+                seq_start_loc=self.seq_start_loc[:batch_size + 1],
+                is_start_loc_match=is_prefill,
+                max_query_len=max_query_len,
+                num_actual_tokens=num_tokens,
+                num_speculative_tokens=self.num_speculative_tokens,
+            )
+            attn_metadata = {}
+            for kv_cache_group_id, kv_cache_group_spec in enumerate(
+                    self.kv_cache_config.kv_cache_groups):
+                if isinstance(self.attn_metadata_builders[kv_cache_group_id],
+                              FlashMLAMetadataBuilder):
+                    attn_metadata_i = (
+                        self.attn_metadata_builders[kv_cache_group_id].build_for_capture(
+                            num_reqs=batch_size,
+                            num_actual_tokens=num_tokens,
+                            max_query_len=max_query_len,
+                            common_prefix_len=0,
+                            common_attn_metadata=common_attn_metadata,
+                        ))
+                else:
+                    attn_metadata_i = (
+                        self.attn_metadata_builders[kv_cache_group_id].build(
+                            num_reqs=batch_size,
+                            num_actual_tokens=num_tokens,
+                            max_query_len=max_query_len,
+                            common_prefix_len=0,
+                            common_attn_metadata=common_attn_metadata,
+                        ))
+                attn_layer_names = (self.drafter.attn_layer_names
+                                    if is_drafter else kv_cache_group_spec.layer_names)
+                for layer_name in attn_layer_names:
+                    attn_metadata[layer_name] = attn_metadata_i
+                    if (self.model_config.is_deepseek_mla
+                            and layer_name.endswith("self_attn.mla_attn")):
+                        prefill_attn_name = layer_name.replace(
+                            "self_attn.mla_attn", "self_attn.attn")
+                        attn_metadata[prefill_attn_name] = attn_metadata_i
+            attn_metadata[COMMON_METADATA_STR] = common_attn_metadata
+
+            with set_forward_context(attn_metadata,
+                                     self.vllm_config,
+                                     num_tokens=num_tokens):
+                graph_runner.capture(**capture_inputs)
+
+            self.graph_memory_pool = graph_runner.graph.pool()
+
+    def capture_model(self) -> None:
+        if not self.use_cuda_graph:
+            logger.warning(
+                "Skipping MLU graph capture. Please add "
+                "-O %s to use MLU graphs.", CompilationLevel.PIECEWISE)
+            return
+
+        start_time = time.perf_counter()
+        start_free_gpu_memory = torch.mlu.mem_get_info()[0]
+
+        # Trigger MLU graph capture for specific shapes.
+        # Capture the large shapes first so that the smaller shapes
+        # can reuse the memory pool allocated for the large shapes.
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: v1 support full graph
+        '''
+        if VLLM_V1_USE_FULL_GRAPH:
+            dummy_lora_id: Optional[int] = None
+            dummy_lora_request: Optional[LoRARequest] = None
+            if self.lora_config:
+                # The goal is to capture the LoRA kernels in cuda graphs.
+                # for this purpose, as single dummy lora is sufficient.
+                dummy_lora_requests = self.add_dummy_loras(num_loras=1)
+                assert len(dummy_lora_requests) == 1
+                dummy_lora_request = dummy_lora_requests[0]
+                dummy_lora_id = dummy_lora_request.lora_int_id
+            with mlu_graph_capture(device=self.device) as gc_ctx:
+                # capture context mlugraph
+                if self.prefill_enable_mlugraph:
+                    batch_size = self.prefill_mlugraph_batch_size
+                    seq_len = self.prefill_mlugraph_seq_len
+                    num_tokens = batch_size * seq_len
+                    assert num_tokens <= self.scheduler_config.max_num_batched_tokens
+                    assert batch_size <= self.scheduler_config.max_num_seqs
+                    logger.info("Capture context mlugraph for batch size "
+                                f"{batch_size} and seq len {seq_len}")
+                    graph_runner = MLUGraphRunner_V1(self.model, self)
+                    self._capture_full_graph(
+                        True, False, batch_size, num_tokens, gc_ctx.stream,
+                        graph_runner, dummy_lora_id, dummy_lora_request)
+                    self.context_graph_runner = graph_runner
+
+                # capture decode mlugraphs for draft model
+                if hasattr(self, "drafter") and isinstance(self.drafter, EagleProposer):
+                    cudagraph_batch_sizes = tqdm(
+                        list(reversed(self.cudagraph_batch_sizes)),
+                        desc="Capturing Draft Model FULL graph shapes",
+                    )
+                    draft_graph_runners: Dict[int, MLUGraphRunner_V1] = {}
+                    for batch_size in cudagraph_batch_sizes:
+                        # Only one extra token is needed for draft model.
+                        num_tokens = batch_size * (1 + self.num_speculative_tokens)
+                        graph_runner = MLUGraphRunner_V1(self.drafter.model, self)
+                        self._capture_full_graph(
+                            False, True, batch_size, num_tokens, gc_ctx.stream,
+                            graph_runner, dummy_lora_id, dummy_lora_request)
+                        draft_graph_runners[batch_size] = graph_runner
+                    self.drafter.draft_graph_runners = draft_graph_runners
+
+                # capture decode mlugraphs for main model
+                cudagraph_batch_sizes = tqdm(
+                    list(reversed(self.cudagraph_batch_sizes)),
+                    desc="Capturing FULL graph shapes",
+                )
+                for batch_size in cudagraph_batch_sizes:
+                    num_tokens = batch_size * (self.num_speculative_tokens + 1)
+                    assert num_tokens <= self.scheduler_config.max_num_batched_tokens
+                    assert batch_size <= self.scheduler_config.max_num_seqs
+                    graph_runner = MLUGraphRunner_V1(self.model, self)
+                    self._capture_full_graph(
+                        False, False, batch_size, num_tokens, gc_ctx.stream,
+                        graph_runner, dummy_lora_id, dummy_lora_request)
+                    self.graph_runners[batch_size] = graph_runner
+            if self.lora_config:
+                self.lora_manager.remove_all_adapters()
+        else:
+            with mlu_graph_capture(device=self.device):
+                skip_attn = not self.vllm_config.compilation_config.full_cuda_graph
+                cudagraph_batch_sizes = tqdm(
+                    self.cudagraph_batch_sizes,
+                    desc="Capturing PIECEWISE graph shapes",
+                )
+                for num_tokens in reversed(cudagraph_batch_sizes):
+                    for _ in range(self.vllm_config.compilation_config.
+                                   cudagraph_num_of_warmups):
+                        self._dummy_run(num_tokens, skip_attn=skip_attn)
+                    self._dummy_run(num_tokens, skip_attn=skip_attn)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        end_time = time.perf_counter()
+        end_free_gpu_memory = torch.mlu.mem_get_info()[0]
+        elapsed_time = end_time - start_time
+        cuda_graph_size = start_free_gpu_memory - end_free_gpu_memory
+        # This usually takes 5~20 seconds.
+        logger.info("Graph capturing finished in %.0f secs, took %.2f GiB",
+                    elapsed_time, cuda_graph_size / (1 << 30))
+
+    def initialize_attn_backend(self, kv_cache_config: KVCacheConfig) -> None:
+        """
+        Initialize the attention backends and attention metadata builders.
+        """
+        assert len(self.attn_backends) == 0 and len(
+            self.attn_metadata_builders
+        ) == 0, "Attention backends are already initialized"
+        for i, kv_cache_group_spec in enumerate(
+                kv_cache_config.kv_cache_groups):
+            kv_cache_spec = kv_cache_group_spec.kv_cache_spec
+            if not isinstance(kv_cache_spec, AttentionSpec):
+                raise NotImplementedError(
+                    "Only AttentionSpec is supported for now.")
+            attn_backend_i = get_attn_backend(
+                kv_cache_spec.head_size,
+                self.dtype,
+                kv_cache_spec.dtype,
+                kv_cache_spec.block_size,
+                self.model_config.is_attention_free,
+                use_mla=kv_cache_spec.use_mla,
+            )
+            if attn_backend_i is None:
+                error_msg = (
+                    f"Error with get_attn_backend: {kv_cache_spec.head_size=}, "
+                    f"{self.dtype=}, {kv_cache_spec.dtype=}, "
+                    f"{kv_cache_spec.block_size=}, "
+                    f"{self.model_config.is_attention_free=}, "
+                    f"{kv_cache_spec.use_mla=}")
+                logger.error(error_msg)
+                raise NotImplementedError(
+                    "Non-Attention backend is not supported by V1 "
+                    "GPUModelRunner.")
+
+            if self.vllm_config.compilation_config.full_cuda_graph:
+                attn_backend_name = attn_backend_i.__name__
+                flash_attn_version = get_flash_attn_version()
+                if attn_backend_name != "FlashAttentionBackend" or \
+                    flash_attn_version != 3:
+                    raise ValueError(
+                        f"full_cuda_graph is only supported with "
+                        f"FA3. Current attention backend is "
+                        f"{attn_backend_name}, FlashAttention version is "
+                        f"{flash_attn_version}.")
+
+            block_table_i = self.input_batch.block_table[i]
+            attn_metadata_builder_i = attn_backend_i.get_builder_cls()(
+                weakref.proxy(self), kv_cache_spec, block_table_i)
+            self.attn_backends.append(attn_backend_i)
+            self.attn_metadata_builders.append(attn_metadata_builder_i)
+
+    def _allocate_kv_cache_tensors(
+        self,
+        kv_cache_config: KVCacheConfig
+    ) -> dict[str, tuple[torch.Tensor, torch.Tensor]]:
+        """
+        Initializes the KV cache buffer with the correct size. The buffer needs
+        to be reshaped to the desired shape before being used by the models.
+
+        Args:
+            kv_cache_config: The KV cache config 
+        Returns:
+            dict[str, torch.Tensor]: A map between layer names to their 
+            corresponding memory buffer for KV cache.
+        """
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: support mlu kv8
+        '''
+        assert len(kv_cache_config.kv_cache_groups) == 1, \
+                f"MLU-V1 only support one kv_cache_group now."
+        kv_cache_group = kv_cache_config.kv_cache_groups[0]
+        kv_cache_raw_tensors: dict[str, tuple[torch.Tensor, torch.Tensor]] = {}
+        for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
+            kv_cache_spac = kv_cache_group.kv_cache_spec
+            assert isinstance(kv_cache_spac, AttentionSpec)
+            assert kv_cache_tensor.size % kv_cache_spac.page_size_bytes == 0
+            num_blocks = (kv_cache_tensor.size //
+                          kv_cache_spac.page_size_bytes)
+            cache_ = torch.zeros((num_blocks *
+                                  kv_cache_spac.cache_size_bytes),
+                                 dtype=torch.int8,
+                                 device=self.device)
+            if kv_cache_spac.dtype in [torch.int8, torch.uint8]:
+                scale_ = torch.zeros((num_blocks *
+                                      kv_cache_spac.scale_size_bytes),
+                                     dtype=torch.int8,
+                                     device=self.device)
+            else:
+                scale_ = torch.tensor([], dtype=torch.int8,
+                                      device=self.device)
+            tensor = [cache_, scale_]
+            for layer_name in kv_cache_tensor.shared_by:
+                kv_cache_raw_tensors[layer_name] = tensor
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        layer_names = set()
+        for group in kv_cache_config.kv_cache_groups:
+            layer_names.update(group.layer_names)
+        assert layer_names == set(kv_cache_raw_tensors.keys(
+        )), "Some layers are not correctly initialized"
+        return kv_cache_raw_tensors
+
+    def _reshape_kv_cache_tensors(
+        self,
+        kv_cache_config: KVCacheConfig,
+        kv_cache_raw_tensors: dict[str, tuple[torch.Tensor, torch.Tensor]],
+    ) -> dict[str, torch.Tensor]:
+        """
+        Reshape the KV cache tensors to the desired shape and dtype.
+
+        Args:
+            kv_cache_config: The KV cache config 
+            kv_cache_raw_tensors: The KV cache buffer of each layer, with 
+            correct size but uninitialized shape.
+        Returns:
+            Dict[str, torch.Tensor]: A map between layer names to their 
+            corresponding memory buffer for KV cache.
+        """
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: support mlu kv8
+        '''
+        kv_caches: dict[str, tuple[torch.Tensor, torch.Tensor]] = {}
+        for i, kv_cache_group_spec in enumerate(
+                kv_cache_config.kv_cache_groups):
+            kv_cache_spec = kv_cache_group_spec.kv_cache_spec
+            for layer_name in kv_cache_group_spec.layer_names:
+                raw_tensor = kv_cache_raw_tensors[layer_name]
+                cache_, scale_ = raw_tensor
+                total_numel = cache_.numel() + scale_.numel()
+                assert total_numel % kv_cache_spec.page_size_bytes == 0
+                num_blocks = (total_numel //
+                              kv_cache_spec.page_size_bytes)
+                if isinstance(kv_cache_spec, AttentionSpec):
+                    # reshape kv_cache
+                    kv_cache_shape = self.attn_backends[i].get_kv_cache_shape(
+                        num_blocks, kv_cache_spec.block_size,
+                        kv_cache_spec.num_kv_heads, kv_cache_spec.head_size)
+                    cache_ = cache_.view(kv_cache_spec.dtype).view(kv_cache_shape)
+                    # reshape kv_cache_scale
+                    if kv_cache_spec.dtype in [torch.int8, torch.uint8]:
+                        kv_cache_scale_shape = \
+                            self.attn_backends[i].get_kv_cache_scale_shape(
+                                num_blocks, kv_cache_spec.block_size,
+                                kv_cache_spec.num_kv_heads)
+                        scale_ = scale_.view(torch.float32).view(kv_cache_scale_shape)
+                    kv_caches[layer_name] = [cache_, scale_]
+                else:
+                    raise NotImplementedError
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return kv_caches
+
+    def initialize_kv_cache_tensors(
+            self, kv_cache_config: KVCacheConfig) -> dict[str, torch.Tensor]:
+        """
+        Initialize the memory buffer for KV cache.
+
+        Args:
+            kv_cache_config: The KV cache config
+        Returns:
+            Dict[str, torch.Tensor]: A map between layer names to their 
+            corresponding memory buffer for KV cache.
+        """
+        # Initialize the memory buffer for KV cache
+        kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
+        # Change the memory buffer to the desired shape
+        kv_caches = self._reshape_kv_cache_tensors(kv_cache_config,
+                                                   kv_cache_raw_tensors)
+
+        # Setup `kv_cache_config` and `kv_caches` for models
+        # with cross-layer KV sharing
+        if self.shared_kv_cache_layers:
+            initialize_kv_cache_for_kv_sharing(
+                self.shared_kv_cache_layers,
+                kv_cache_config.kv_cache_groups,
+                kv_caches,
+            )
+
+        bind_kv_cache(
+            kv_caches,
+            self.vllm_config.compilation_config.static_forward_context,
+            self.kv_caches)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: bind kv cache to deepseek prefill attn
+        '''
+        if self.model_config.is_deepseek_mla:
+            forward_context = self.vllm_config.compilation_config.static_forward_context
+            for layer_name, kv_cache in kv_caches.items():
+                if layer_name.endswith("self_attn.mla_attn"):
+                    layer_name = layer_name.replace(
+                        "self_attn.mla_attn", "self_attn.attn")
+                forward_context[layer_name].kv_cache = [kv_cache]
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        return kv_caches
+
+    def get_kv_cache_spec(self) -> dict[str, KVCacheSpec]:
+        """
+        Generates the KVCacheSpec by parsing the kv cache format from each
+        Attention module in the static forward context.
+        Returns:
+            KVCacheSpec: A dictionary mapping layer names to their KV cache
+            format. Layers that do not need KV cache are not included.
+        """
+
+        layers = get_layers_from_vllm_config(self.vllm_config, Attention)
+        block_size = self.vllm_config.cache_config.block_size
+        use_mla = self.vllm_config.model_config.use_mla
+        kv_cache_spec: dict[str, KVCacheSpec] = {}
+        for layer_name, attn_module in layers.items():
+            if (kv_tgt_layer :=
+                    attn_module.kv_sharing_target_layer_name) is not None:
+                # The layer doesn't need its own KV cache and will use that of
+                # the target layer. We skip creating a KVCacheSpec for it, so
+                # that KV cache management logic will act as this layer does
+                # not exist, and doesn't allocate KV cache for the layer. This
+                # enables the memory saving of cross-layer kv sharing, allowing
+                # a given amount of memory to accommodate longer context lengths
+                # or enable more requests to be processed simultaneously.
+                self.shared_kv_cache_layers[layer_name] = kv_tgt_layer
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: skip deepseek prefill attn init kv_cache
+            '''
+            if (self.model_config.is_deepseek_mla
+                    and layer_name.endswith("self_attn.attn")):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # TODO: Support other attention modules, e.g., cross-attention
+            if attn_module.attn_type == AttentionType.DECODER:
+                if attn_module.sliding_window is not None:
+                    kv_cache_spec[layer_name] = MLUSlidingWindowSpec(
+                        block_size=block_size,
+                        num_kv_heads=attn_module.num_kv_heads,
+                        head_size=attn_module.head_size,
+                        dtype=self.kv_cache_dtype,
+                        sliding_window=attn_module.sliding_window,
+                        use_mla=use_mla)
+                else:
+                    kv_cache_spec[layer_name] = MLUFullAttentionSpec(
+                        block_size=block_size,
+                        num_kv_heads=attn_module.num_kv_heads,
+                        head_size=attn_module.head_size,
+                        dtype=self.kv_cache_dtype,
+                        use_mla=use_mla)
+            elif attn_module.attn_type in (AttentionType.ENCODER,
+                                           AttentionType.ENCODER_ONLY):
+                # encoder-only attention does not need KV cache.
+                continue
+            elif attn_module.attn_type == AttentionType.ENCODER_DECODER:
+                raise NotImplementedError
+            else:
+                raise ValueError(
+                    f"Unknown attention type: {attn_module.attn_type}")
+
+        return kv_cache_spec
+
+    def reset_capture_context(self,
+                              prefill_enable_mlugraph: bool,
+                              batch_size: int,
+                              input_len: int):
+        self.graph_runners = {}
+        self.context_graph_runner = None
+        self.graph_memory_pool = None
+
+        # reset prefill mlugraph infos
+        self.prefill_enable_mlugraph = prefill_enable_mlugraph
+        self.prefill_mlugraph_batch_size = batch_size
+        self.prefill_mlugraph_seq_len = input_len
+
+        gc.collect()
+        torch.mlu.empty_cache()
+
+
+class MLUGraphRunner_V1(nn.Module):
+
+    def __init__(self, model: nn.Module, model_runner: MLUModelRunner):
+        super().__init__()
+        self.model = model
+        self.model_runner = model_runner
+
+        self.input_buffers: Dict[str, torch.Tensor] = {}
+        self.output_buffers: Dict[str, torch.Tensor] = {}
+
+        self._graph: Optional[torch.mlu.MLUGraph] = None
+
+    @property
+    def graph(self):
+        assert self._graph is not None
+        return self._graph
+
+    def capture(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        inputs_embeds: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        kv_caches: List[torch.Tensor],
+        memory_pool: Optional[Tuple[int, int]],
+        stream: torch.mlu.Stream,
+        **kwargs,
+    ):
+        assert self._graph is None
+        # Run the model a few times without capturing the graph.
+        # This is to make sure that the captured graph does not include the
+        # kernel launches for initial benchmarking (e.g., Triton autotune).
+        # Note one iteration is not enough for torch.compile
+        for _ in range(_NUM_WARMUP_ITERS):
+            self.model(
+                input_ids=input_ids,
+                positions=positions,
+                intermediate_tensors=intermediate_tensors,
+                inputs_embeds=inputs_embeds,
+                **kwargs,
+            )
+        # Wait for the warm up operations to finish before proceeding with
+        # Graph Capture.
+        torch.mlu.synchronize()
+        # Capture the graph.
+        self._graph = torch.mlu.MLUGraph()
+        with torch.mlu.graph(self._graph, pool=memory_pool, stream=stream):
+            output_hidden_or_intermediate_states = self.model(
+                input_ids=input_ids,
+                positions=positions,
+                intermediate_tensors=intermediate_tensors,
+                inputs_embeds=inputs_embeds,
+                **kwargs,
+            )
+
+            if isinstance(output_hidden_or_intermediate_states, torch.Tensor):
+                hidden_or_intermediate_states = weak_ref_tensor(
+                    output_hidden_or_intermediate_states)
+            elif isinstance(output_hidden_or_intermediate_states,
+                            IntermediateTensors):
+                hidden_or_intermediate_states = IntermediateTensors(
+                    tensors={
+                        key: weak_ref_tensor(value)
+                        for key, value in
+                        output_hidden_or_intermediate_states.tensors.items()
+                    })
+
+            del output_hidden_or_intermediate_states
+            # make sure `output_hidden_or_intermediate_states` is deleted
+            # in the graph's memory pool
+            gc.collect()
+        torch.mlu.synchronize()
+
+        common_metadata: MLUCommonAttentionMetadata = get_common_metadata()
+        num_reqs = common_metadata.num_actual_tokens
+        block_table: BlockTable = self.model_runner.input_batch.block_table[0]
+
+        # Save the input and output buffers.
+        self.input_buffers = {
+            "input_ids": input_ids,
+            "positions": positions,
+            "inputs_embeds": inputs_embeds,
+            "kv_caches": kv_caches,
+            "query_start_loc": self.model_runner.query_start_loc[:num_reqs + 1],
+            "seq_start_loc": self.model_runner.seq_start_loc[:num_reqs + 1],
+            "slot_mapping": self.model_runner.slot_mapping[:num_reqs],
+            "seq_lens": self.model_runner.seq_lens[:num_reqs],
+            "block_table": block_table.get_device_tensor()[:num_reqs],
+            **kwargs,
+        }
+
+        if intermediate_tensors is not None:
+            self.input_buffers.update(intermediate_tensors.tensors)
+        if get_pp_group().is_last_rank:
+            self.output_buffers = {
+                "hidden_states": hidden_or_intermediate_states
+            }
+        else:
+            self.output_buffers = hidden_or_intermediate_states
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor],
+        **kwargs,
+    ) -> torch.Tensor:
+
+        for key, value in kwargs.items():
+            if key in self.input_buffers and value is not None:
+                self.input_buffers[key].copy_(value, non_blocking=True)
+
+        # Copy the input tensors to the input buffers.
+        if input_ids is not None:
+            self.input_buffers["input_ids"].copy_(
+                input_ids, non_blocking=True)
+        if positions is not None:
+            # in some case like MLA, it will reuse positions in metadata
+            # but truncate them to the original size
+            # so the shape is not padded, we need to copy partial only
+            self.input_buffers["positions"][:positions.shape[0]].copy_(
+                positions, non_blocking=True)
+        if inputs_embeds is not None:
+            self.input_buffers["inputs_embeds"].copy_(
+                inputs_embeds, non_blocking=True)
+
+        if intermediate_tensors is not None:
+            for key in intermediate_tensors.tensors:
+                self.input_buffers[key].copy_(intermediate_tensors[key],
+                                              non_blocking=True)
+
+        common_metadata: MLUCommonAttentionMetadata = get_common_metadata()
+        num_reqs = common_metadata.num_input_tokens
+        block_table: BlockTable = self.model_runner.input_batch.block_table[0]
+
+        # Copy attention meta data
+        self.input_buffers["query_start_loc"].copy_(
+            self.model_runner.query_start_loc[:num_reqs + 1], non_blocking=True)
+        self.input_buffers["seq_start_loc"].copy_(
+            self.model_runner.seq_start_loc[:num_reqs + 1], non_blocking=True)
+        self.input_buffers["slot_mapping"].copy_(
+            self.model_runner.slot_mapping[:num_reqs], non_blocking=True)
+        self.input_buffers["seq_lens"].copy_(
+            self.model_runner.seq_lens[:num_reqs], non_blocking=True)
+        self.input_buffers["block_table"].copy_(
+            block_table.get_device_tensor()[:num_reqs], non_blocking=True)
+
+        # Add time markers for MLUGraph mode
+        if VLLM_LATENCY_DEBUG_WITH_DEVICE_EN:
+            self.start = torch.mlu.Event(enable_timing=True)
+            self.start.record()
+
+        # Run the graph.
+        self.graph.replay()
+        # Return the output tensor.
+        if get_pp_group().is_last_rank:
+            return self.output_buffers["hidden_states"]
+
+        return self.output_buffers
+

