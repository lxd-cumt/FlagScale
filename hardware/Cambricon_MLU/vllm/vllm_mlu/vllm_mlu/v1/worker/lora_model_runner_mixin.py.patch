diff --git a/vllm_mlu/vllm_mlu/v1/worker/lora_model_runner_mixin.py b/vllm_mlu/vllm_mlu/v1/worker/lora_model_runner_mixin.py
new file mode 100644
index 000000000..1f8073ce8
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/v1/worker/lora_model_runner_mixin.py
@@ -0,0 +1,30 @@
+from typing import List
+
+from vllm.lora.request import LoRARequest
+from vllm.v1.worker.lora_model_runner_mixin import LoRAModelRunnerMixin
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm_mlu__v1__worker__LoRAModelRunnerMixin__add_dummy_loras(self, num_loras: int) -> List[LoRARequest]:
+    assert num_loras > 0
+    assert self.lora_manager is not None
+
+    dummy_lora_requests: list[LoRARequest] = []
+    with self.lora_manager.dummy_lora_cache():
+        for idx in range(num_loras):
+            lora_id = idx + 1
+            dummy_lora_request = LoRARequest(
+                lora_name=f"capture_graph_{lora_id}",
+                lora_int_id=lora_id,
+                lora_path="/not/a/real/path",
+            )
+            self.lora_manager.add_dummy_lora(dummy_lora_request,
+                                             rank=self.LORA_WARMUP_RANK)
+            dummy_lora_requests.append(dummy_lora_request)
+    return dummy_lora_requests
+
+
+MluHijackObject.apply_hijack(LoRAModelRunnerMixin,
+                             "add_dummy_loras",
+                             vllm_mlu__v1__worker__LoRAModelRunnerMixin__add_dummy_loras)

