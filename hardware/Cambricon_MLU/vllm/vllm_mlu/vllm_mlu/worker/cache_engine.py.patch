diff --git a/vllm_mlu/vllm_mlu/worker/cache_engine.py b/vllm_mlu/vllm_mlu/worker/cache_engine.py
new file mode 100644
index 000000000..8b17af595
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/worker/cache_engine.py
@@ -0,0 +1,216 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""CacheEngine class for managing the KV cache."""
+from typing import List
+
+import torch
+
+from vllm.attention import get_attn_backend
+from vllm.config import CacheConfig, DeviceConfig, ModelConfig, ParallelConfig
+from vllm.logger import init_logger
+from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, LayerBlockType,
+                        get_dtype_size, is_pin_memory_available)
+from vllm.worker.cache_engine import CacheEngine
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+logger = init_logger(__name__)
+
+
+class CacheEngine_MluHijack(CacheEngine):
+
+    def _allocate_kv_cache(
+        self,
+        num_blocks: int,
+        device: str,
+    ) -> List[torch.Tensor]:
+        """Allocates KV cache on the specified device."""
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add kv_cache_scale for int8 support
+        '''
+        kv_cache_generic_shape = self.attn_backend.get_kv_cache_shape(
+            num_blocks, self.block_size, self.num_kv_heads, self.head_size)
+        kv_cache_scales_generic_shape = self.attn_backend.get_kv_cache_scale_shape(
+            num_blocks, self.block_size, self.num_kv_heads)
+        pin_memory = is_pin_memory_available() if device == "cpu" else False
+        kv_cache: List[List[torch.Tensor]] = []
+        try:
+            kv_cache_stride_order = self.attn_backend.get_kv_cache_stride_order(
+            )
+            kv_cache_scale_stride_order = self.attn_backend.get_kv_cache_scale_stride_order(
+            )
+        except (AttributeError, NotImplementedError):
+            kv_cache_stride_order = tuple(range(len(kv_cache_generic_shape)))
+            kv_cache_scale_stride_order = tuple(range(len(kv_cache_scales_generic_shape)))
+
+        # The allocation respects the backend-defined stride order to ensure
+        # the semantic remains consistent for each backend. We first obtain the
+        # generic kv cache shape and then permute it according to the stride
+        # order which could result in a non-contiguous tensor.
+        kv_cache_allocation_shape = tuple(kv_cache_generic_shape[i]
+                                          for i in kv_cache_stride_order)
+
+        for _ in range(self.num_attention_layers):
+            # null block in CpuGpuBlockAllocator requires at least that
+            # block to be zeroed-out.
+            # We zero-out everything for simplicity.
+            kv_cache_ = torch.zeros(
+                kv_cache_allocation_shape,
+                dtype=self.dtype,
+                pin_memory=pin_memory,
+                device=device).permute(*kv_cache_stride_order)
+
+            if self.dtype in [torch.int8, torch.uint8]:
+                kv_cache_scale_ = torch.zeros(
+                    kv_cache_scales_generic_shape,
+                    dtype=torch.float32,
+                    pin_memory=pin_memory,
+                    device=device).permute(*kv_cache_scale_stride_order)
+            else:
+                kv_cache_scale_ = torch.tensor(
+                    [],
+                    dtype=torch.float32,
+                    device=device)
+
+            # view back to (TOTAL_PAGES, PAGE_SIZE, entry_shape...) for cases
+            # when entry_shape is higher than 1D
+            kv_cache.append([kv_cache_, kv_cache_scale_])
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return kv_cache
+
+    def swap_in(self, src_to_dst: torch.Tensor) -> None:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: swap kv_cache_scale for int8 support
+        ''' 
+        for i in range(self.num_attention_layers):
+            # swap kv_cache
+            self.attn_backend.swap_blocks(self.cpu_cache[i][0], self.gpu_cache[i][0],
+                                          src_to_dst)
+            if self.dtype in [torch.int8, torch.uint8]:
+                # swap kv_cache_scale
+                self.attn_backend.swap_blocks(self.cpu_cache[i][1], self.gpu_cache[i][1],
+                                              src_to_dst)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def swap_out(self, src_to_dst: torch.Tensor) -> None:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: swap kv_cache_scale for int8 support
+        ''' 
+        for i in range(self.num_attention_layers):
+            # swap kv_cache
+            self.attn_backend.swap_blocks(self.gpu_cache[i][0], self.cpu_cache[i][0],
+                                          src_to_dst)
+            if self.dtype in [torch.int8, torch.uint8]:
+                # swap kv_cache_scale
+                self.attn_backend.swap_blocks(self.gpu_cache[i][1], self.cpu_cache[i][1],
+                                              src_to_dst)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    @staticmethod
+    def get_cache_block_size(
+        cache_config: CacheConfig,
+        model_config: ModelConfig,
+        parallel_config: ParallelConfig,
+    ) -> int:
+        head_size = model_config.get_head_size()
+        num_heads = model_config.get_num_kv_heads(parallel_config)
+        num_attention_layers = model_config.get_num_layers_by_block_type(
+            parallel_config, LayerBlockType.attention)
+
+        if cache_config.cache_dtype == "auto":
+            dtype = model_config.dtype
+        else:
+            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]
+
+        key_cache_entry = num_heads * head_size
+
+        # For MLA there is no value cache, since the latent vector
+        # is joint keys and values.
+        value_cache_entry = key_cache_entry if not model_config.use_mla else 0
+        total = num_attention_layers * cache_config.block_size * \
+            (key_cache_entry + value_cache_entry)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: compute kv_cache_scale total size
+        '''
+        dtype_size = get_dtype_size(dtype)
+        kv_cache_total_size = dtype_size * total
+
+        kv_cache_scale_total_size = 0
+        if cache_config.cache_dtype == 'int8':
+            key_cache_scale_block = cache_config.block_size * num_heads
+            value_cache_scale_block = key_cache_scale_block if not model_config.use_mla else 0
+            scale_total = num_attention_layers * (key_cache_scale_block + value_cache_scale_block)
+            dtype_size = get_dtype_size(torch.float32)
+            kv_cache_scale_total_size = dtype_size * scale_total
+
+        return kv_cache_total_size + kv_cache_scale_total_size
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    @staticmethod
+    def get_max_num_gpu_blocks(
+        cache_config: CacheConfig,
+        model_config: ModelConfig,
+        parallel_config: ParallelConfig,
+    ) -> int:
+        '''
+        Fix: tmo.reshape_paged_cache limit: RuntimeError: The addressing range of kv_cache cannot exceed 4G
+        Fix: cnnlScaledDotProductAttn_v5 check failed: cnnlGetTensorElementNum(key_desc) * kv_cache_dbyte <= INT32_MAX
+        '''
+        head_size = model_config.get_head_size()
+        num_heads = model_config.get_num_kv_heads(parallel_config)
+        if cache_config.cache_dtype == "auto":
+            dtype = model_config.dtype
+        else:
+            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]
+        dtype_size = get_dtype_size(dtype)
+        max_kv_cache_size = min(4 * 1024 * 1024 * 1024, torch.iinfo(torch.int).max)
+
+        max_num_gpu_blocks = max_kv_cache_size // (cache_config.block_size * num_heads * head_size * dtype_size)
+
+        return max_num_gpu_blocks
+
+
+MluHijackObject.apply_hijack(CacheEngine,
+                             CacheEngine._allocate_kv_cache,
+                             CacheEngine_MluHijack._allocate_kv_cache)
+MluHijackObject.apply_hijack(CacheEngine,
+                             CacheEngine.swap_in,
+                             CacheEngine_MluHijack.swap_in)
+MluHijackObject.apply_hijack(CacheEngine,
+                             CacheEngine.swap_out,
+                             CacheEngine_MluHijack.swap_out)
+MluHijackObject.apply_hijack(CacheEngine,
+                             CacheEngine.get_cache_block_size,
+                             CacheEngine_MluHijack.get_cache_block_size)
+MluHijackObject.apply_hijack(CacheEngine,
+                             "get_max_num_gpu_blocks",
+                             CacheEngine_MluHijack.get_max_num_gpu_blocks)
\ No newline at end of file

