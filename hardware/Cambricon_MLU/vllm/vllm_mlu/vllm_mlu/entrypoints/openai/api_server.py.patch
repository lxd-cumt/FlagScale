diff --git a/vllm_mlu/vllm_mlu/entrypoints/openai/api_server.py b/vllm_mlu/vllm_mlu/entrypoints/openai/api_server.py
new file mode 100644
index 000000000..a1fbe0035
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/entrypoints/openai/api_server.py
@@ -0,0 +1,26 @@
+from fastapi import Request
+from fastapi.responses import Response
+
+from vllm.logger import logger
+from vllm.entrypoints.openai.api_server import (
+    router, engine_client
+)
+import vllm_mlu._mlu_utils as mlu_envs
+
+if mlu_envs.VLLM_SCHEDULER_PROFILE:
+    logger.info(
+        "vLLM V1 Scheduler Profiler is enabled in the API server. Please use "
+        "'tools/utils/post_scheduler_view_action.py' to dump profiling data "
+        "after all requests finished.")
+
+    @router.post("/v1/start_scheduler_profile")
+    async def start_scheduler_profile(raw_request: Request):
+        logger.info("VLLM-V1 starting scheduler profiler...")
+        await engine_client(raw_request).start_scheduler_profile()
+        return Response(status_code=200)
+
+    @router.post("/v1/stop_scheduler_profile")
+    async def stop_scheduler_profile(raw_request: Request):
+        logger.info("VLLM-V1 scheduler stopping profiler...")
+        await engine_client(raw_request).stop_scheduler_profile()
+        return Response(status_code=200)
\ No newline at end of file

