diff --git a/vllm_mlu/vllm_mlu/lora/fully_sharded_layers.py b/vllm_mlu/vllm_mlu/lora/fully_sharded_layers.py
new file mode 100644
index 000000000..54166fead
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/lora/fully_sharded_layers.py
@@ -0,0 +1,77 @@
+from typing import Optional
+
+import torch
+
+from vllm.distributed import tensor_model_parallel_all_reduce
+from vllm.lora.fully_sharded_layers import RowParallelLinearWithShardedLoRA
+from vllm.platforms import current_platform
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__lora__fully_sharded_layers__RowParallelLinearWithShardedLoRA__apply(
+    self,
+    x: torch.Tensor,
+    bias: Optional[torch.Tensor],
+    residual: Optional[torch.Tensor]
+) -> torch.Tensor:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add residual and bias in matmul
+    '''
+    output = self.base_layer.quant_method.apply(
+        self.base_layer, x, bias, residual)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    x = x.view(-1, x.shape[-1])
+    output, out_orig_shape = output.view(-1,
+                                            output.shape[-1]), output.shape
+    buffer = torch.zeros(
+        (self.n_slices, x.shape[0], self.lora_a_stacked[0].shape[2]),
+        dtype=torch.float32,
+        device=x.device,
+    )
+
+    shrunk_buffer: Optional[torch.Tensor] = self.punica_wrapper.add_shrink(
+        buffer, x, self.lora_a_stacked, 1.0)
+    if not current_platform.can_update_inplace():
+        buffer = shrunk_buffer
+
+    buffer = tensor_model_parallel_all_reduce(buffer)
+
+    # following S-LoRA, allows the fusing of all_gather and all_reduce
+    # by adding the column partitioned lora output to a slice of output
+    # tensor, which is a partial sum due to row parallel. All that
+    # remains is a standard all_reduce. User should be aware though that
+    # the output is not the same as a normal row_parallel, it should be
+    # reduced before being used
+    # NOTE offset are based on the rank.
+    shard_size = self.lora_b_stacked[0].shape[2]
+    offset_start = self.tp_rank * shard_size
+    lora_output: Optional[torch.Tensor] = self.punica_wrapper.add_expand(
+        output,
+        buffer,
+        self.lora_b_stacked,
+        self.lora_bias_stacked,
+        self.output_slices,
+        offset_start=offset_start,
+        add_input=True,
+    )
+
+    if not current_platform.can_update_inplace():
+        output = lora_output
+
+    output = output.view(*out_orig_shape)
+    return output
+
+
+MluHijackObject.apply_hijack(
+    RowParallelLinearWithShardedLoRA,
+    RowParallelLinearWithShardedLoRA.apply,
+    vllm__lora__fully_sharded_layers__RowParallelLinearWithShardedLoRA__apply
+)

