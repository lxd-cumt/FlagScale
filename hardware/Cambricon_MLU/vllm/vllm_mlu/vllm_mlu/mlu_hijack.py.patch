diff --git a/vllm_mlu/vllm_mlu/mlu_hijack.py b/vllm_mlu/vllm_mlu/mlu_hijack.py
new file mode 100644
index 000000000..db296974a
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/mlu_hijack.py
@@ -0,0 +1,91 @@
+import importlib.util
+import logging
+from logging import Logger
+from vllm_mlu._mlu_utils import *
+
+
+def mlu_init_logger(name: str) -> Logger:
+    """Initialize loggers for vllm_mlu module,
+    and keep the configuration consistent with the vllm module"""
+    mlu_logger = logging.getLogger(name)
+    vllm_logger = logging.Logger.manager.loggerDict.get('vllm', None)
+    if vllm_logger:
+        mlu_logger.setLevel(vllm_logger.level)
+        mlu_logger.propagate = vllm_logger.propagate
+        mlu_logger.handlers = vllm_logger.handlers
+    return mlu_logger
+
+
+from vllm import logger
+logger.init_logger = mlu_init_logger
+from vllm.logger import init_logger
+
+logger = init_logger(__name__)
+
+
+def is_module_available(module_name):
+    spec = importlib.util.find_spec(module_name)
+    return spec is not None
+
+def check_environ_compatibility():
+    if is_module_available('apex'):
+        logger.error(f"The `apex` package is currently present in your environment, "
+                     f"which may cause model accuracy issues or other problems. It is "
+                     f"strongly recommended that you uninstall it before using vLLM.")
+
+# Check environment compatibility first before applying mlu hijack.
+check_environ_compatibility()
+
+
+# Apply v1 hijack
+import vllm_mlu.v1.engine.core
+import vllm_mlu.v1.engine.core_client
+import vllm_mlu.v1.engine.llm_engine
+import vllm_mlu.v1.engine.async_llm
+import vllm_mlu.v1.core.sched.scheduler
+import vllm_mlu.v1.core.single_type_kv_cache_manager
+import vllm_mlu.v1.executor.abstract
+import vllm_mlu.v1.sample.rejection_sampler
+import vllm_mlu.v1.worker.lora_model_runner_mixin
+import vllm_mlu.v1.worker.block_table
+import vllm_mlu.compilation.fix_functionalization
+
+# Apply common hijack
+import vllm_mlu.config
+import vllm_mlu.utils
+import vllm_mlu.attention.layer
+import vllm_mlu.distributed.parallel_state
+import vllm_mlu.distributed.kv_transfer.kv_connector.factory
+import vllm_mlu.engine.arg_utils
+import vllm_mlu.entrypoints.llm
+import vllm_mlu.executor.multiproc_worker_utils
+import vllm_mlu.executor.executor_base
+import vllm_mlu.executor.ray_distributed_executor
+import vllm_mlu.lora.fully_sharded_layers
+import vllm_mlu.lora.layers
+import vllm_mlu.model_executor.parameter
+import vllm_mlu.model_executor.guided_decoding.xgrammar_decoding
+import vllm_mlu.model_executor.layers.linear
+import vllm_mlu.model_executor.layers.rotary_embedding
+import vllm_mlu.model_executor.layers.quantization.utils.w8a8_utils
+import vllm_mlu.model_executor.layers.quantization.fp8
+import vllm_mlu.model_executor.layers.activation
+import vllm_mlu.model_executor.layers.layernorm
+import vllm_mlu.model_executor.layers.fused_moe.layer
+import vllm_mlu.model_executor.model_loader.tensorizer_loader
+import vllm_mlu.model_executor.models.registry
+import vllm_mlu.model_executor.models.bert
+import vllm_mlu.model_executor.models.clip
+import vllm_mlu.model_executor.models.intern_vit
+import vllm_mlu.model_executor.models.mllama
+import vllm_mlu.model_executor.models.deepseek_v2
+import vllm_mlu.model_executor.models.deepseek_mtp
+import vllm_mlu.model_executor.models.glm4_moe
+import vllm_mlu.model_executor.models.glm4_1v
+import vllm_mlu.worker.cache_engine
+
+if VLLM_CI_ACCURACY_TEST:
+    import vllm_mlu.model_executor.model_loader.dummy_loader
+
+if VLLM_SCHEDULER_PROFILE:
+    import vllm_mlu.entrypoints.openai.api_server

