diff --git a/vllm_mlu/vllm_mlu/model_executor/layers/quantization/utils/w8a8_utils.py b/vllm_mlu/vllm_mlu/model_executor/layers/quantization/utils/w8a8_utils.py
new file mode 100644
index 000000000..218b590e6
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/layers/quantization/utils/w8a8_utils.py
@@ -0,0 +1,172 @@
+from typing import Optional, Callable
+import torch
+
+from vllm import _custom_ops as ops
+from vllm.model_executor.layers.quantization.utils.w8a8_utils import (
+    Fp8LinearOp, USE_ROWWISE_TORCH_SCALED_MM, cutlass_w8a8_scaled_mm,
+    rocm_per_tensor_w8a8_scaled_mm, torch_per_tensor_w8a8_scaled_mm,
+    torch_per_token_w8a8_scaled_mm, torch_channelwise_w8a8_scaled_mm)
+from vllm.platforms import current_platform
+
+from vllm_mlu import _mlu_ops as mlu_ops
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def mlu_w8a8_scaled_mm(
+    qinput: torch.Tensor, weight: torch.Tensor, out_dtype: torch.dtype,
+    scale_a: torch.Tensor, scale_b: torch.Tensor, bias: torch.Tensor,
+    output_shape: list, **kwargs
+) -> torch.Tensor:
+    output = mlu_ops.scaled_matmul(
+        qinput, # a
+        weight, # b
+        scale_a, # a_scale
+        scale_b, # b_scale
+        out_dtype, # output_dtype
+        bias, # bias
+        c=None, act_mode="none",quant_bit_size=8, alpha=1, beta=1, use_hp_active=False,
+                              a_quant_bit_size=8, a_calib=None, b_calib=None
+    )
+    return output.view(*output_shape)
+
+
+def dispatch_w8a8_scaled_mm(
+    cutlass_fp8_supported: bool, per_tensor_weights: bool,
+    per_tensor_activations: bool, use_per_token_if_dynamic: Optional[bool],
+    weight_per_channel: bool, activation_per_token: bool,
+) -> Callable[..., torch.Tensor]:
+
+    if cutlass_fp8_supported:
+        return cutlass_w8a8_scaled_mm
+    if per_tensor_weights and per_tensor_activations:
+        if current_platform.is_rocm():
+            return rocm_per_tensor_w8a8_scaled_mm
+        return torch_per_tensor_w8a8_scaled_mm
+    # torch.scaled_mm supports per tensor weights + activations only
+    # so fallback to naive if per channel or per token
+    if (use_per_token_if_dynamic and not per_tensor_weights
+            and not per_tensor_activations and USE_ROWWISE_TORCH_SCALED_MM):
+        return torch_per_token_w8a8_scaled_mm
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: dispatch to mlu_w8a8_scaled_mm
+    '''
+    if weight_per_channel and activation_per_token:
+        return mlu_w8a8_scaled_mm
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return torch_channelwise_w8a8_scaled_mm
+
+
+def vllm__model_executor__layers__quantization__utils__w8a8_util__Fp8LinearOp__apply(
+    self,
+    input: torch.Tensor,
+    weight: torch.Tensor,
+    weight_scale: torch.Tensor,
+    out_dtype: Optional[torch.dtype] = None,
+    input_scale: Optional[torch.Tensor] = None,
+    input_scale_ub: Optional[torch.Tensor] = None,
+    bias: Optional[torch.Tensor] = None,
+    # TODO(luka) remove this parameter in favor of __init__
+    use_per_token_if_dynamic: Optional[bool] = None,
+    weight_per_channel: bool = True,
+    activation_per_token: bool = True,
+) -> torch.Tensor:
+    # ops.scaled_fp8_quant supports both dynamic and static quant.
+    #   If dynamic, layer.input_scale is None and x_scale computed from x.
+    #   If static, layer.input_scale is scalar and x_scale is input_scale.
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add mlu_fp8_supported
+    '''
+    self.mlu_fp8_supported = False
+    if weight_per_channel and activation_per_token:
+        self.mlu_fp8_supported = True
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    # View input as 2D matrix for fp8 methods
+    input_2d = input.view(-1, input.shape[-1])
+    output_shape = [*input.shape[:-1], weight.shape[1]]
+
+    # TODO(luka) this is here because currently MLA only decides this
+    #  during the forward method instead of in __init__.
+    if use_per_token_if_dynamic is None:
+        use_per_token_if_dynamic = self.use_per_token_if_dynamic
+
+    if out_dtype is None:
+        out_dtype = input.dtype
+
+    # cutlass_scaled_mm supports per tensor/channel W and per tensor/token A
+    if self.cutlass_fp8_supported:
+        assert input.dtype != current_platform.fp8_dtype(
+        ), "FP8 input to cutlass is not currently implemented"
+        qinput, x_scale = ops.scaled_fp8_quant(
+            input_2d,
+            input_scale,
+            scale_ub=input_scale_ub,
+            use_per_token_if_dynamic=use_per_token_if_dynamic)
+    elif self.mlu_fp8_supported:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: Add support for activation-per-token weight-per-channel quantization.
+        '''
+        qinput, x_scale = mlu_ops.scaled_quantize(
+            input_2d,# x
+            None, # scale
+            None, # zero
+            None, # scale_ub
+            quant_type=torch.float8_e4m3fn,
+            quant_mode='dynamic_per_token'
+        )
+        output_shape = [*input.shape[:-1], weight.shape[0]]
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    else:
+        if input.dtype != current_platform.fp8_dtype():
+            # Maybe apply padding to output, see comment in __init__
+            qinput, x_scale = ops.scaled_fp8_quant(
+                input_2d,
+                input_scale,
+                num_token_padding=self.output_padding,
+                use_per_token_if_dynamic=use_per_token_if_dynamic)
+        else:
+            qinput, x_scale = input_2d, input_scale
+
+    per_tensor_weights = (weight_scale.numel() == 1)
+    per_tensor_activations = (x_scale.numel() == 1)
+
+    w8a8_scaled_mm_func = dispatch_w8a8_scaled_mm(
+        self.cutlass_fp8_supported, per_tensor_weights,
+        per_tensor_activations, use_per_token_if_dynamic,
+        weight_per_channel, activation_per_token)
+
+    return w8a8_scaled_mm_func(qinput=qinput,
+                                weight=weight,
+                                out_dtype=out_dtype,
+                                scale_a=x_scale,
+                                scale_b=weight_scale,
+                                bias=bias,
+                                input_2d=input_2d,
+                                output_shape=output_shape)
+
+
+MluHijackObject.apply_hijack(
+    Fp8LinearOp,
+    Fp8LinearOp.apply,
+    vllm__model_executor__layers__quantization__utils__w8a8_util__Fp8LinearOp__apply
+)
\ No newline at end of file

