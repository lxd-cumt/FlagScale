diff --git a/vllm_mlu/vllm_mlu/model_executor/guided_decoding/xgrammar_decoding.py b/vllm_mlu/vllm_mlu/model_executor/guided_decoding/xgrammar_decoding.py
new file mode 100644
index 000000000..2daf21ccb
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/guided_decoding/xgrammar_decoding.py
@@ -0,0 +1,168 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# noqa: UP007
+from __future__ import annotations
+
+import json
+from typing import TYPE_CHECKING
+
+
+from vllm.logger import init_logger
+
+try:
+    import xgrammar as xgr
+    xgr_installed = True
+except ImportError:
+    xgr_installed = False
+    pass
+
+from vllm.model_executor.guided_decoding.utils import (convert_lark_to_gbnf,
+                                                       grammar_is_likely_lark)
+from vllm.model_executor.guided_decoding.xgrammar_decoding import (
+    GrammarConfig,
+    TokenizerDataCache,
+)
+
+if TYPE_CHECKING:
+    from transformers import PreTrainedTokenizer
+
+    from vllm.config import ModelConfig
+    from vllm.sampling_params import GuidedDecodingParams
+
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+logger = init_logger(__name__)
+
+
+def vllm__model_executor__guided_decoding__xgrammar_decoding__GrammarConfig__from_guided_params(
+    guided_params: GuidedDecodingParams,
+    model_config: ModelConfig,
+    tokenizer: PreTrainedTokenizer,
+    max_threads: int = 8) -> GrammarConfig:
+
+    tokenizer_hash = hash(tokenizer)
+    tokenizer_data = TokenizerDataCache.get_tokenizer_data(
+        tokenizer,
+        tokenizer_hash=tokenizer_hash,
+        vocab_size=model_config.hf_text_config.vocab_size,
+    )
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: in the community implementation, any_whitespace of GrammarConfig
+    is not initialized correctly except handling guided_params.json. Fix this issue.
+    '''
+    any_whitespace = not guided_params.disable_any_whitespace
+
+    if guided_params.json:
+        if not isinstance(guided_params.json, str):
+            json_str = json.dumps(guided_params.json)
+        else:
+            json_str = guided_params.json
+
+        # Check and log if model with xgrammar and whitespace have history
+        # of runaway generation of whitespaces.
+        # References:
+        # https://github.com/vllm-project/vllm/pull/12744
+        # https://github.com/mlc-ai/xgrammar/issues/212
+        model_with_warn = None
+
+        if 'Mistral' in model_config.model:
+            model_with_warn = 'Mistral'
+        elif 'Qwen' in model_config.model:
+            model_with_warn = 'Qwen'
+
+        if model_with_warn is not None and any_whitespace:
+            logger.info_once(
+                "%s model detected, consider setting `disable_any_whitespace` to prevent runaway generation of whitespaces.",  # noqa: E501
+                model_with_warn,
+            )
+        # Validate the schema and raise ValueError here if it is invalid.
+        # This is to avoid exceptions in model execution, which will crash
+        # the engine worker process.
+        try:
+            xgr.Grammar.from_json_schema(json_str,
+                                            any_whitespace=any_whitespace)
+        except RuntimeError as err:
+            raise ValueError(str(err)) from err
+
+        return GrammarConfig(json_str=json_str,
+                    tokenizer_hash=tokenizer_hash,
+                    max_threads=max_threads,
+                    tokenizer_data=tokenizer_data,
+                    any_whitespace=any_whitespace)
+    elif guided_params.grammar:
+        # XGrammar only supports GBNF grammars, so we must convert Lark
+        if grammar_is_likely_lark(guided_params.grammar):
+            try:
+                grammar_str = convert_lark_to_gbnf(guided_params.grammar)
+            except ValueError as e:
+                raise ValueError(
+                    "Failed to convert the grammar from Lark to GBNF. "
+                    "Please either use GBNF grammar directly or specify"
+                    " --guided-decoding-backend=outlines.\n"
+                    f"Conversion error: {str(e)}") from e
+        else:
+            grammar_str = guided_params.grammar
+
+        # Validate the grammar and raise ValueError here if it is invalid.
+        # This is to avoid exceptions in model execution, which will crash
+        # the engine worker process.
+        try:
+            xgr.Grammar.from_ebnf(grammar_str)
+        except RuntimeError as err:
+            raise ValueError(str(err)) from err
+
+        return GrammarConfig(grammar_str=grammar_str,
+                    tokenizer_hash=tokenizer_hash,
+                    max_threads=max_threads,
+                    tokenizer_data=tokenizer_data,
+                    any_whitespace=any_whitespace)
+    elif guided_params.json_object:
+        return GrammarConfig(
+            json_object=True,
+            tokenizer_hash=tokenizer_hash,
+            max_threads=max_threads,
+            tokenizer_data=tokenizer_data,
+            any_whitespace=any_whitespace,
+        )
+    elif guided_params.choice:
+        choice_str = GrammarConfig.choice_as_grammar(guided_params.choice)
+        try:
+            xgr.Grammar.from_ebnf(choice_str)
+        except RuntimeError as err:
+            raise ValueError(str(err)) from err
+
+        return GrammarConfig(
+            grammar_str=choice_str,
+            tokenizer_hash=tokenizer_hash,
+            max_threads=max_threads,
+            tokenizer_data=tokenizer_data,
+            any_whitespace=any_whitespace,
+        )
+    elif guided_params.regex:
+        return GrammarConfig(
+            regex_str=guided_params.regex,
+            tokenizer_hash=tokenizer_hash,
+            max_threads=max_threads,
+            tokenizer_data=tokenizer_data,
+            any_whitespace=any_whitespace
+        )
+    else:
+        raise ValueError(
+            "Currently only support JSON and EBNF grammar mode for xgrammar"
+        )
+    '''
+    =================
+    End of MLU Hijack
+    =================
+    '''
+
+MluHijackObject.apply_hijack(
+    GrammarConfig,
+    GrammarConfig.from_guided_params,
+    vllm__model_executor__guided_decoding__xgrammar_decoding__GrammarConfig__from_guided_params,
+)
\ No newline at end of file

