diff --git a/vllm_mlu/vllm_mlu/model_executor/models/chatglm.py b/vllm_mlu/vllm_mlu/model_executor/models/chatglm.py
new file mode 100644
index 000000000..989e74287
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/chatglm.py
@@ -0,0 +1,256 @@
+import json
+import torch
+from torch import nn
+from torch.nn import LayerNorm, Module
+from typing import Optional
+
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.transformers_utils.configs import ChatGLMConfig
+
+from vllm.model_executor.models.interfaces import SupportsLoRA, SupportsPP, SupportsQuant
+from vllm.model_executor.models.utils import (
+    make_empty_intermediate_tensors_factory, make_layers)
+from vllm.logger import init_logger
+
+from vllm.transformers_utils.configs import ChatGLMConfig
+from vllm.model_executor.models.chatglm import (
+    GLMAttention, ChatGLMModel, GLMTransformer, ChatGLMBaseModel, ChatGLMForCausalLM)
+
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base,
+    is_smoothquant
+)
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm, QuantFusionLayerNorm
+
+logger = init_logger(__name__)
+
+
+class MLUGLMAttention(GLMAttention):
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        position_ids: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.query_key_value(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        @brief: apply residual fusion
+        '''
+        qk, _ = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)
+        self.rotary_emb(position_ids, qk.view(
+            -1, self.num_heads + self.num_kv_heads, self.head_dim))
+        context_layer = self.attn(q, k, v)
+        attn_output, _ = self.dense(context_layer, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return attn_output
+
+
+class MLUGLMBlock(nn.Module):
+    """A single transformer layer.
+
+    Transformer layer takes input with size [s, b, h] and returns an
+    output of the same size.
+    """
+
+    def __init__(
+        self,
+        config: ChatGLMConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.apply_residual_connection_post_layernorm = (
+            config.apply_residual_connection_post_layernorm)
+
+        self.fp32_residual_connection = config.fp32_residual_connection
+
+        layer_norm_func = RMSNorm if config.rmsnorm else LayerNorm
+        # Layernorm on the input data.
+        self.input_layernorm = layer_norm_func(config.hidden_size,
+                                               eps=config.layernorm_epsilon)
+
+        # Self attention.
+        self.self_attention = MLUGLMAttention(config,
+                                            cache_config,
+                                            quant_config,
+                                            prefix=f"{prefix}.self_attention")
+        self.hidden_dropout = config.hidden_dropout
+
+        # Layernorm on the attention output
+        self.post_attention_layernorm = layer_norm_func(
+            config.hidden_size, eps=config.layernorm_epsilon)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: 1) use FeedForward instead of MLP
+                2) prepare to perf per-tensor sq cases if suitable
+        '''
+        # MLP
+        self.mlp = FeedForward(
+            hidden_size=config.hidden_size,
+            intermediate_size=config.ffn_hidden_size,
+            hidden_act='silu',
+            up_proj_name='dense_h_to_4h',
+            is_gated=True,
+            down_proj_name='dense_4h_to_h',
+            bias=config.add_bias_linear,
+            quant_config=quant_config
+        )
+
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config) and not self.apply_residual_connection_post_layernorm:
+            QuantNorm = (QuantFusionRMSNorm if config.rmsnorm else QuantFusionLayerNorm)
+            self.input_layernorm = QuantNorm(
+                config.hidden_size, config.layernorm_epsilon,
+                self.self_attention.query_key_value)
+            self.post_attention_layernorm = QuantNorm(
+                config.hidden_size, config.layernorm_epsilon,
+                self.mlp.dense_h_to_4h)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        position_ids: torch.Tensor,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return decoder_layer_forward_base(
+            positions=position_ids,
+            hidden_states=hidden_states,
+            input_layernorm=self.input_layernorm,
+            self_attn=self.self_attention,
+            post_layernorm=self.post_attention_layernorm,
+            mlp=self.mlp,
+            apply_residual_connection_post_layernorm=self.apply_residual_connection_post_layernorm,
+            position_name='position_ids',
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+class MLUGLMTransformer(GLMTransformer):
+    """Transformer class."""
+
+    def __init__(
+        self,
+        config: ChatGLMConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        Module.__init__(self)
+        self.post_layer_norm = config.post_layer_norm
+
+        # Number of layers.
+        self.num_layers = config.num_layers
+
+        # Transformer layers.
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            self.num_layers,
+            lambda prefix: MLUGLMBlock(
+                config, cache_config, quant_config, prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+
+        if self.post_layer_norm:
+            layer_norm_func = RMSNorm if config.rmsnorm else LayerNorm
+            # Final layer norm before output.
+            self.final_layernorm = layer_norm_func(
+                config.hidden_size, eps=config.layernorm_epsilon)
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(["hidden_states"],
+                                                    config.hidden_size))
+
+
+@support_torch_compile
+class MLUChatGLMModel(ChatGLMModel):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        Module.__init__(self)
+        SupportsQuant.__init__(self)
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+
+        self.embedding = VocabParallelEmbedding(config.padded_vocab_size,
+                                                config.hidden_size,
+                                                quant_config=quant_config,
+                                                prefix=f"{prefix}.embedding")
+
+        self.num_layers = config.num_layers
+        self.multi_query_group_num = config.multi_query_group_num
+        self.kv_channels = config.kv_channels
+        self.encoder = MLUGLMTransformer(config,
+                                        cache_config,
+                                        quant_config,
+                                        prefix=f"{prefix}.encoder")
+
+        self.output_layer = ParallelLMHead(config.padded_vocab_size,
+                                           config.hidden_size,
+                                           quant_config=quant_config,
+                                           prefix=f"{prefix}.output_layer")
+
+        self.make_empty_intermediate_tensors = (
+            self.encoder.make_empty_intermediate_tensors)
+
+
+class MLUChatGLMForCausalLM(ChatGLMForCausalLM, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        config = vllm_config.model_config.hf_config
+        if hasattr(config, "vision_config"):
+            hf_overrides = {"architectures": ["GLM4VForCausalLM"]}
+            raise RuntimeError(
+                "The configuration of this model indicates that it supports "
+                "vision inputs, but you instantiated the text-only version "
+                "of this model. Please use the vision model by setting "
+                f"`--hf-overrides '{json.dumps(hf_overrides)}'`")
+
+        ChatGLMBaseModel.__init__(self, vllm_config=vllm_config,
+                                  prefix=prefix, transformer_type=MLUChatGLMModel)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        SupportsLoRA.__init__(self)
+        SupportsPP.__init__(self)
+        SupportsQuant.__init__(self)

