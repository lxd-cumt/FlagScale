diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen.py
new file mode 100644
index 000000000..d945eb134
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen.py
@@ -0,0 +1,218 @@
+import json
+import torch
+from torch import nn
+from torch.nn import Module
+from typing import Optional, Union
+from transformers import PretrainedConfig
+
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.models.qwen import (QWenAttention, QWenModel,
+                                             QWenBaseModel, QWenLMHeadModel)
+from vllm.sequence import IntermediateTensors
+from vllm.model_executor.models.interfaces import SupportsLoRA, SupportsPP
+from vllm.model_executor.models.utils import (is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+from vllm.logger import init_logger
+
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+logger = init_logger(__name__)
+
+
+class MLUQWenAttention(QWenAttention):
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.c_attn(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.chunk(chunks=3, dim=-1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        @brief: c_proj add residual
+        '''
+        qk, _ = qkv.split([self.head_dim * self.num_heads * 2,
+                           self.head_dim * self.num_heads], dim=-1)
+        self.rotary_emb(positions, qk.view(
+            -1, self.num_heads + self.num_heads, self.head_dim))
+        attn_output = self.attn(q, k, v)
+        output, _ = self.c_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+
+class MLUQWenBlock(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.ln_1 = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)
+
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        self.attn = MLUQWenAttention(config.hidden_size,
+                                  config.num_attention_heads,
+                                  config.max_position_embeddings,
+                                  rope_theta=rope_theta,
+                                  rope_scaling=rope_scaling,
+                                  cache_config=cache_config,
+                                  quant_config=quant_config,
+                                  prefix=f"{prefix}.attn")
+
+        self.ln_2 = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: 1) use FeedForward instead of MLP
+                2) prepare to perf per-tensor sq cases if suitable
+        '''
+        self.mlp = FeedForward(hidden_size=config.hidden_size,
+                                intermediate_size=config.intermediate_size // 2,
+                                hidden_act='silu',
+                                up_proj_name='gate_up_proj',
+                                is_gated=True,
+                                down_proj_name='c_proj',
+                                bias=False,
+                                quant_config=quant_config)
+
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            self.ln_1 = QuantFusionRMSNorm(
+                config.hidden_size, config.layer_norm_epsilon,
+                self.attn.c_attn)
+            self.ln_2 = QuantFusionRMSNorm(
+                config.hidden_size, config.layer_norm_epsilon,
+                self.mlp.gate_up_proj)
+            self.is_per_token_sq_perf_cases = self.ln_1.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return decoder_layer_forward_base(
+            positions=positions,
+            hidden_states=hidden_states,
+            input_layernorm=self.ln_1,
+            self_attn=self.attn,
+            post_layernorm=self.ln_2,
+            mlp=self.mlp,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+@support_torch_compile
+class MLUQWenModel(QWenModel):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        Module.__init__(self)
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+        self.vocab_size = config.vocab_size
+
+        self.wte = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+        )
+        self.start_layer, self.end_layer, self.h = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUQWenBlock(
+                config, cache_config, quant_config, prefix=prefix),
+            prefix=f"{prefix}.h")
+        self.ln_f = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        return decoder_model_forward_base_pp(
+            input_ids, positions, intermediate_tensors,
+            self.h, self.start_layer, self.end_layer,
+            self.get_input_embeddings,
+            self.ln_f,
+            inputs_embeds
+        )
+
+
+class MLUQWenLMHeadModel(QWenLMHeadModel, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        config = vllm_config.model_config.hf_config
+        if hasattr(config, "visual"):
+            hf_overrides = {
+                "architectures": ["QwenVLForConditionalGeneration"]
+            }
+            raise RuntimeError(
+                "The configuration of this model indicates that it supports "
+                "vision inputs, but you instantiated the text-only version "
+                "of this model. Please use the vision model by setting "
+                f"`--hf-overrides '{json.dumps(hf_overrides)}'`")
+
+        QWenBaseModel.__init__(self,
+                               vllm_config=vllm_config,
+                               prefix=prefix,
+                               transformer_type=MLUQWenModel)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        SupportsPP.__init__(self)
+        SupportsLoRA.__init__(self)
+
+    def get_ffn_inner_size(self):
+        ffn_size = self.model_config.intermediate_size // 2
+        return ffn_size

