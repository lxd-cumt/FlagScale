diff --git a/vllm_mlu/vllm_mlu/model_executor/models/hunyuan.py b/vllm_mlu/vllm_mlu/model_executor/models/hunyuan.py
new file mode 100755
index 000000000..6e0dc4036
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/hunyuan.py
@@ -0,0 +1,607 @@
+import torch
+import re
+from torch import nn
+from torch.nn import Module
+
+from typing import Optional, Tuple, Iterable, Union
+from transformers import PretrainedConfig
+
+from vllm.config import CacheConfig, LoRAConfig, VllmConfig
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.distributed import get_pp_group
+from vllm.model_executor.models.utils import is_pp_missing_parameter
+from vllm.sequence import IntermediateTensors
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.models.interfaces import SupportsLoRA
+from vllm.model_executor.models.utils import (
+    PPMissingLayer, is_pp_missing_parameter, make_layers, maybe_prefix)
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.models.mlu_models.hunyuan import (
+    HunYuanAttention, HunYuanDecoderLayer, HunYuanForCausalLM, HunYuanModel)
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp, MoeGroupInfo
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.models.layer_utils import (
+    hunyuan_decoder_layer_forward_base, hunyuan_decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+
+
+class MLUHunYuanSparseMoeBlock(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+    ):
+        super().__init__(num_experts=config.num_experts,
+                         top_k=config.moe_topk,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=True if config.moe_topk>1 else False,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True)
+        self.config = config
+        self.shared_mlp = None
+        if config.use_mixed_mlp_moe > 0:
+            self.shared_mlp = FeedForward(
+                hidden_size=config.hidden_size,
+                intermediate_size=config.intermediate_size * config.num_shared_expert,
+                hidden_act=config.hidden_act,
+                up_proj_name='gate_up_proj',
+                is_gated=True,
+                down_proj_name='down_proj',
+                bias=False,
+                quant_config=quant_config,
+                reduce_results=False
+            )
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None
+    ) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+        shared_output = None
+        if self.shared_mlp is not None:
+            shared_output = self.shared_mlp(hidden_states)
+
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.forward_experts(hidden_states, router_logits, residual)
+
+        if shared_output is not None:
+            final_hidden_states = final_hidden_states + shared_output
+        final_hidden_states = self.reduce_results(final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+
+class MLUHunYuanAttention(HunYuanAttention):
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        kv_states: Optional[Tuple[torch.Tensor]] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        if self.attention_type == "self":
+            qkv, _ = self.qkv_proj(hidden_states, smooth_quant_scale)
+            q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: pack q & k to fit tmo.apply_rotary
+            '''
+            qk, _ = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)
+            self.rotary_emb(positions, qk.view(
+                -1, self.num_heads + self.num_kv_heads, self.head_dim))
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            ori_k = k
+            if self.use_qk_norm:
+                q = self.query_layernorm(
+                        q.reshape(-1, self.num_heads, self.head_dim).contiguous()
+                    ).reshape(-1, self.num_heads*self.head_dim)
+                k = self.key_layernorm(
+                        k.reshape(-1, self.num_kv_heads, self.head_dim).contiguous()
+                    ).reshape(-1, self.num_kv_heads*self.head_dim)
+        elif self.attention_type == "cross":
+            assert kv_states is not None
+            ori_k, v = kv_states # use last layer kv,
+            k = ori_k
+            q, _ = self.q_proj(hidden_states, smooth_quant_scale)
+            k_tmp = torch.empty_like(k) # Todo: reduant rotary embedding
+            qk_temp = torch.cat((q, k_tmp), dim=-1)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: pack q & k to fit tmo.apply_rotary
+            '''
+            self.rotary_emb(positions, qk_temp.view(
+                -1, self.num_heads + self.num_kv_heads, self.head_dim))
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            if self.use_qk_norm:
+                q = self.query_layernorm(
+                        q.view(-1, self.num_heads, self.head_dim).contiguous()
+                    ).reshape(-1, self.num_heads*self.head_dim)
+                k = self.key_layernorm(
+                        k.view(-1, self.num_kv_heads, self.head_dim).contiguous()
+                    ).reshape(-1, self.num_kv_heads*self.head_dim)
+        else:
+            raise RuntimeError("Not support attnention type")
+        attn_output = self.attn(q, k, v)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add residual
+        '''
+        output, _ = self.o_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output, (ori_k, v)
+
+
+class MLUHunYuanDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        layer_id: int = -1,
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        if rope_scaling is not None and getattr(
+                config, "original_max_position_embeddings", None):
+            rope_scaling["original_max_position_embeddings"] = (
+                config.original_max_position_embeddings)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        # Support abacusai/Smaug-72B-v0.1 with attention_bias
+        # Support internlm/internlm-7b with bias
+        attention_bias = getattr(config, "attention_bias", False) or getattr(
+            config, "bias", False)
+        cla_factor = getattr(config, "cla_share_factor", 1)
+        attention_type = "cross" \
+            if layer_id >= 0 and layer_id % cla_factor != 0 else "self"
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUHunYuanAttention,
+                    MLUHunYuanSparseMoeBlock and FeedForward
+        '''
+        self.self_attn = MLUHunYuanAttention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=getattr(config, "num_key_value_heads",
+                                 config.num_attention_heads),
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            quant_config=quant_config,
+            bias=attention_bias,
+            cache_config=cache_config,
+            prefix=f"{prefix}.self_attn",
+            attention_type=attention_type,
+        )
+        if getattr(config, "num_experts", None):
+            self.mlp = MLUHunYuanSparseMoeBlock(config=config,
+                                                quant_config=quant_config)
+        else:
+            self.mlp = FeedForward(
+                hidden_size=config.hidden_size,
+                intermediate_size=config.intermediate_size,
+                hidden_act=config.hidden_act,
+                up_proj_name='gate_up_proj',
+                is_gated=True,
+                down_proj_name='down_proj',
+                bias=getattr(config, "mlp_bias", False),
+                prefix=f"{prefix}.mlp",
+                quant_config=quant_config
+            )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: prepare to perf per-tensor sq cases if suitable. For moe
+            model, we only do quant fusion in attn block.
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            proj = self.self_attn.qkv_proj if self.self_attn.attention_type == "self" \
+                else self.self_attn.q_proj
+            self.input_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps, proj)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        kv_states: Optional[Tuple[torch.Tensor]] = None,
+        residual: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return hunyuan_decoder_layer_forward_base(
+            positions=positions,
+            hidden_states=hidden_states,
+            input_layernorm=self.input_layernorm,
+            self_attn=self.self_attn,
+            post_layernorm=self.post_attention_layernorm,
+            mlp=self.mlp,
+            kv_states=kv_states,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+class MLUHunYuanModel(HunYuanModel):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        lora_config: Optional[LoRAConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        Module.__init__(self)
+        self.config = config
+        self.padding_idx = config.pad_token_id
+        lora_vocab = (lora_config.lora_extra_vocab_size *
+                      (lora_config.max_loras or 1)) if lora_config else 0
+        self.vocab_size = config.vocab_size + lora_vocab
+        self.org_vocab_size = config.vocab_size
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.embed_tokens = VocabParallelEmbedding(
+                self.vocab_size,
+                config.hidden_size,
+                org_num_embeddings=config.vocab_size,
+                quant_config=quant_config,
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUHunYuanDecoderLayer(config=config,
+                                               layer_id=int(
+                                                    prefix.split(".")[-1]),
+                                               cache_config=cache_config,
+                                               quant_config=quant_config,
+                                               prefix=prefix),
+            prefix=f"{prefix}.layers")
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor],
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        return hunyuan_decoder_model_forward_base_pp(
+            config=self.config,
+            input_ids=input_ids,
+            positions=positions,
+            intermediate_tensors=intermediate_tensors,
+            layers=self.layers,
+            start_layer=self.start_layer,
+            end_layer=self.end_layer,
+            get_input_embeddings=self.get_input_embeddings,
+            norm=self.norm,
+            inputs_embeds=inputs_embeds
+        )
+
+
+class MLUHunYuanForCausalLM(HunYuanForCausalLM, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = "") -> None:
+        Module.__init__(self)
+        SupportsLoRA.__init__(self)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = MLUHunYuanModel(config,
+                                    cache_config,
+                                    quant_config,
+                                    lora_config=lora_config,
+                                    prefix=maybe_prefix(prefix, "model"))
+        if get_pp_group().is_last_rank:
+            self.unpadded_vocab_size = config.vocab_size
+            if lora_config:
+                self.unpadded_vocab_size += lora_config.lora_extra_vocab_size
+            self.lm_head = ParallelLMHead(
+                self.unpadded_vocab_size,
+                config.hidden_size,
+                org_num_embeddings=config.vocab_size,
+                padding_size=DEFAULT_VOCAB_PADDING_SIZE
+                # We need bigger padding if using lora for kernel
+                # compatibility
+                if not lora_config else lora_config.lora_vocab_padding_size,
+                quant_config=quant_config,
+            )
+            if config.tie_word_embeddings:
+                self.lm_head.weight = self.model.embed_tokens.weight
+
+            logit_scale = getattr(config, "logit_scale", 1.0)
+            self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,
+                                                    config.vocab_size,
+                                                    logit_scale)
+        else:
+            self.lm_head = PPMissingLayer()
+
+    def load_weights(
+        self,
+        weights: Iterable[Tuple[str, torch.Tensor]]
+    ):
+        cla_factor = getattr(self.config, "cla_share_factor", 1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params and cal start expert id
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params()
+
+        # expert parallel modification start
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        moe_ep_rank = moe_group_info.moe_ep_rank
+        num_total_experts = self.config.num_experts
+        start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+        # expert parallel modification end
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            (".qkv_proj", ".q_proj", "q"),
+            (".qkv_proj", ".k_proj", "k"),
+            (".qkv_proj", ".v_proj", "v"),
+            (".gate_up_proj", ".gate_proj", 0),
+            (".gate_up_proj", ".up_proj", 1),
+        ]
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: delete expert_params_mapping for useless
+        '''
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        params_dict = dict(self.named_parameters())
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            if ("rotary_emb.cos_cached" in name
+                    or "rotary_emb.sin_cached" in name):
+                # Models trained using ColossalAI may include these tensors in
+                # the checkpoint. Skip them.
+                continue
+            # With tie_word_embeddings, we can skip lm_head.weight
+            # The weight might appear unnecessarily in the files if the model is
+            # processed with quantization, LoRA, fine-tuning, etc.
+            if self.config.tie_word_embeddings and "lm_head.weight" in name:
+                continue
+            if (self.quant_config is not None and
+                    (scale_name := self.quant_config.get_cache_scale(name))):
+                # Loading kv cache scales for compressed-tensors quantization
+                param = params_dict[scale_name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                loaded_weight = loaded_weight[0]
+                weight_loader(param, loaded_weight)
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace expert_id in weight to named_expert_id in params_dict
+            '''
+            if start_expert_id > 0 and "mlp.experts." in name:
+                expert_str = re.search(r'experts\.\d+', name).group(0)
+                expert_id = int(expert_str.split(".")[1])
+                named_expert_id = expert_id - start_expert_id
+                old_expert_name = f"experts.{expert_id}"
+                new_expert_name = f"experts.{named_expert_id}"
+                name = name.replace(old_expert_name, new_expert_name)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: delete if "mlp.experts" in name: continue condition
+                '''
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                # cross layer only have q_proj, skip qkv pack
+                if weight_name == ".q_proj":
+                    match = re.search(r'layers\.\d+', name)
+                    if match:
+                        layer_id = int(match.group(0).split('.')[-1])
+                        if cla_factor > 1 and layer_id % cla_factor != 0:
+                            continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if (name.endswith(".bias") and name not in params_dict):
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                # Skip experts that are not assigned to this worker.
+                if (("mlp.experts." in name or "mlp.shared_mlp." in name)
+                        and name not in params_dict):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                if (name.endswith(".bias") and name not in params_dict):
+                    continue
+                # Remapping the name of FP8 kv-scale.
+                name = maybe_remap_kv_scale_name(name, params_dict)
+                if name is None:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                if "mlp.gate.wg." in name:
+                    name = name.replace("wg.", "")
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition
+                '''
+                # Skip experts that are not assigned to this worker.
+                if (("mlp.experts." in name or "mlp.shared_mlp." in name)
+                        and name not in params_dict):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params after loading
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params_after_loading()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    def get_experts_num(self):
+        if hasattr(self.model_config, "num_experts"):
+            return self.model_config.num_experts
+        return 0
+
+    def get_topk_num(self):
+        return self.model_config.moe_topk
+
+    def get_moe_num(self):
+        return self.get_layer_num()
+
+    def get_ffn_num(self):
+        return 0
+
+    def get_cla_coefficient(self):
+        return 0.5
+
+    def get_moe_inner_size(self):
+        return self.model_config.intermediate_size
+
+    def get_shared_experts_inner_size(self):
+        if hasattr(self.model_config, "num_shared_expert") and self.model_config.num_shared_expert is not None:
+            return self.model_config.num_shared_expert * self.model_config.intermediate_size
+        return 0

