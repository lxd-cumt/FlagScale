diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py
new file mode 100644
index 000000000..1f2bbc5ed
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen3_moe.py
@@ -0,0 +1,1065 @@
+import re
+from typing import Iterable, Optional, Set, Tuple, Union, Dict, Any, Callable
+
+
+import torch
+from torch import nn
+import torch.nn.functional as F
+from transformers import PretrainedConfig
+
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import (
+    get_tp_group, get_pp_group, get_dp_group, get_tensor_model_parallel_world_size,
+    get_tensor_model_parallel_rank, get_tp_world_group, get_tp_world_rank,
+    get_data_parallel_group_rank)
+from vllm.distributed.communication_op import(
+    tensor_model_parallel_all_reduce, tensor_model_parallel_reduce_scatter)
+from vllm.logger import init_logger
+from vllm.attention import Attention
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+from vllm.model_executor.layers.fused_moe.layer import FusedMoE
+from vllm.model_executor.layers.linear import (QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.sequence import IntermediateTensors
+
+from vllm.model_executor.models.interfaces import SupportsPP
+from vllm.model_executor.models.utils import (extract_layer_index,
+                    is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+from vllm.model_executor.models.qwen3_moe import (
+    Qwen3MoeAttention, Qwen3MoeForCausalLM, Qwen3MoeModel)
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.models.dp_utils import(
+    enable_data_parallel, get_dp_metadata, DataParallelRuntimeParams,
+    tensor_model_parallel_all_gather_dp, remove_paddings_after_all_gather)
+from vllm_mlu.model_executor.models.utils import is_tie_word_embeddings
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp, MoeGroupInfo
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+from vllm_mlu._mlu_utils import VLLM_MOE_PREFILL_CHUNK_SIZE
+
+logger = init_logger(__name__)
+
+def chunked_mlp(self, input: torch.Tensor, is_prefill: bool, **kwargs):
+    """
+    divides input into chunks in the leading dimension (dimension 0), and
+    compute the chunks in a loop, instead of in a batch at once.
+    """
+    chunk_size = VLLM_MOE_PREFILL_CHUNK_SIZE
+    total = input.shape[0]
+    if not is_prefill or chunk_size <= 0 or chunk_size >= total:
+        return self.mlp(input, reduce_results=False, **kwargs)
+
+    output = input.new_empty(total, self.hidden_size)
+    num_chunks = (total + chunk_size - 1) // chunk_size
+
+    for i in range(num_chunks):
+        start = i * chunk_size
+        end = min((i + 1) * chunk_size, total)
+        output[start : end] = self.mlp(input[start : end], reduce_results=False, **kwargs)
+
+    return output
+
+def dp_gather_moe_input_comm(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    hidden_states: torch.Tensor,
+    residual: torch.Tensor,
+):
+    """
+    Do post attention layernorm and gather expert inputs.
+    """
+    dtype = hidden_states.dtype
+    device = hidden_states.device
+    hidden_size = self.hidden_size
+    # add residual before reduce_sum(reduce_scatter or all_reduce called below)
+    if get_tensor_model_parallel_rank() == 0:
+        hidden_states = hidden_states + residual
+
+    if dp_params.prefill_pad_to_token_num != -1:
+        # use reduce_scatter + global all_gather to reduce memory consumption
+        # pad hidden_states to use reduce_scatter and global all gather
+        pad_num = dp_params.prefill_pad_to_token_num - dp_params.token_num
+        if pad_num < 0:
+            raise ValueError("pad_num must be non-negative")
+        if pad_num != 0:
+            hidden_states = F.pad(hidden_states, (0, 0, 0, pad_num))
+
+        # scatter hidden_states in tp group on dim 0
+        hidden_states = tensor_model_parallel_reduce_scatter(
+            hidden_states, dim=0)
+
+        # gather hidden_states in tp world group
+        hidden_states = tensor_model_parallel_all_gather_dp(
+            group_num_tokens=dp_params.attn_token_split_list_reduce_scatter,
+            rank=get_tp_world_rank(),
+            hidden_states=hidden_states,
+            group=get_tp_world_group(),
+        )
+
+        # get origin hidden_states for moe compute
+        hidden_states = remove_paddings_after_all_gather(
+            hidden_states,
+            dp_params.prefill_pad_to_token_num,
+            dp_params.token_split_list
+        )
+    else:
+        # use all_reduce + all_gather
+        hidden_states = tensor_model_parallel_all_reduce(
+            hidden_states)
+
+        hidden_states = tensor_model_parallel_all_gather_dp(
+            group_num_tokens=dp_params.token_split_list,
+            rank=get_data_parallel_group_rank(),
+            hidden_states=hidden_states,
+            group=get_dp_group(),
+            hidden_size=hidden_size,
+            dtype=dtype,
+            device=device,
+        )
+
+    # post attention layernorm
+    residual = hidden_states[dp_params.token_num_offset:
+                             dp_params.token_num_offset+dp_params.token_num]
+    hidden_states = self.post_attention_layernorm(hidden_states)
+    return hidden_states, residual
+
+def dp_scatter_moe_output_comm(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    hidden_states: torch.Tensor
+):
+    """
+    Scatter the output of experts.
+    """
+    hidden_states = tensor_model_parallel_all_reduce(
+        hidden_states, tp_group=get_tp_world_group())
+
+    hidden_states = hidden_states[dp_params.token_num_offset:
+                                  dp_params.token_num_offset+dp_params.token_num]
+    return hidden_states
+
+def dp_forward_layer_comm(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    residual: Optional[torch.Tensor] = None,
+):
+    # Self Attention
+    if residual is None:
+        residual = hidden_states
+        hidden_states = self.input_layernorm(hidden_states)
+    else:
+        hidden_states, residual = self.input_layernorm(
+            hidden_states, residual)
+    hidden_states = self.self_attn(
+        positions=positions,
+        hidden_states=hidden_states,
+    )
+
+    # Gather moe input and do post attention layernorm
+    hidden_states, residual = dp_gather_moe_input_comm(
+        self, dp_params, hidden_states, residual)
+
+    # Forward experts
+    is_prefill: bool = dp_params.dp_is_prefill[get_data_parallel_group_rank()]
+    hidden_states = chunked_mlp(self, hidden_states, is_prefill)
+
+    # Scatter moe output
+    hidden_states = dp_scatter_moe_output_comm(self, dp_params, hidden_states)
+
+    return hidden_states, residual
+
+def dp_gather_moe_input_opt(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    hidden_states: torch.Tensor,
+    residual: torch.Tensor,
+):
+    """
+    Do post attention layernorm and gather expert inputs.
+    Use reduce_scatter + global all_gather to reduce memory consumption.
+    """
+    # scatter hidden_states in tp group on dim 0
+    hidden_states = tensor_model_parallel_reduce_scatter(
+        hidden_states, dim=0)
+
+    # post attention layernorm
+    hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
+
+    # global all gather
+    hidden_states = tensor_model_parallel_all_gather_dp(
+        group_num_tokens=dp_params.attn_token_split_list_reduce_scatter,
+        rank=get_tp_world_rank(),
+        hidden_states=hidden_states,
+        group=get_tp_world_group(),
+    )
+
+    return hidden_states, residual
+
+def dp_scatter_moe_output_opt(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    hidden_states: torch.Tensor,
+    residual: torch.Tensor,
+    layernorm_for_next_layer: Optional[Callable] = None,
+):
+    """
+    Scatter the output of experts.
+    """
+    # global reduce scatter
+    hidden_states = tensor_model_parallel_reduce_scatter(
+        hidden_states, dim=0, tp_group=get_tp_world_group())
+
+    # layernorm
+    if layernorm_for_next_layer:
+        hidden_states, residual = layernorm_for_next_layer(hidden_states, residual)
+    else:
+        hidden_states = hidden_states + residual
+        residual = None
+
+    # all gather in tp group
+    hidden_states = tensor_model_parallel_all_gather_dp(
+        group_num_tokens=dp_params.moe_token_split_list_reduce_scatter,
+        rank=get_tensor_model_parallel_rank(),
+        hidden_states=hidden_states,
+        group=get_tp_group(),
+    )
+    return hidden_states, residual
+
+def dp_forward_layer_opt(
+    self,
+    dp_params: DataParallelRuntimeParams,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    residual: Optional[torch.Tensor] = None,
+    layernorm_for_next_layer: Optional[Callable] = None,
+):
+    # Self Attention
+    if residual is None:
+        '''
+        first layer
+        split residual by tp_world_size to adapt to reduce_scatter
+        '''
+        tp_world_size = get_tensor_model_parallel_world_size()
+        tp_rank = get_tensor_model_parallel_rank()
+        tokens_per_tp_rank = hidden_states.shape[0] // tp_world_size
+        residual = hidden_states[tokens_per_tp_rank * tp_rank:
+                                 tokens_per_tp_rank * (tp_rank + 1)]
+        hidden_states = self.input_layernorm(hidden_states)
+    else:
+        '''
+        not first layer
+        input_layernorm will be executed within function dp_scatter_moe_output_opt,
+        which is the last function called in the previous layer.
+        '''
+        pass
+
+    hidden_states = self.self_attn(
+        positions=positions,
+        hidden_states=hidden_states,
+    )
+
+    # Gather expert input and do post attention layernorm
+    hidden_states, residual = dp_gather_moe_input_opt(
+        self, dp_params, hidden_states, residual)
+
+    # Forward experts
+    is_prefill: bool = dp_params.dp_is_prefill[get_data_parallel_group_rank()]
+    hidden_states = chunked_mlp(self, hidden_states, is_prefill)
+
+    # Scatter expert output and do next layernrom
+    hidden_states, residual = dp_scatter_moe_output_opt(
+        self, dp_params, hidden_states, residual, layernorm_for_next_layer)
+
+    return hidden_states, residual
+
+def get_input_layernorm_for_next_layer(self, layer_id):
+    if layer_id < 0 or layer_id >= len(self.layers):
+        raise ValueError("layer_id out of range")
+    if layer_id == self.end_layer - 1:
+        return self.norm if get_pp_group().is_last_rank else None
+    else:
+        return self.layers[layer_id + 1].input_layernorm
+
+
+class MLUQwen3MoeSparseMoeBlock(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        # set tp_world_group to mlp
+        tp_group = get_tp_world_group() if enable_data_parallel() else None
+        super().__init__(num_experts=config.num_experts,
+                         top_k=config.num_experts_per_tok,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=config.norm_topk_prob,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True,
+                         tp_group=tp_group)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        reduce_results = True
+    ) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.forward_experts(hidden_states, router_logits, residual)
+
+        if reduce_results:
+            final_hidden_states = self.reduce_results(final_hidden_states)
+
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+class MLUQwen3MoeAttention(Qwen3MoeAttention):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        head_dim: Optional[int] = None,
+        rms_norm_eps: float = 1e-06,
+        qkv_bias: bool = False,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        nn.Module.__init__(self)
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+
+        self.qkv_proj = QKVParallelLinear(hidden_size,
+                                          self.head_dim,
+                                          self.total_num_heads,
+                                          self.total_num_kv_heads,
+                                          bias=qkv_bias,
+                                          quant_config=quant_config,
+                                          prefix=f"{prefix}.qkv_proj")
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: do not reduce results when dp enabled
+        '''
+        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                        hidden_size,
+                                        bias=False,
+                                        reduce_results= not enable_data_parallel(),
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.o_proj")
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn")
+
+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                           self.head_dim)
+        q_by_head = self.q_norm(q_by_head)
+        q = q_by_head.view(q.shape)
+
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                           self.head_dim)
+        k_by_head = self.k_norm(k_by_head)
+        k = k_by_head.view(k.shape)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        '''
+        qk = torch.cat([q, k], dim=-1)
+        self.rotary_emb(positions, qk.view(-1, self.num_heads + self.num_kv_heads, self.head_dim))
+        q, k = qk.split([self.q_size, self.kv_size], dim=-1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        attn_output = self.attn(q, k, v)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add residual
+        '''
+        output, _ = self.o_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+
+class MLUQwen3MoeDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        self.self_attn = MLUQwen3MoeAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+
+        # `mlp_only_layers` in the config.
+        layer_idx = extract_layer_index(prefix)
+        mlp_only_layers = ([] if not hasattr(config, "mlp_only_layers") else
+                           config.mlp_only_layers)
+        if (layer_idx not in mlp_only_layers) and (
+                config.num_experts > 0 and
+            (layer_idx + 1) % config.decoder_sparse_step == 0):
+            self.mlp = MLUQwen3MoeSparseMoeBlock(config=config,
+                                                 quant_config=quant_config,
+                                                 prefix=f"{prefix}.mlp")
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: use FeedForward instead of MLP
+            '''
+            self.mlp = FeedForward(
+                hidden_size=config.hidden_size,
+                intermediate_size=config.intermediate_size,
+                hidden_act=config.hidden_act,
+                up_proj_name='gate_up_proj',
+                is_gated=True,
+                down_proj_name='down_proj',
+                bias=False,
+                quant_config=quant_config,
+            )
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief:
+            prepare to perf per-tensor sq cases if suitable. For moe
+                model, we only do quant fusion in attn block;
+            bypass residual in matmul logics if fp8_block_wise quantization;
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if not enable_data_parallel() and is_smoothquant(quant_config):
+            self.input_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.self_attn.qkv_proj)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        self.is_fp8_block_wise = isinstance(quant_config, Fp8Config) \
+            and quant_config.weight_block_size is not None
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief:
+            bypass residual in matmul logics if fp8_block_wise quantization;
+            perf model by:
+                1) add residual in matmul;
+                2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        if self.is_fp8_block_wise:
+            # Self Attention
+            if residual is None:
+                residual = hidden_states
+                hidden_states = self.input_layernorm(hidden_states)
+            else:
+                hidden_states, residual = self.input_layernorm(
+                    hidden_states, residual)
+
+            hidden_states = self.self_attn(
+                positions=positions,
+                hidden_states=hidden_states,
+            )
+
+            # Fully Connected
+            hidden_states, residual = self.post_attention_layernorm(
+                hidden_states, residual)
+            hidden_states = self.mlp(hidden_states)
+            return hidden_states, residual
+
+        return decoder_layer_forward_base(
+            positions=positions,
+            hidden_states=hidden_states,
+            input_layernorm=self.input_layernorm,
+            self_attn=self.self_attn,
+            post_layernorm=self.post_attention_layernorm,
+            mlp=self.mlp,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+@support_torch_compile
+class MLUQwen3MoeModel(Qwen3MoeModel):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        nn.Module.__init__(self)
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.padding_idx = config.pad_token_id
+        self.vocab_size = config.vocab_size
+        self.config = config
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+            prefix=f"{prefix}.embed_tokens")
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUQwen3MoeDecoderLayer(config=config,
+                                                   cache_config=cache_config,
+                                                   quant_config=quant_config,
+                                                   prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief:
+            bypass residual in matmul logics if fp8_block_wise quantization;
+        '''
+        self.is_fp8_block_wise = isinstance(quant_config, Fp8Config) \
+            and quant_config.weight_block_size is not None
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        # save dp config
+        """Number of data parallel groups."""
+        self.data_parallel_size = vllm_config.parallel_config.data_parallel_size
+        """Rank of the data parallel group."""
+        self.data_parallel_rank = vllm_config.parallel_config.data_parallel_rank
+        """Number of tensor parallel groups."""
+        self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size
+        """Reduce_scatter and All_gather"""
+        self.prefill_dispatch_use_RS_AG = vllm_config.mlu_config.prefill_dispatch_use_RS_AG
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief non data parallel
+        '''
+        if not enable_data_parallel():
+            if self.is_fp8_block_wise:
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                if get_pp_group().is_first_rank:
+                    if inputs_embeds is not None:
+                        hidden_states = inputs_embeds
+                    else:
+                        hidden_states = self.get_input_embeddings(input_ids)
+                    residual = None
+                else:
+                    assert intermediate_tensors is not None
+                    hidden_states = intermediate_tensors["hidden_states"]
+                    residual = intermediate_tensors["residual"]
+                for i in range(self.start_layer, self.end_layer):
+                    layer = self.layers[i]
+                    hidden_states, residual = layer(positions, hidden_states, residual)
+                if not get_pp_group().is_last_rank:
+                    return IntermediateTensors({
+                        "hidden_states": hidden_states,
+                        "residual": residual
+                    })
+                hidden_states, _ = self.norm(hidden_states, residual)
+                return hidden_states
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief non fp8 block case
+            '''
+            return decoder_model_forward_base_pp(
+                input_ids=input_ids,
+                positions=positions,
+                intermediate_tensors=intermediate_tensors,
+                layers=self.layers,
+                start_layer=self.start_layer,
+                end_layer=self.end_layer,
+                get_input_embeddings=self.embed_tokens,
+                norm=self.norm,
+                inputs_embeds=inputs_embeds,
+            )
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: enable data parallel
+            '''
+            if dp_params is None:
+                dp_params = get_dp_metadata(positions.numel(),
+                                            self.data_parallel_size,
+                                            self.data_parallel_rank,
+                                            self.tensor_parallel_size,
+                                            self.prefill_dispatch_use_RS_AG)
+
+            if get_pp_group().is_first_rank:
+                if inputs_embeds is not None:
+                    hidden_states = inputs_embeds
+                else:
+                    hidden_states = self.get_input_embeddings(input_ids)
+                residual = None
+            else:
+                assert intermediate_tensors is not None
+                hidden_states = intermediate_tensors["hidden_states"]
+                residual = intermediate_tensors["residual"]
+
+            for i in range(self.start_layer, self.end_layer):
+                layer = self.layers[i]
+                if dp_params.layer_use_reduce_scatter:
+                    layernorm_for_next_layer = get_input_layernorm_for_next_layer(self, i)
+                    hidden_states, residual = dp_forward_layer_opt(
+                        layer, dp_params, positions, hidden_states,
+                        residual, layernorm_for_next_layer)
+                else:
+                    hidden_states, residual = dp_forward_layer_comm(
+                        layer, dp_params, positions, hidden_states, residual)
+
+            if not get_pp_group().is_last_rank:
+                return IntermediateTensors({
+                    "hidden_states": hidden_states,
+                    "residual": residual
+                })
+
+            # Skip layernorm if using reduce-scatter as it's already fused in dp_forward_layer_opt
+            if not dp_params.layer_use_reduce_scatter:
+                hidden_states, _ = self.norm(hidden_states, residual)
+            return hidden_states
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+class MLUQwen3MoeForCausalLM(Qwen3MoeForCausalLM, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        nn.Module.__init__(self)
+        SupportsPP.__init__(self)
+        config = vllm_config.model_config.hf_config
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        self.model = MLUQwen3MoeModel(vllm_config=vllm_config,
+                                      prefix=maybe_prefix(prefix, "model"))
+        self.lm_head = ParallelLMHead(config.vocab_size,
+                                      config.hidden_size,
+                                      quant_config=quant_config)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: workaround for internvl3.5 multimodal models for
+          is_tie_word_embeddings config
+        '''
+        if is_tie_word_embeddings(vllm_config.model_config,
+                                  self.config.tie_word_embeddings):
+            self.lm_head.weight = self.model.embed_tokens.weight
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params and cal start expert id
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params()
+
+        # expert parallel modification start
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        moe_ep_rank = moe_group_info.moe_ep_rank
+        num_total_experts = self.config.num_experts
+        start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+        # expert parallel modification end
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: expert_params_mapping for fp8 block_wise
+        '''
+        is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) and \
+            self.quant_config.weight_block_size is not None
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts) if is_fp8_block_wise else {}
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace expert_id in weight to named_expert_id in params_dict
+            '''
+            if start_expert_id > 0 and "mlp.experts." in name:
+                expert_str = re.search(r'experts\.\d+', name).group(0)
+                expert_id = int(expert_str.split(".")[1])
+                named_expert_id = expert_id - start_expert_id
+                if named_expert_id < 0:
+                    continue
+                old_expert_name = f"experts.{expert_id}"
+                new_expert_name = f"experts.{named_expert_id}"
+                name = name.replace(old_expert_name, new_expert_name)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                if is_fp8_block_wise and ("mlp.experts." in name):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                    continue
+                # Skip layers on other devices.
+                if is_pp_missing_parameter(name, self):
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                # Skip experts that are not assigned to this worker.
+                if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                        and name not in params_dict):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief:
+                    handle the experts in expert_params_mapping;
+                    add expert skiped condition;
+                '''
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Remapping the name of FP8 kv-scale.
+                    if name.endswith("kv_scale"):
+                        remapped_kv_scale_name = name.replace(
+                            ".kv_scale", ".attn.kv_scale")
+                        if remapped_kv_scale_name not in params_dict:
+                            logger.warning_once(
+                                "Found kv scale in the checkpoint "
+                                f"(e.g. {name}), but not found the expected "
+                                f"name in the model "
+                                f"(e.g. {remapped_kv_scale_name}). "
+                                "kv-scale is not loaded.")
+                            continue
+                        else:
+                            name = remapped_kv_scale_name
+
+                    # Skip experts that are not assigned to this worker.
+                    if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                            and name not in params_dict):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+            loaded_params.add(name)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params after loading
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params_after_loading()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return loaded_params
+
+    def get_head_size(self):
+        return self.model_config.head_dim
+
+    def get_experts_num(self):
+        if hasattr(self.model_config, "num_experts"):
+            return self.model_config.num_experts
+        return 0
+
+    def get_moe_num(self):
+        return self.get_layer_num()
+
+    def get_ffn_num(self):
+        return 0
+
+    def forward(self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add dp_params
+        '''
+        hidden_states = self.model(
+            input_ids=input_ids,
+            positions=positions,
+            intermediate_tensors=intermediate_tensors,
+            inputs_embeds=inputs_embeds,
+            dp_params=dp_params
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return hidden_states

