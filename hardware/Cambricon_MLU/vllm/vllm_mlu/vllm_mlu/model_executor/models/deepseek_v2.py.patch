diff --git a/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py
new file mode 100644
index 000000000..5f76b7271
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_v2.py
@@ -0,0 +1,2990 @@
+import enum
+import re
+import types
+from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+import torch.nn.functional as F
+from transformers import PretrainedConfig
+from dataclasses import replace
+
+import vllm.envs as envs
+from vllm.attention import Attention, AttentionMetadata
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, ModelConfig, VllmConfig
+from vllm.distributed import (
+    get_dp_group,
+    get_pp_group,
+    get_tp_group,
+    get_tensor_model_parallel_world_size,
+    get_data_parallel_group_rank,
+    get_data_parallel_group_world_size,
+    get_dense_mlp_tp_group,
+    get_dense_mlp_tp_world_size,
+    get_dense_mlp_tp_rank,
+    get_tp_world_rank,
+    get_tensor_model_parallel_rank,
+    get_tp_world_world_size,
+    get_tp_world_group,
+    get_parallel_rank_with_group,
+    divide
+)
+from vllm.distributed.communication_op import (
+    tensor_model_parallel_all_reduce,
+    tensor_model_parallel_reduce_scatter,
+)
+from vllm.forward_context import ForwardContext, get_forward_context
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (
+    ColumnParallelLinear,
+    ReplicatedLinear,
+    RowParallelLinear,
+)
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+from vllm.model_executor.layers.quantization.utils.fp8_utils import scaled_dequantize
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.models.deepseek_v2 import (
+    DeepseekV2MLAAttention,
+    DeepseekV2DecoderLayer,
+)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.models.deepseek_v2 import (
+    DeepseekV2Attention,
+    DeepseekV2ForCausalLM,
+    get_spec_layer_idx_from_weight_name,
+    yarn_get_mscale,
+)
+from vllm.model_executor.models.interfaces import SupportsPP
+from vllm.model_executor.models.utils import (
+    PPMissingLayer,
+    is_pp_missing_parameter,
+    make_empty_intermediate_tensors_factory,
+    make_layers,
+    maybe_prefix,
+)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+from vllm.utils import get_dtype_size
+from vllm.logger import logger
+from vllm.forward_context import set_forward_context
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM, data_type_byte_width_map
+from vllm_mlu import _mlu_ops as mlu_ops
+from vllm_mlu._mlu_utils import (
+    VLLM_MOE_PREFILL_CHUNK_SIZE, VLLM_AVG_MOE_EN, VLLM_RANDOM_MOE_EN
+)
+from vllm_mlu.mlu_forward_context import MLUDPMetadata
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import (
+    SparseMoeMlp,
+    MoeGroupInfo,
+)
+from vllm_mlu.model_executor.models.layer_utils import (
+    compute_in_loop,
+    is_per_token_smoothquant,
+)
+from vllm_mlu.model_executor.models.dp_utils import (
+    DataParallelRuntimeParams,
+    enable_data_parallel,
+    get_dp_metadata,
+    process_post_attention_communication,
+    tensor_model_parallel_all_gather_dp,
+    tensor_model_parallel_all_gather_op_v2,
+)
+from vllm_mlu.model_executor.layers.rotary_embedding import MLURotaryEmbedding
+import vllm_mlu.model_executor.models.partition_utils as partition_utils
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+from vllm_mlu.model_executor.layers.dp_vocab_parallel_embedding import (
+    DPVocabParallelEmbedding,
+    DPParallelLMHead,
+)
+from vllm_mlu.model_executor.layers.dp_logits_processor import DPLogitsProcessor
+from vllm_mlu.model_executor.layers.quantization.smoothquant import SmoothQuantConfig
+from vllm_mlu.v1.attention.backends.utils import (MLUCommonAttentionMetadata,
+                                                  get_common_metadata)
+from vllm.attention.backends.abstract import is_quantized_kv_cache
+from vllm_mlu.v1.attention.backends.mla.common import MLACommonMetadata
+from vllm_mlu.v1.attention.backends.utils import (
+    MLUCommonAttentionMetadata, get_common_metadata,
+    MLUInferMode,
+)
+from vllm_mlu.distributed.parallel_state import(
+    CnclEP, cnclep_dispatch, cnclep_combine, cnclep_combine
+)
+
+
+def forward_decoder(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+) -> torch.Tensor:
+    q_len = hidden_states.shape[0]
+    q_input = hidden_states.new_empty(
+        q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
+    )
+    if self.q_lora_rank is not None:
+        q = self.q_a_proj(hidden_states)[0]
+        q_scale = None
+        if self.use_quant_fusion_rmsnorm:
+            q, q_scale = self.q_a_layernorm(q)
+            q = self.q_b_proj(q, q_scale)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(
+            -1, self.num_local_heads, self.qk_head_dim
+        )
+    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
+    torch.bmm(q_nope.transpose(0, 1), self.w_kc, out=q_input[..., : self.kv_lora_rank].transpose(0, 1))
+
+    latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+    v_input = latent_cache[..., : self.kv_lora_rank]
+    k_input = latent_cache
+    self.kv_a_layernorm(v_input, out=k_input[..., : self.kv_lora_rank])
+
+    k_input = k_input.unsqueeze(1)
+    k_pe = k_input[..., self.kv_lora_rank :]
+    q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe, only_decode=True)
+    q_input[..., self.kv_lora_rank :] = q_pe
+
+    q_quant_scale = None
+    q_input = q_input.reshape(q_input.shape[0], -1)
+    k_input = k_input.reshape(k_input.shape[0], -1)
+    v_input = v_input.reshape(v_input.shape[0], -1)
+
+    decode_kwargs = {"only_decode": True, "q_quant_scale": q_quant_scale}
+    attn_output = self.attn_decoder(q_input, k_input, v_input, kwargs=decode_kwargs)
+    attn_output = attn_output.reshape(-1, self.num_local_heads,
+                                      self.kv_lora_rank)
+    attn_bmm_output = torch.empty(
+        q_len, self.num_local_heads, self.v_head_dim, device=attn_output.device, dtype=attn_output.dtype)
+    torch.bmm(attn_output.transpose(0, 1), self.w_vc.transpose(1, 2), out=attn_bmm_output.transpose(0, 1))
+    attn_output = attn_bmm_output.flatten(1, 2)
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+def forward_decoder_fused_mla_kv(self, kv_latent_cache, position, kv_cache, attn_metadata):
+    head_num = 1
+    if kv_latent_cache.dim() == 2:
+        batch, head_size = kv_latent_cache.shape
+        seq = 1
+    else:
+        batch, seq, head_size = kv_latent_cache.shape
+
+    kv = kv_latent_cache.view(batch, seq, head_num, head_size)
+
+    position_id, interleaved, _ = self.rotary_emb.get_param(position)
+    sin = self.rotary_emb.sin_
+    cos = self.rotary_emb.cos_
+
+    gamma = self.kv_a_layernorm.weight.data
+    key_cache = None
+    key_cache_scale = None
+    if kv_cache[0].numel() > 0:
+        kv_cache_, kv_cache_scale_ = kv_cache
+        key_cache = kv_cache_[0]
+        if kv_cache_scale_.numel() > 0:
+            key_cache_scale = kv_cache_scale_[0]
+
+    is_paged_cache = True
+    slot_mapping = None
+    cache_bs_id = None
+    cache_seq_offset = None
+    slot_mapping = attn_metadata.slot_mapping[:attn_metadata.num_decode_tokens]
+    slot_mapping = slot_mapping.view(batch, seq)
+    eps = self.kv_a_layernorm.variance_epsilon
+    return mlu_ops.fused_mla_kv(kv, sin, cos, position_id, gamma, key_cache, key_cache_scale, slot_mapping, cache_bs_id,
+                                cache_seq_offset, is_paged_cache, eps, interleaved)
+
+
+def forward_decoder_fused_mla_q(self, q, output, position, q_quant_scale: Optional[torch.Tensor] = None):
+    if q.dim() == 2:
+        batch, input_size = q.shape
+        seq = 1
+    else:
+        batch, seq, input_size = q.shape
+    q = q.view(batch, seq, -1)
+
+    quant_mode = "none"
+    if output is not None:
+        output = output.view(batch, seq, output.shape[1], output.shape[2])
+    if q_quant_scale is not None:
+        quant_mode = "dynamic_per_token"
+        q_quant_scale = q_quant_scale.view(batch, seq, -1)
+        assert output.dtype == torch.int8, "q_quant_scale is not None, but output is not int8"
+
+    gamma = self.q_a_layernorm.weight.data
+    smooth_quant_scale = None
+    weight_b_scale = None
+    if self.use_quant_fusion_rmsnorm:
+        smooth_quant_scale = self.q_b_proj.smooth.data
+        weight_b_scale = self.q_b_proj.per_channel_scale.data
+        weight_b = self.q_b_proj.qweight.data
+    else:
+        weight_b = self.q_b_proj.qweight.data
+
+    position_id, interleaved, _ = self.rotary_emb.get_param(position)
+    sin = self.rotary_emb.sin_
+    cos = self.rotary_emb.cos_
+    eps = self.q_a_layernorm.variance_epsilon
+    return mlu_ops.fused_mla_q(
+        q,
+        gamma,
+        smooth_quant_scale,
+        weight_b,
+        weight_b_scale,
+        self.weight_c,
+        sin,
+        cos,
+        position_id,
+        output,
+        eps,
+        interleaved,
+        quant_mode,
+        q_quant_scale,
+    )
+
+
+def forward_decoder_fused(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+) -> torch.Tensor:
+    q_len = hidden_states.shape[0]
+    is_fused_mla_q = False
+    q_quant_scale = None
+    q_input_dtype = hidden_states.dtype
+    # if decoder_attn_dtype and fused_mla_q, use int8 q_input and q_quant_scale inplace
+    if (
+        self.attn_decoder.decoder_attn_dtype
+        and self.q_lora_rank is not None
+        and self.use_quant_fusion_rmsnorm
+    ):
+        q_input_dtype = self.attn_decoder.decoder_attn_dtype
+        q_quant_scale = torch.empty(
+            q_len,
+            self.num_local_heads,
+            device=hidden_states.device,
+            dtype=torch.float32,
+        )
+    q_input = torch.empty(
+        q_len,
+        self.num_local_heads,
+        self.kv_lora_rank + self.qk_rope_head_dim,
+        device=hidden_states.device,
+        dtype=q_input_dtype,
+    )
+    if self.q_lora_rank is not None:
+        if self.use_fused_qkv_a:
+            assert self.pack_params_done, "q_a_proj and kv_a_proj weights hasn't merged"
+            qkv_latent_cache = self.qkv_a_proj(hidden_states)[0]
+            q = qkv_latent_cache[..., : self.q_lora_rank]
+            latent_cache = qkv_latent_cache[..., self.q_lora_rank:]
+        else:
+            q = self.q_a_proj(hidden_states)[0]
+            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+
+        if self.use_quant_fusion_rmsnorm:
+            forward_decoder_fused_mla_q(self, q, q_input, positions, q_quant_scale)
+            is_fused_mla_q = True
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(
+            -1, self.num_local_heads, self.qk_head_dim
+        )
+        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+
+    if is_fused_mla_q is False:
+        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
+        torch.bmm(q_nope.transpose(0, 1), self.w_kc, out=q_input[..., :self.kv_lora_rank].transpose(0, 1))
+        q_pe, _ = self.rotary_emb(positions, q_pe, None, only_decode=True)
+        q_input[..., self.kv_lora_rank:] = q_pe
+
+    forward_decoder_fused_mla_kv(self, latent_cache, positions, kv_cache, attn_metadata)
+
+    k_input = latent_cache
+    k_input = k_input.unsqueeze(1)
+
+    v_input = latent_cache[..., : self.kv_lora_rank]
+
+    q_input = q_input.reshape(q_input.shape[0], -1)
+    k_input = k_input.reshape(k_input.shape[0], -1)
+    v_input = v_input.reshape(v_input.shape[0], -1)
+
+    decode_kwargs = {"only_decode": True, "q_quant_scale": q_quant_scale}
+    attn_output = self.attn_decoder(q_input, k_input, v_input, kwargs=decode_kwargs)
+    attn_output = attn_output.reshape(-1, self.num_local_heads,
+                                      self.kv_lora_rank)
+    attn_bmm_output = torch.empty(
+        q_len, self.num_local_heads, self.v_head_dim, device=attn_output.device, dtype=attn_output.dtype)
+    torch.bmm(attn_output.transpose(0, 1), self.w_vc.transpose(1, 2), out=attn_bmm_output.transpose(0, 1))
+    attn_output = attn_bmm_output.flatten(1, 2)
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+def _compute_prefill_context(
+    self,
+    q: torch.Tensor,
+    kv_c_and_k_pe_cache: torch.Tensor,
+    prefill_metadata: AttentionMetadata,
+):
+    assert prefill_metadata is not None
+    assert prefill_metadata.context_chunk_seq_tot is not None
+    assert prefill_metadata.context_chunk_cu_seq_lens is not None
+    assert prefill_metadata.context_chunk_starts is not None
+    assert prefill_metadata.context_chunk_max_seq_lens is not None
+
+    output = None
+    iters = len(prefill_metadata.context_chunk_seq_tot)
+
+    # Fetch from attn_metadata directly, since it late bound by
+    # MLAAttentionState, grabbing it directly `attn_metadata` can avoid
+    # any weirdness around prefill_metadata caching
+    assert prefill_metadata.context_chunk_workspace is not None
+    workspace = prefill_metadata.context_chunk_workspace
+
+    # get the default common_metadata
+    common_metadata = get_common_metadata()
+
+    for i in range(iters):
+        toks = prefill_metadata.context_chunk_seq_tot[i]
+
+        mlu_ops.gather_cache(
+            kv_cache=kv_c_and_k_pe_cache,
+            dst=workspace,
+            block_table=prefill_metadata.block_tables,
+            cu_seq_lens=prefill_metadata.context_chunk_cu_seq_lens[i],
+            batch_size=prefill_metadata.num_prefills,
+            seq_starts=prefill_metadata.context_chunk_starts[i],
+            kv_cache_dtype=self.attn.kv_cache_dtype
+        )
+
+        kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]
+        k_pe = workspace[:toks][..., self.kv_lora_rank:].unsqueeze(1)
+
+        kv_nope = self.kv_b_proj(kv_c_normed)[0].view(
+            -1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)
+        k_nope, v = kv_nope.split([self.qk_nope_head_dim, self.v_head_dim],
+                                  dim=-1)
+
+        k = torch.cat((k_nope, k_pe.expand((*k_nope.shape[:-1], -1))), dim=-1)
+
+        q = q.reshape(-1, self.num_local_heads * self.qk_head_dim)
+        k = k.reshape(-1, self.num_local_heads * self.qk_head_dim)
+        v = v.reshape(-1, self.num_local_heads * self.v_head_dim)
+
+        # update the num_prefill_kv_tokens
+        #  so that we can adapt to the design of vllm community
+        adapt_common_metadata = replace(
+            common_metadata,
+            num_prefill_kv_tokens=prefill_metadata.context_chunk_max_seq_lens[i],
+        )
+        prefill_kwargs = {
+            "only_prefill": True,
+            "prefill_causal": False,
+            "cu_seq_lens_q": prefill_metadata.query_start_loc,
+            "cu_seq_lens_kv": prefill_metadata.context_chunk_cu_seq_lens[i],
+            "max_seq_len_q": prefill_metadata.max_query_len,
+            "max_seq_len_kv": prefill_metadata.context_chunk_max_seq_lens[i],
+            "return_lse": True,
+            "common_metadata": adapt_common_metadata,
+        }
+        attn_output, attn_softmax_lse = self.attn(q, k, v, kwargs=prefill_kwargs)
+        attn_output = attn_output.reshape(-1, self.num_local_heads, self.v_head_dim)
+        if output is None:
+            output = attn_output
+            output_lse = attn_softmax_lse
+        else:
+            output_tmp = torch.empty_like(output)
+            output_lse_tmp = torch.empty_like(output_lse)
+            mlu_ops.merge_attn_states(
+                output=output_tmp,
+                output_lse=output_lse_tmp,
+                prefix_output=output,
+                prefix_lse=output_lse,
+                suffix_output=attn_output,
+                suffix_lse=attn_softmax_lse,
+            )
+            output_lse = output_lse_tmp
+            output = output_tmp
+
+    return output, output_lse
+
+
+def forward_prefill(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+    kv_cache: torch.Tensor,
+    attn_metadata: AttentionMetadata,
+    prefill_k_tensor: torch.Tensor = None,
+    prefill_v_tensor: torch.Tensor = None,
+    req_seq_len: int = -1,
+    enable_prefill_mcc: bool = False,
+) -> torch.Tensor:
+    """
+    req_seq_len is used to indicates the request sequence length, which is only available
+    when sequence dimension is partition and in prefill stage. For example, for a request
+    with seq_len 20, and is partition into 3 parts - [7, 7, 6], and req_seq_len for each
+    call is [7, 14, 20] respectively.
+    """
+    if self.q_lora_rank is not None:
+        q = self.q_a_proj(hidden_states)[0]
+        if self.use_quant_fusion_rmsnorm:
+            q, q_scale = self.q_a_layernorm(q)
+            q = self.q_b_proj(q, q_scale)[0].view(-1, self.num_local_heads, self.qk_head_dim)
+        else:
+            q = self.q_a_layernorm(q)
+            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads,
+                                         self.qk_head_dim)
+    else:
+        q = self.q_proj(hidden_states)[0].view(-1, self.num_local_heads,
+                                               self.qk_head_dim)
+    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim],
+                           dim=-1)
+    latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
+    kv_a, _ = latent_cache.split(
+        [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
+    latent_cache = latent_cache.unsqueeze(1)
+    kv_a = self.kv_a_layernorm(kv_a)
+    kv = self.kv_b_proj(kv_a)[0]
+    kv = kv.view(-1, self.num_local_heads,
+                 self.qk_nope_head_dim + self.v_head_dim)
+    k_nope, v = kv.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)
+    k_pe = latent_cache[:, :, self.kv_lora_rank:]
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: Update rotary status.
+    '''
+    common_metadata, layer_metadata = partition_utils.get_common_and_layer_metadata(attn_metadata)
+    if enable_prefill_mcc:
+        if common_metadata is None:
+            common_metadata = MLUCommonAttentionMetadata(
+                query_start_loc=layer_metadata.query_start_loc,
+                seq_lens=None,  # Explicitly disabled for rotary embedding
+                seq_start_loc=None,  # Explicitly disabled for rotary embedding
+                max_query_len=layer_metadata.prefill.max_query_len,
+                num_actual_tokens=layer_metadata.num_actual_tokens,
+                num_input_tokens=0,  # Explicitly disabled for rotary embedding
+                infer_mode=MLUInferMode.PREFILL_ONLY,
+                num_prefill_query_tokens=layer_metadata.num_actual_tokens,
+                num_prefill_kv_tokens=max(req_seq_len, layer_metadata.num_actual_tokens),
+            )
+        # Only update rotary status for prefill-only stage.
+        if layer_metadata is None or layer_metadata.decode is None:
+            MLURotaryEmbedding.set_mlu_var_v1(common_metadata=common_metadata)
+            # Note: Since we use prefill_k/prefill_v to record prefix, we need to
+            # set chunk mode directly to use positions.
+            if (positions is not None
+                and hasattr(layer_metadata, 'num_prefills')
+                and layer_metadata.num_prefills == 1
+                and hasattr(layer_metadata, 'num_actual_tokens')
+                and req_seq_len is not None
+                and req_seq_len - layer_metadata.num_actual_tokens > 0):
+                MLURotaryEmbedding.is_chunked = True
+    q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe, only_prefill=True)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: MLA save cache before flashattn
+    '''
+    if len(kv_cache) != 0  and kv_cache[0].numel() > 0:
+        key_cache = kv_cache[0][0]
+        key_value = torch.concat((kv_a.unsqueeze(1), k_pe), dim=-1)
+        if isinstance(attn_metadata, MLACommonMetadata):
+            slot_mapping = attn_metadata.slot_mapping[attn_metadata.num_decode_tokens:]
+        else:
+            slot_mapping = attn_metadata.slot_mapping
+        updated_slot_mapping = slot_mapping[:key_value.size(0)]
+        if is_quantized_kv_cache(self.attn.kv_cache_dtype):
+            key_cache_scale = kv_cache[1][0]
+            mlu_ops.quant_to_paged_cache(key_value,
+                                         None,
+                                         key_cache,
+                                         None,
+                                         key_cache_scale,
+                                         None,
+                                         updated_slot_mapping.flatten())
+        else:
+            mlu_ops.reshape_paged_cache(key_value,
+                                        None,
+                                        key_cache,
+                                        None,
+                                        updated_slot_mapping.flatten())
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    k = torch.empty_like(q)
+    k[..., :self.qk_nope_head_dim] = k_nope
+    k[..., self.qk_nope_head_dim:] = k_pe
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: mlu attention not pad but qk_head_dim 192 v_head_dim 128.
+    '''
+    q = q.reshape(-1, self.num_local_heads * self.qk_head_dim)
+    k = k.reshape(-1, self.num_local_heads * self.qk_head_dim)
+    v = v.reshape(-1, self.num_local_heads * self.v_head_dim)
+
+    prefill_metadata = layer_metadata.prefill
+    assert prefill_metadata is not None, "chunked prefill can't support prefill_metada is None"
+
+    cu_seq_lens_q = prefill_metadata.query_start_loc
+    max_seq_len_q = prefill_metadata.max_query_len
+    max_seq_len_kv = prefill_metadata.max_query_len
+    cu_seqlens_kv = cu_seq_lens_q
+    if req_seq_len != -1:
+        max_seq_len_kv = req_seq_len
+        assert hasattr(prefill_metadata, "cu_seqlens_kv"), \
+            f"cu_seqlens_kv is not provided for sequence dimension partition"
+        cu_seqlens_kv = prefill_metadata.cu_seqlens_kv
+
+    prefill_kwargs = {
+        "only_prefill": True,
+        "prefill_causal": True,
+        "cu_seq_lens_q": cu_seq_lens_q,
+        "cu_seq_lens_kv": cu_seqlens_kv,
+        "max_seq_len_q": max_seq_len_q,
+        "max_seq_len_kv": max_seq_len_kv,
+        "return_lse": prefill_metadata.context_chunk_seq_tot is not None,
+    }
+    if prefill_k_tensor is None:
+        attn_output = self.attn(q, k, v, kwargs=prefill_kwargs)
+    else:
+        assert req_seq_len <= prefill_k_tensor.shape[0], \
+            "req_seq_len exceeds prefill_k_tensor length"
+        end = req_seq_len
+        start = end - layer_metadata.num_actual_tokens
+        assert start >= 0, "start index cannot be negative"
+        prefill_k_tensor[start:end, ...] = k
+        prefill_v_tensor[start:end, ...] = v
+
+        attn_output = self.attn(
+            q, prefill_k_tensor[:end, ...], prefill_v_tensor[:end, ...], kwargs=prefill_kwargs)
+    # chunked prefill
+    if prefill_metadata.context_chunk_seq_tot is not None:
+        suffix_output, suffix_lse = attn_output
+        suffix_output = suffix_output.reshape(-1, self.num_local_heads, self.v_head_dim)
+        context_output, context_lse = self._compute_prefill_context(q, kv_cache, prefill_metadata)
+        attn_output = torch.empty_like(suffix_output)
+        mlu_ops.merge_attn_states(
+            output=attn_output,
+            prefix_output=context_output,
+            prefix_lse=context_lse,
+            suffix_output=suffix_output,
+            suffix_lse=suffix_lse,
+        )
+    if isinstance(attn_output, (list, tuple)):
+        attn_output = attn_output[0]
+
+    attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    output, _ = self.o_proj(attn_output)
+    return output
+
+
+class MLUDeepseekV2MoE(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        keep_full_weights: bool = False,
+        use_all2all: bool = False,
+    ):
+        tp_group = get_tp_world_group() if enable_data_parallel() else None
+        super().__init__(num_experts=config.n_routed_experts,
+                         top_k=config.num_experts_per_tok,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=config.norm_topk_prob,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True,
+                         expert_group=config.n_group,
+                         topk_group=config.topk_group,
+                         scoring_func=config.scoring_func,
+                         topk_method=config.topk_method,
+                         routed_scaling_factor=config.routed_scaling_factor,
+                         tp_group=tp_group,
+                         use_all2all=use_all2all)
+        self.keep_full_weights = keep_full_weights
+        self.config = config
+        self.n_shared_experts = config.n_shared_experts
+        self.routed_scaling_factor = config.routed_scaling_factor
+        self.tp_size = get_tp_world_world_size()
+        if self.moe_tp_size > config.n_routed_experts:
+            raise ValueError(
+                f"Moe Tensor parallel size {self.moe_tp_size} is greater than "
+                f"the number of experts {config.n_routed_experts}.")
+
+        if config.hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {config.hidden_act}. "
+                             "Only silu is supported for now.")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.n_routed_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+        if config.topk_method == "noaux_tc":
+            self.gate.e_score_correction_bias = nn.Parameter(
+                torch.empty(config.n_routed_experts))
+        else:
+            self.gate.e_score_correction_bias = None
+        if config.n_shared_experts is not None:
+            intermediate_size = (config.moe_intermediate_size *
+                                 config.n_shared_experts)
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace MLP with FeedForward.
+            '''
+            tp_group = None
+            if enable_data_parallel() and not keep_full_weights:
+                tp_group = get_tp_world_group()
+            self.shared_experts = FeedForward(hidden_size=config.hidden_size,
+                                             intermediate_size=intermediate_size,
+                                             hidden_act=config.hidden_act,
+                                             up_proj_name='gate_up_proj',
+                                             is_gated=True,
+                                             down_proj_name='down_proj',
+                                             bias=False,
+                                             quant_config=quant_config,
+                                             reduce_results=False,
+                                             tp_group=tp_group,
+                                             keep_full_weights=keep_full_weights)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        if self.is_fp8_block_wise:
+            self.experts.e_score_correction_bias = self.gate.e_score_correction_bias
+
+    def prepare_for_cnclep(self, cnclep: CnclEP) -> None:
+        # prepare smooth parameter for _all_ experts globally, which would be needed during
+        # input quantization before dispatch.
+        assert self.a13_scale is not None, "a13_scale has not been loaded"
+        self.a13_scale_all_experts = torch.zeros((self.num_total_experts, self.hidden_size),
+                                                  dtype=self.a13_scale.dtype,
+                                                  device=self.a13_scale.device)
+        torch.distributed.all_gather_into_tensor(self.a13_scale_all_experts,
+                                                 self.a13_scale,
+                                                 group=self.moe_group.device_group,
+                                                 async_op=False)
+
+        # prepare buffers for the forward process
+        buffer = cnclep.buffer
+        self.dispatch_send_buffer = buffer.dispatch_send_token_tensor
+        self.dispatch_recv_buffer = buffer.dispatch_recv_token_tensor
+        self.combine_send_buffer = buffer.combine_send_token_tensor
+        self.combine_recv_buffer = buffer.combine_recv_token_tensor
+        self.max_num_tokens_per_rank = cnclep.max_num_tokens_per_rank
+
+        # get sizes in bytes
+        self.quant_size = self.config.hidden_size
+        self.scale_size = get_dtype_size(torch.float32)
+        self.dispatch_token_size = self.quant_size + self.scale_size
+        # [nranks, 2]
+        self.dispatch_recv_layout = torch.empty((self.moe_ep_size, 2), dtype=torch.int32, device="mlu")
+        # [num_total_experts]
+        self.dispatch_recv_token_num = torch.empty((self.num_total_experts), dtype=torch.int32, device="mlu")
+
+        self.max_num_tokens_recv = self.max_num_tokens_per_rank * self.moe_ep_size
+        self.max_num_tokens_per_expert = divide(self.max_num_tokens_recv, self.top_k)
+
+        quant_input_recv_size = self.max_num_tokens_recv * self.quant_size
+        input_scale_recv_size = self.max_num_tokens_recv * self.scale_size
+        self.quant_input_recv = (
+            self.combine_send_buffer[:quant_input_recv_size]
+            .view(self.max_num_tokens_recv, self.quant_size))
+        self.input_scale_recv = (
+            self.combine_send_buffer[quant_input_recv_size : quant_input_recv_size + input_scale_recv_size]
+            .view(self.max_num_tokens_recv, self.scale_size))
+
+    def forward_all2all(
+        self,
+        hidden_states: torch.Tensor,
+        streams: Dict[str, torch.mlu.Stream] | None
+    ) -> torch.Tensor:
+
+        ori_input_shape = hidden_states.shape
+        dtype = hidden_states.dtype
+        self.pack_params()
+        self.pack_params_after_loading()
+        w1=self.w13
+        w2=self.w2
+        bias2=self.b2
+        input_smooth=self.a13_scale_all_experts
+        act_smooth=self.a2_scale
+        w1_scale=self.w13_scale
+        w2_scale=self.w2_scale
+        topk=self.top_k
+        renormalized=self.renormalize
+        act_mode=self.hidden_act
+        quant_input=None
+
+        start_expert_id=self.start_expert_id
+        expert_size = w1.size(0)
+        max_m = hidden_states.shape[0]
+        gating_output, _ = self.gate(hidden_states)
+        gating_output = gating_output.view(-1, gating_output.size(-1))
+        if self.scoring_func == "softmax":
+            reduce_weight, expert_id = mlu_ops.moe_softmax_topk(gating_output, topk, renormalized, self.expert_group,
+                                                                self.topk_group, route_scale=self.routed_scaling_factor)
+        elif self.scoring_func == "sigmoid":
+            reduce_weight, expert_id = mlu_ops.moe_sigmoid_topk(gating_output, topk, renormalized,
+                                                                self.expert_group, self.topk_group,
+                                                                self.routed_scaling_factor,
+                                                                self.gate.e_score_correction_bias)
+        else:
+            raise ValueError(f"Unsupported scoring function: {self.scoring_func}")
+
+        if VLLM_AVG_MOE_EN:
+            # get dp rank
+            dp_rank = get_dp_group().rank_in_group
+            tp_rank = get_tp_group().rank_in_group
+            global_rank = dp_rank * get_tp_group().world_size + tp_rank
+            n_tokens = hidden_states.shape[0]
+            reduce_weight = SparseMoeMlp.reduce_weight[:n_tokens]
+            if self.use_all2all and VLLM_RANDOM_MOE_EN:
+                expert_id = SparseMoeMlp.expert_id[global_rank * n_tokens : (global_rank+1) * n_tokens]
+            elif self.use_all2all:
+                expert_id = SparseMoeMlp.expert_id[dp_rank * n_tokens: dp_rank * n_tokens + n_tokens]
+            else:
+                expert_id = SparseMoeMlp.expert_id[:n_tokens]
+        
+        expand_idx, combine_idx, token_count, cusum_token_count \
+            = mlu_ops.moe_gen_idx(expert_id, self.num_total_experts)
+
+        num_token_expand = hidden_states.shape[0] * self.top_k
+        dispatch_bytes = num_token_expand * self.dispatch_token_size
+
+        dispatch_send_token_tensor = (
+            self.dispatch_send_buffer[:dispatch_bytes]
+            .view(num_token_expand, self.dispatch_token_size)
+        )
+
+        quant_size = self.hidden_size
+        quant_input = dispatch_send_token_tensor[:, : quant_size]
+        input_scale = dispatch_send_token_tensor[:, quant_size :].view(torch.float32)
+        quant_input, input_scale = mlu_ops.moe_quantize(
+            hidden_states, input_smooth, None, token_count, expand_idx, None,
+            output=quant_input,
+            output_scale=input_scale)
+
+        dispatch_send_layout = mlu_ops.moe_all2all_gen_send_layout(token_count, self.moe_ep_size)
+
+        cnclep_dispatch(self.dispatch_token_size, 
+                        num_token_expand, 
+                        dispatch_send_layout, 
+                        token_count, 
+                        self.dispatch_recv_layout, 
+                        self.dispatch_recv_token_num) 
+
+        recv_token_num = self.dispatch_recv_token_num.view(self.moe_ep_size, self.num_experts_per_rank)
+        pad_num = self.max_num_tokens_per_rank
+
+        (
+            gather_by_expert_index,
+            gather_by_rank_index,
+            tokens_per_local_expert,
+            token_sum
+        ) = mlu_ops.moe_all2all_gen_gather_index(recv_token_num, pad_num)
+
+        max_tokens_bytes_recv = self.max_num_tokens_recv * self.dispatch_token_size
+        dispatch_recv_token_tensor = (
+            self.dispatch_recv_buffer[:max_tokens_bytes_recv]
+            .view(self.max_num_tokens_recv, self.dispatch_token_size))
+        
+        mlu_ops.gather_split(dispatch_recv_token_tensor, 
+                             gather_by_expert_index,
+                             token_sum,
+                             self.quant_input_recv,
+                             self.input_scale_recv)
+
+        max_m = self.max_num_tokens_per_expert
+        gemm_out = mlu_ops.smooth_quant_group_gemm(self.quant_input_recv, w1,
+                                                   tokens_per_local_expert,
+                                                   None, None, None, None,
+                                                   self.input_scale_recv.view(torch.float32).flatten(),
+                                                   w1_scale, dtype, max_m)
+
+        # continue reusing self.quant_input_recv and self.input_scale_recv
+        quant_input = self.quant_input_recv[:, :gemm_out.shape[-1] // 2]
+        input_scale_fp32 = self.input_scale_recv.view(torch.float32).flatten()[:gemm_out.shape[0]]
+        quant_input, input_scale = mlu_ops.moe_quantize(gemm_out, act_smooth, None,
+                                                        tokens_per_local_expert,
+                                                        output=quant_input,
+                                                        output_scale=input_scale_fp32,
+                                                        act_mode=act_mode,
+                                                        is_gated=self.is_gated)
+
+        gemm_out = mlu_ops.smooth_quant_group_gemm(quant_input, w2,
+                                                   tokens_per_local_expert,
+                                                   None, None, None, None, input_scale, w2_scale, dtype, max_m)
+
+        combine_send_token_tensor = self.combine_send_buffer.view(self.max_num_tokens_recv, -1).view(hidden_states.dtype)
+        mlu_ops.gather_split(gemm_out,
+                             gather_by_rank_index,
+                             token_sum,
+                             combine_send_token_tensor,
+                             None)
+
+        combine_send_layout = mlu_ops.moe_all2all_gen_send_layout(self.dispatch_recv_token_num, self.moe_ep_size)
+        combine_recv_layout = self.dispatch_recv_layout
+
+        # combine
+        combine_args = dict(
+            token_byte=self.hidden_size * 2,
+            token_num=num_token_expand,
+            send_src_layout=combine_send_layout,
+            send_dst_layout=combine_recv_layout,
+            send_token=None,
+            recv_token=None)
+        parallelize_shared_expert = streams is not None
+        if parallelize_shared_expert:
+            compute_stream = streams['shared']
+            comm_stream = streams['routed']
+            curr_stream = torch.mlu.current_stream()
+            compute_stream.wait_stream(curr_stream)
+            comm_stream.wait_stream(curr_stream)
+
+            with torch.mlu.stream(compute_stream):
+                shared_output = self.shared_experts(hidden_states, use_tp_weight=False)
+
+            with torch.mlu.stream(comm_stream):
+                cnclep_combine(**combine_args)
+
+            curr_stream.wait_stream(compute_stream)
+            curr_stream.wait_stream(comm_stream)
+        else:
+            shared_output = self.shared_experts(hidden_states, use_tp_weight=False)
+            cnclep_combine(**combine_args)
+        
+        numel_recv = num_token_expand * self.hidden_size
+        recv_token = (self.combine_recv_buffer.view(hidden_states.dtype)[:numel_recv]
+                      .view(num_token_expand, self.hidden_size))
+
+        assert shared_output is not None
+        residual_ = shared_output
+        output = mlu_ops.moe_combine_result(recv_token, reduce_weight, combine_idx,
+                     residual_, None, start_expert_id,
+                     expert_size, bias2, output=hidden_states)
+
+        return output.view(ori_input_shape)
+
+    def forward_compute(
+        self,
+        hidden_states: torch.Tensor,
+        only_compute_routed: bool = False,
+        use_tp_weight: bool = False,
+        shared_output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+        if self.n_shared_experts is not None and not only_compute_routed:
+            assert shared_output is None
+            shared_output = self.shared_experts(
+                hidden_states, use_tp_weight=use_tp_weight)
+        # determine if hidden states packs reduce weight and expert id in it
+        # packed dim includes (hidden_states, reduce_weight, expert_id), in that order
+        packed_dim = self.get_precompute_dim_bytes(hidden_states.dtype)
+        hidden_dim_int8 = hidden_states.view(torch.int8).shape[1]
+        is_precompute_weight_expert_id: bool = (hidden_dim_int8 == packed_dim)
+        if not is_precompute_weight_expert_id:
+            # router_logits: (num_tokens, n_experts)
+            router_logits, _ = self.gate(hidden_states)
+        else:
+            # router logits have been pre-computed and then used to calculate
+            # reduce_weight and expert_id
+            router_logits = None
+            # hidden_states are now packed with reduce weights and expert id,
+            # hidden_dim needs to be manually set to get correct output shape
+            hidden_dim = self.hidden_size
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace experts() with forward_experts, which defined by SparseMoeMlp.
+        '''
+        final_hidden_states = self.forward_experts(
+            hidden_states, router_logits, shared_output=shared_output)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+    def forward_communication(
+        self,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        # Note: Here we only handle reduce operation in non-dp scenarioes.
+        # In dp scenarioes, reduce operation should be added manually after
+        # forward operation.
+        if not enable_data_parallel():
+            hidden_states = self.reduce_results(hidden_states)
+
+        return hidden_states
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        only_compute_routed: bool = False,
+        use_tp_weight: bool = False,
+        shared_output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        hidden_states = self.forward_compute(
+            hidden_states,
+            only_compute_routed=only_compute_routed,
+            use_tp_weight=use_tp_weight,
+            shared_output=shared_output,
+        )
+
+        hidden_states = self.forward_communication(hidden_states)
+        return hidden_states
+
+
+class MLUDeepseekV2MLAAttention(DeepseekV2MLAAttention):
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        hidden_size: int,
+        num_heads: int,
+        qk_nope_head_dim: int,
+        qk_rope_head_dim: int,
+        v_head_dim: int,
+        q_lora_rank: int,
+        kv_lora_rank: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[Dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        # Skip DeepseekV2MLAAttention `__init__` method.
+        super(DeepseekV2MLAAttention, self).__init__()
+        self.hidden_size = hidden_size
+        self.qk_nope_head_dim = qk_nope_head_dim
+        self.qk_rope_head_dim = qk_rope_head_dim
+        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim
+        self.v_head_dim = v_head_dim
+        self.q_lora_rank = q_lora_rank
+        self.kv_lora_rank = kv_lora_rank
+        self.num_heads = num_heads
+        self.attn_data_parallel_size = get_data_parallel_group_world_size()
+        self.attn_tensor_parallel_size = get_tensor_model_parallel_world_size()
+        tp_size = self.attn_tensor_parallel_size
+        assert num_heads % tp_size == 0
+        self.num_local_heads = num_heads // tp_size
+        self.scaling = self.qk_head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief:
+        1) skip q_a_proj, kv_a_proj_with_mqa, kv_b_proj weight quant,
+        split kv_b_proj weight if not is_fp8_block_wise
+        2) do all reduce outside when use data parallel
+        3) add self.quant_config
+        '''
+        self.is_fp8_block_wise = isinstance(quant_config, Fp8Config) and quant_config.weight_block_size is not None
+        self.is_fp8_smootquant = isinstance(quant_config, SmoothQuantConfig) and quant_config.is_fp8
+        self.use_quant_fusion_rmsnorm = (quant_config is not None) and is_per_token_smoothquant(quant_config) and (
+            not self.is_fp8_smootquant)
+        assert envs.VLLM_USE_V1, "DeepseekV2MLAAttention only support use_v1=True"
+        self.quant_config = quant_config
+
+        if self.q_lora_rank is not None:
+            self.q_a_proj = ReplicatedLinear(
+                self.hidden_size,
+                self.q_lora_rank,
+                bias=False,
+                quant_config=quant_config if self.is_fp8_block_wise else None,
+                prefix=f"{prefix}.q_a_proj",
+            )
+            self.q_a_layernorm = RMSNorm(
+                self.q_lora_rank,
+                eps=config.rms_norm_eps,
+            )
+            self.q_b_proj = ColumnParallelLinear(
+                q_lora_rank,
+                self.num_heads * self.qk_head_dim,
+                bias=False,
+                quant_config=quant_config,
+                prefix=f"{prefix}.q_b_proj",
+            )
+            if self.use_quant_fusion_rmsnorm:
+                self.q_a_layernorm = QuantFusionRMSNorm(
+                    self.q_lora_rank,
+                    config.rms_norm_eps,
+                    self.q_b_proj,
+                )
+        else:
+            self.q_proj = ColumnParallelLinear(
+                self.hidden_size,
+                self.num_heads * self.qk_head_dim,
+                bias=False,
+                quant_config=quant_config,
+                prefix=f"{prefix}.q_proj",
+            )
+
+        self.kv_a_proj_with_mqa = ReplicatedLinear(
+            self.hidden_size,
+            self.kv_lora_rank + self.qk_rope_head_dim,
+            bias=False,
+            quant_config=quant_config if self.is_fp8_block_wise else None,
+            prefix=f"{prefix}.kv_a_proj_with_mqa",
+        )
+        self.kv_a_layernorm = RMSNorm(
+            self.kv_lora_rank,
+            eps=config.rms_norm_eps,
+        )
+        self.kv_b_proj = ColumnParallelLinear(
+            self.kv_lora_rank,
+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),
+            bias=False,
+            quant_config=quant_config if self.is_fp8_block_wise else None,
+            prefix=f"{prefix}.kv_b_proj")
+        kv_b_proj_weight = self.kv_b_proj.weight
+        w_kc, w_vc = kv_b_proj_weight.unflatten(
+            0, (-1, self.qk_nope_head_dim + self.v_head_dim)
+            ).split([self.qk_nope_head_dim, self.v_head_dim], dim=1)
+        self.w_kc = w_kc
+        self.w_vc = w_vc
+        # O projection.
+        self.o_proj = RowParallelLinear(
+            self.num_heads * self.v_head_dim,
+            self.hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            reduce_results=False,
+            prefix=f"{prefix}.o_proj",
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        if rope_scaling:
+            rope_scaling["rope_type"] = 'deepseek_yarn'
+        self.use_normal_rope = not rope_scaling
+        self.rotary_emb = get_rope(
+            qk_rope_head_dim,
+            rotary_dim=qk_rope_head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+            is_neox_style=False,
+        )
+
+        if rope_scaling:
+            mscale_all_dim = rope_scaling.get("mscale_all_dim", False)
+            scaling_factor = rope_scaling["factor"]
+            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))
+            self.scaling = self.scaling * mscale * mscale
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add fused mla_q/kv and fused_qkv_a.
+        @brief: mlu attention support head_size 192.
+        '''
+
+        has_fp8 = self.is_fp8_block_wise or self.is_fp8_smootquant or cache_config.cache_dtype.startswith('fp8')
+        self.use_fused_mla_qkv = not has_fp8
+        self.use_fused_qkv_a = False and self.use_fused_mla_qkv and self.q_lora_rank is not None and quant_config is not None
+
+        self.attn = Attention(
+            self.num_local_heads,
+            self.qk_nope_head_dim + self.qk_rope_head_dim,
+            self.scaling,
+            num_kv_heads=self.num_local_heads,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attn",
+            use_mla=True,
+            v_head_dim=self.v_head_dim,
+        )
+        self.attn_decoder = Attention(
+            self.num_local_heads,
+            self.kv_lora_rank + self.qk_rope_head_dim,
+            self.scaling,
+            num_kv_heads=1,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mla_attn",
+            use_mla=True,
+            v_head_dim=self.kv_lora_rank,
+            use_fused_mla_qkv=self.use_fused_mla_qkv,
+        )
+        self.forward_prefill = types.MethodType(forward_prefill, self)
+        self.forward_decoder = types.MethodType(forward_decoder, self)
+        self._compute_prefill_context = types.MethodType(_compute_prefill_context, self)
+        if self.use_fused_mla_qkv:
+            self.forward_decoder_fused = types.MethodType(forward_decoder_fused, self)
+            self.weight_c = self.w_kc.transpose(-2, -1).contiguous()
+            if self.use_fused_qkv_a:
+                # self.qkv_a_proj is the fusion of self.q_a_proj and self.kv_a_proj_with_mqa
+                self.qkv_a_proj = ReplicatedLinear(self.hidden_size,
+                                                self.q_lora_rank + self.kv_lora_rank + self.qk_rope_head_dim,
+                                                bias=False,
+                                                quant_config=None,
+                                                prefix=f"{prefix}.qkv_a_proj")
+        self.prefix = prefix
+        self.pack_params_done = False
+        self.pack_params_after_loading_done = False
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward_mla_attn(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        attn_metadata: AttentionMetadata,
+        kv_cache: List[torch.Tensor],
+        prefill_k_tensor: torch.Tensor = None,
+        prefill_v_tensor: torch.Tensor = None,
+        req_seq_len: int = -1,
+        enable_prefill_mcc: bool = False,
+    ) -> torch.Tensor:
+        forward_decoder_func = (self.forward_decoder_fused
+                                if self.use_fused_mla_qkv else self.forward_decoder)
+        num_decode_tokens = attn_metadata.num_decode_tokens
+        if attn_metadata.prefill:
+            attn_metadata.prefill.compute_dtype = torch.float16
+            prefill_positions = positions[num_decode_tokens:, ...]
+            prefill_hidden_states = hidden_states[num_decode_tokens:, ...]
+            prefill_output = self.forward_prefill(
+                prefill_positions,
+                prefill_hidden_states,
+                kv_cache,
+                attn_metadata,
+                prefill_k_tensor,
+                prefill_v_tensor,
+                req_seq_len,
+                enable_prefill_mcc=enable_prefill_mcc,
+            )
+        decode_output = None
+        if attn_metadata.decode:
+            attn_metadata.decode.compute_dtype = torch.float16
+            decode_positions = positions[:num_decode_tokens, ...]
+            decode_hidden_states = hidden_states[:num_decode_tokens, ...]
+            decode_output = forward_decoder_func(
+                decode_positions,
+                decode_hidden_states,
+                kv_cache,
+                attn_metadata,
+            )
+
+        if attn_metadata.prefill is not None and attn_metadata.decode is not None:
+            return torch.cat([decode_output, prefill_output], dim=0)
+        elif attn_metadata.prefill is not None:
+            return prefill_output
+
+        return decode_output
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        prefill_k_tensor: torch.Tensor = None,
+        prefill_v_tensor: torch.Tensor = None,
+        req_seq_len: int = -1,
+        enable_prefill_mcc: bool = False,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use normal computation for prefill and use weight absorption for extend/decode.
+                pack_params() is called for dummy model
+        '''
+        forward_context: ForwardContext = get_forward_context()
+        attn_metadata = forward_context.attn_metadata
+        if attn_metadata is None:
+            return torch.empty_like(hidden_states)
+
+        # self.attn and self.attn_decoder always have the same attn_metadata
+        # and share the same kv cache for each layer
+        if isinstance(attn_metadata, dict):
+            attn_metadata = attn_metadata[self.attn.layer_name]
+        kv_cache = self.attn.kv_cache[forward_context.virtual_engine]
+        self.pack_params()
+        self.pack_params_after_loading()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return self.forward_mla_attn(
+            positions,
+            hidden_states,
+            attn_metadata,
+            kv_cache,
+            prefill_k_tensor,
+            prefill_v_tensor,
+            req_seq_len,
+            enable_prefill_mcc=enable_prefill_mcc,
+        )
+
+    def pack_params(self):
+        if self.pack_params_done:
+            return
+        if self.use_fused_qkv_a:
+            self.q_a_proj.weight.data = self.qkv_a_proj.weight.data[:self.q_lora_rank, ...]
+            self.kv_a_proj_with_mqa.weight.data = self.qkv_a_proj.weight.data[self.q_lora_rank:, ...]
+
+        self.pack_params_done = True
+
+    def pack_params_after_loading(self):
+        if self.pack_params_after_loading_done:
+            return
+        if self.is_fp8_block_wise:
+            kv_b_proj_dequant_weight = scaled_dequantize(
+                self.kv_b_proj.weight,
+                self.kv_b_proj.weight_scale_inv,
+                self.kv_b_proj.quant_method.quant_config.weight_block_size,
+                self.kv_b_proj.params_dtype,
+            )
+            w_kc, w_vc = kv_b_proj_dequant_weight.unflatten(
+                0,
+                (-1, self.qk_nope_head_dim + self.v_head_dim)
+            ).split([self.qk_nope_head_dim, self.v_head_dim], dim=1)
+            self.w_kc = w_kc
+            self.w_vc = w_vc
+
+        if self.use_fused_mla_qkv and self.q_lora_rank is not None:
+            self.weight_c = self.w_kc.transpose(-2, -1).contiguous()
+
+        self.pack_params_after_loading_done = True
+
+
+class MLUDeepseekV2DecoderLayerBase(DeepseekV2DecoderLayer):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        vllm_config: VllmConfig,
+        model_config: ModelConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        # Skip DeepseekV2DecoderLayer `__init__` method.
+        super(DeepseekV2DecoderLayer, self).__init__()
+        self.hidden_size = config.hidden_size
+        '''
+        =============================
+        Modify by vllm_mlu attn_dp
+        =============================
+        @brief: add self.vllm_config, self.torch_dtype and self.quant_config
+        '''
+        self.vllm_config = vllm_config
+        self.torch_dtype = model_config.dtype
+        self.quant_config = quant_config
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                        8192)
+        # DecoderLayers are created with `make_layers` which passes the prefix
+        # with the layer's index.
+        self.layer_idx = int(prefix.split(sep='.')[-1])
+        if model_config.use_mla:
+            attn_cls = MLUDeepseekV2MLAAttention
+        else:
+            attn_cls = DeepseekV2Attention
+        self.self_attn = attn_cls(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            qk_nope_head_dim=config.qk_nope_head_dim,
+            qk_rope_head_dim=config.qk_rope_head_dim,
+            v_head_dim=config.v_head_dim,
+            q_lora_rank=config.q_lora_rank
+            if hasattr(config, "q_lora_rank") else None,
+            kv_lora_rank=config.kv_lora_rank,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief:
+        1) add parallelize_shared_expert for moe.
+        2) replace MLP with FeedForward and do reduce outside for dp.
+        '''
+        self.use_all2all = vllm_config.mlu_config.decode_dispatch_combine_use_all2all
+        if (config.n_routed_experts is not None
+                and self.layer_idx >= config.first_k_dense_replace
+                and self.layer_idx % config.moe_layer_freq == 0):
+            parallelize_shared_expert = getattr(
+                model_config, "parallelize_shared_expert", False)
+            self.mlp = MLUDeepseekV2MoE(
+                config=config,
+                quant_config=quant_config,
+                prefix=f"{prefix}.mlp",
+                keep_full_weights=parallelize_shared_expert or self.use_all2all,
+                use_all2all=self.use_all2all,
+            )
+        else:
+            enable_dp = enable_data_parallel()
+            reduce_results = not enable_dp
+            tp_group = get_tp_world_group() if enable_dp else None
+            if get_dense_mlp_tp_world_size() != get_tp_world_world_size():
+                tp_group = get_dense_mlp_tp_group()
+                reduce_results = False
+            self.mlp = FeedForward(hidden_size=config.hidden_size,
+                                intermediate_size=config.intermediate_size,
+                                hidden_act=config.hidden_act,
+                                up_proj_name='gate_up_proj',
+                                is_gated=True,
+                                down_proj_name='down_proj',
+                                bias=False,
+                                reduce_results=False,
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.mlp",
+                                tp_group=tp_group)
+            self.mlp.forward_compute = self.mlp.forward
+            self.mlp.forward_communication = types.MethodType(
+                lambda self, x: tensor_model_parallel_all_reduce(
+                    x, tp_group=tp_group),
+                self.mlp
+            )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                    eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.routed_scaling_factor = config.routed_scaling_factor
+
+    def forward_without_dp(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # Add tensor model group all reduce here.
+        hidden_states = tensor_model_parallel_all_reduce(hidden_states)
+
+        if hidden_states.dtype == torch.float16:
+            # Fix FP16 overflow
+            # We scale both hidden_states and residual before
+            # rmsnorm, and rmsnorm result would not affect by scale.
+            hidden_states *= 1. / self.routed_scaling_factor
+            if self.layer_idx == 0:
+                # The residual is shared by all layers, we only scale it on
+                # first layer.
+                residual *= 1. / self.routed_scaling_factor
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: split mlp.forward into forward_compute and forward_communication.
+        '''
+        hidden_states = self.mlp.forward_compute(hidden_states)
+        hidden_states = self.mlp.forward_communication(hidden_states)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        return hidden_states, residual
+
+    def forward_dp_opt(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+        dp_params: DataParallelRuntimeParams,
+        streams: Optional[Dict],
+        is_first_layer: bool,
+        is_last_layer: bool,
+        next_input_layernorm: Any,
+    ):
+        common_metadata = get_common_metadata()
+        is_decode_only = common_metadata is not None and common_metadata.is_decode_only
+        use_all2all_in_decode = self.use_all2all and is_decode_only
+
+        if residual is None:
+            residual = hidden_states
+
+        # We move the input_layernorm of i+1 layer to the end of i layer.
+        # But for the first layer, we need to do input_layernorm first.
+        if is_first_layer:
+            hidden_states = self.input_layernorm(hidden_states)
+
+        # Self Attention
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # add residual here
+        if is_first_layer and get_tensor_model_parallel_rank() == 0:
+            hidden_states = hidden_states + residual
+
+        hidden_states = tensor_model_parallel_reduce_scatter(
+            hidden_states, dim=0)
+
+        # do norm before all gather
+        parallelize_shared_expert = streams is not None
+        if is_first_layer:
+            residual = hidden_states
+            hidden_states = self.post_attention_layernorm(hidden_states)
+        else:
+            do_precompute: bool = (
+                isinstance(self.mlp, MLUDeepseekV2MoE) and parallelize_shared_expert
+                # all2all does not need precompute
+                and not use_all2all_in_decode
+            )
+            if do_precompute:
+                # precompute gate + sigmoid_topk/softmax_topk and pack their results
+                # as
+                # [hidden_states | reduce_weights | expert_id ]
+                shape = (hidden_states.shape[0], self.mlp.get_precompute_dim_bytes(hidden_states.dtype))
+                packed = torch.empty(shape, dtype=torch.int8, device=hidden_states.device).view(hidden_states.dtype)
+                hidden_states_strided = packed[:, :hidden_states.shape[1]]
+                hidden_states, residual = self.post_attention_layernorm(hidden_states, residual, out=hidden_states_strided)
+                packed = self.mlp.precompute_weight_expert_id(packed)
+            else:
+                # Here we fuse residual into post layernorm.
+                hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
+
+        is_prefill_only: bool = dp_params.dp_is_prefill[get_data_parallel_group_rank()]
+        if use_all2all_in_decode and isinstance(self.mlp, MLUDeepseekV2MoE):
+            assert isinstance(self.quant_config, SmoothQuantConfig), (
+                "all2all only supports smoothquant for now")
+            hidden_states = self.mlp.forward_all2all(hidden_states, streams)
+        elif parallelize_shared_expert and isinstance(self.mlp, MLUDeepseekV2MoE):
+            shared_expert_stream = streams['shared']
+            routed_expert_stream = streams['routed']
+            moe = self.mlp
+            curr_stream = torch.mlu.current_stream()
+            shared_expert_stream.wait_stream(curr_stream)
+            routed_expert_stream.wait_stream(curr_stream)
+
+            with torch.mlu.stream(shared_expert_stream):
+                if moe.n_shared_experts is not None:
+                    shared_output = moe.shared_experts(hidden_states)
+            with torch.mlu.stream(routed_expert_stream):
+                packed = tensor_model_parallel_all_gather_dp(
+                    group_num_tokens=dp_params.attn_token_split_list_reduce_scatter,
+                    rank=get_tp_world_rank(),
+                    hidden_states=packed,
+                    group=get_tp_world_group(),
+                )
+            curr_stream.wait_stream(shared_expert_stream)
+            curr_stream.wait_stream(routed_expert_stream)
+
+            if is_prefill_only and VLLM_MOE_PREFILL_CHUNK_SIZE > 0:
+                kwargs = {'only_compute_routed': True}
+                hidden_states = compute_in_loop(self.mlp.forward_compute, packed,
+                                                VLLM_MOE_PREFILL_CHUNK_SIZE,
+                                                feature_size=self.hidden_size,
+                                                **kwargs)
+            else:
+                hidden_states = self.mlp.forward_compute(
+                    packed, only_compute_routed=True)
+
+            hidden_states = tensor_model_parallel_reduce_scatter(
+                hidden_states, dim=0, tp_group=get_tp_world_group())
+            if shared_output is not None:
+                hidden_states = hidden_states + shared_output
+        else:
+            if (
+                isinstance(self.mlp, FeedForward)
+                and get_dense_mlp_tp_world_size() < get_tp_world_world_size()
+                and dp_params.dense_attn_token_split_list is not None
+            ):
+                tp_group = get_dense_mlp_tp_group()
+                attn_token_split_list = dp_params.dense_attn_token_split_list
+            else:
+                tp_group = get_tp_world_group()
+                attn_token_split_list = dp_params.attn_token_split_list_reduce_scatter
+
+            hidden_states = tensor_model_parallel_all_gather_dp(
+                group_num_tokens=attn_token_split_list,
+                rank=get_parallel_rank_with_group(tp_group),
+                hidden_states=hidden_states,
+                group=tp_group,
+            )
+            # Fully Connected
+            if is_prefill_only and VLLM_MOE_PREFILL_CHUNK_SIZE > 0:
+                hidden_states = compute_in_loop(self.mlp.forward_compute, hidden_states,
+                                                VLLM_MOE_PREFILL_CHUNK_SIZE)
+            else:
+                if isinstance(self.mlp, MLUDeepseekV2MoE):
+                    hidden_states = self.mlp.forward_compute(hidden_states,
+                                                             use_tp_weight=self.mlp.keep_full_weights)
+                else:
+                    hidden_states = self.mlp.forward_compute(hidden_states)
+
+            hidden_states = tensor_model_parallel_reduce_scatter(
+                hidden_states, dim=0, tp_group=tp_group)
+
+        if is_last_layer:
+            hidden_states = hidden_states + residual
+            residual = None
+        else:
+            # To reduce layernorm computation, we move the layernorm of i+1 layer to
+            # the end of i layer. Besides, we fuse residual addition into layernorm.
+            assert next_input_layernorm is not None
+            hidden_states, residual = next_input_layernorm(hidden_states, residual)
+
+        hidden_states = tensor_model_parallel_all_gather_dp(
+            group_num_tokens=dp_params.moe_token_split_list_reduce_scatter,
+            rank=get_tensor_model_parallel_rank(),
+            hidden_states=hidden_states,
+            group=get_tp_group(),
+        )
+
+        return hidden_states, residual
+
+    def forward_dp_common(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+        dp_params: DataParallelRuntimeParams,
+        shared_expert_use_tp_weight: bool,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        if residual is None:
+            residual = hidden_states
+
+        # Self Attention
+        hidden_states = self.input_layernorm(hidden_states)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # add residual here
+        if get_tensor_model_parallel_rank() == 0:
+            hidden_states = hidden_states + residual
+        if (
+            isinstance(self.mlp, FeedForward)
+            and dp_params.dense_attn_token_split_list is not None
+        ):
+            tp_group = get_dense_mlp_tp_group()
+            cur_rank = get_dense_mlp_tp_rank()
+            token_num_offset = sum(dp_params.dense_attn_token_split_list[:cur_rank])
+            assert dp_params.dense_attn_token_split_list[cur_rank] == dp_params.token_num, \
+                f"[rank: {cur_rank}] token_num: {dp_params.token_num}, " \
+                f"dense_attn_token_split_list: {dp_params.dense_attn_token_split_list}, " \
+                f"token_num_offset: {token_num_offset}"
+        else:
+            tp_group = None
+            token_num_offset = dp_params.token_num_offset
+        hidden_states = process_post_attention_communication(
+            hidden_states, dp_params, self.hidden_size, self.torch_dtype, positions.device, tp_group
+        )
+        residual = hidden_states[token_num_offset : token_num_offset + dp_params.token_num]
+
+        # Fully Connected
+        hidden_states = self.post_attention_layernorm(hidden_states)
+        if isinstance(self.mlp, MLUDeepseekV2MoE):
+            hidden_states = self.mlp(hidden_states, use_tp_weight=self.mlp.keep_full_weights)
+        else:
+            hidden_states = self.mlp(hidden_states)
+        tp_group = tp_group if tp_group is not None else get_tp_world_group()
+        hidden_states = tensor_model_parallel_all_reduce(
+            hidden_states, tp_group=tp_group)
+
+        # add residual here
+        hidden_states = hidden_states[token_num_offset:
+                                    token_num_offset+dp_params.token_num]
+        hidden_states = hidden_states + residual
+        residual = hidden_states
+
+        return hidden_states, residual
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+        dp_params: DataParallelRuntimeParams = None,
+        streams: Optional[Dict] = None,
+        is_first_layer: bool = None,
+        is_last_layer: bool = None,
+        next_input_layernorm: Any = None,
+    ) -> torch.Tensor:
+        """run layers with dp."""
+        if not enable_data_parallel():
+            return self.forward_without_dp(
+                positions, hidden_states, residual)
+        if dp_params.layer_use_reduce_scatter:
+            hidden_states, residual = self.forward_dp_opt(
+                positions,
+                hidden_states,
+                residual,
+                dp_params,
+                streams,
+                is_first_layer,
+                is_last_layer,
+                next_input_layernorm)
+        else:
+            shared_expert_use_tp_weight = True if streams is not None else False
+            hidden_states, residual = self.forward_dp_common(
+                positions,
+                hidden_states,
+                residual,
+                dp_params,
+                shared_expert_use_tp_weight)
+
+        return hidden_states, residual
+
+
+class MLUDeepseekV2DecoderLayerWithMCC(MLUDeepseekV2DecoderLayerBase):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        vllm_config: VllmConfig,
+        model_config: ModelConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__(
+            config=config,
+            prefix=prefix,
+            vllm_config=vllm_config,
+            model_config=model_config,
+            cache_config=cache_config,
+            quant_config=quant_config,
+        )
+        moe_group_info = MoeGroupInfo()
+        self.moe_tp_rank = moe_group_info.moe_tp_rank
+        self.moe_ep_size = moe_group_info.moe_ep_size
+        self.moe_ep_rank = moe_group_info.moe_ep_rank
+
+    def dispatch_stage(
+        self,
+        kwargs: Dict[str, Any],
+    ) -> Any:
+        assert "dp_params" in kwargs, \
+            f"dp_params is required in kwargs, but got {kwargs.keys()}"
+        dp_params = kwargs["dp_params"]
+        if dp_params is None:
+            return self.forward_without_dp_per_stage(kwargs)
+        elif dp_params.layer_use_reduce_scatter:
+            return self.forward_dp_opt_per_stage(kwargs)
+        else:
+            return self.forward_dp_common_per_stage(kwargs)
+
+    def forward(
+        self,
+        positions_list: List[torch.Tensor],
+        hidden_states_list: List[torch.Tensor],
+        attn_metadata_list: List[AttentionMetadata],
+        residual_list: List[Optional[torch.Tensor]],
+        dp_params_list: List[Optional[DataParallelRuntimeParams]],
+        event_list: List[torch.mlu.Event],
+        is_first_layer: bool,
+        is_last_layer: bool,
+        streams: Optional[Dict] = None,
+        compute_stream: Optional[torch.mlu.Stream] = None,
+        communication_stream: Optional[torch.mlu.Stream] = None,
+        bs_parts: int = 1,
+        seq_parts: int = 1,
+        prefill_k_tensor: torch.Tensor = None,
+        prefill_v_tensor: torch.Tensor = None,
+        next_input_layernorm: Any = None,
+    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.mlu.Event], List[torch.Tensor]]:
+        num_parts = bs_parts * seq_parts
+        assert num_parts == len(event_list), \
+            f"num_parts {num_parts} should be equal to the number of events {len(event_list)}"
+        assert num_parts > 1, \
+            f"num_parts {num_parts} should be greater than 1"
+
+        reserved_tensors = []
+        STAGE_NUM = 4
+        # The process includes 4 stages:
+        # Stage 0: input layer norm and self attention.
+        # Stage 1: communication after self attention.
+        # Stage 2: post layernorm and moe/ffn.
+        # Stage 3: communication after moe/ffn.
+        req_seq_len = 0
+        for stage_id in range(STAGE_NUM):
+            current_stream = compute_stream if stage_id % 2 == 1 else communication_stream
+            with torch.mlu.stream(current_stream):
+                for i in range(num_parts):
+                    if stage_id == 0 and prefill_k_tensor is not None:
+                        assert hidden_states_list[i] is not None, \
+                            f"hidden_states_list[{i}] should not be None."
+                        req_seq_len += hidden_states_list[i].shape[0]
+                    event_list[i].wait(current_stream)
+                    kwargs = {
+                        "stage_id": stage_id,
+                        "positions": positions_list[i],
+                        "hidden_states": hidden_states_list[i],
+                        "residual": residual_list[i],
+                        "dp_params": dp_params_list[i],
+                        "attn_metadata": attn_metadata_list[i],
+                        "num_parts": num_parts,
+                        "prefill_k_tensor": prefill_k_tensor,
+                        "prefill_v_tensor": prefill_v_tensor,
+                        # Note: only set req_seq_len when seq_parts > 1
+                        "req_seq_len": req_seq_len if seq_parts > 1 else -1,
+                    }
+
+                    hidden_states_list[i], residual_list[i], reserved_tensors_ = \
+                        self.dispatch_stage(kwargs)
+                    reserved_tensors.extend(reserved_tensors_)
+                    event_list[i].record(current_stream)
+
+        return hidden_states_list, residual_list, event_list, reserved_tensors
+
+    def forward_dp_opt_per_stage(
+        self,
+        kwargs: Dict[str, Any],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        stage_id = kwargs["stage_id"]
+        residual = kwargs["residual"]
+        hidden_states = kwargs["hidden_states"]
+        positions = kwargs["positions"]
+        attn_metadata = kwargs["attn_metadata"]
+        dp_params = kwargs["dp_params"]
+        reserved_tensors = []
+
+        if stage_id == 0:
+            if residual is None:
+                residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+            kwargs_ = {
+                "positions": positions,
+                "hidden_states": hidden_states,
+                "prefill_k_tensor": kwargs["prefill_k_tensor"],
+                "prefill_v_tensor": kwargs["prefill_v_tensor"],
+                "req_seq_len": kwargs["req_seq_len"],
+                "enable_prefill_mcc": True,
+            }
+            hidden_states = partition_utils.execute_with_updated_forward_context(
+                self.vllm_config,
+                attn_metadata,
+                self.self_attn.forward,
+                kwargs_,
+            )
+
+            # Add residual here.
+            if get_tensor_model_parallel_rank() == 0:
+                hidden_states = hidden_states + residual
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 1:
+            reserved_tensors.append(hidden_states)
+            hidden_states = tensor_model_parallel_reduce_scatter(
+                hidden_states,
+                dim=0,
+                tp_group=get_tp_group(),
+            )
+            reserved_tensors.append(hidden_states)
+
+            hidden_states = tensor_model_parallel_all_gather_op_v2(
+                hidden_states,
+                dp_params.attn_token_split_list_reduce_scatter,
+                get_tp_world_group(),
+                self.hidden_size,
+                self.torch_dtype,
+                positions.device,
+            )
+            # For scenarioes like MoE TP parallel, residual should not be partition.
+            residual = hidden_states
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 2:
+            hidden_states = self.post_attention_layernorm(hidden_states)
+            if isinstance(self.mlp, MLUDeepseekV2MoE):
+                hidden_states = self.mlp.forward_compute(
+                    hidden_states,
+                    # dp_params,
+                    only_compute_routed=False,
+                    use_tp_weight=True,
+                )
+            else:
+                hidden_states = self.mlp.forward_compute(hidden_states)
+
+            # Add residual.
+            # Note: Here is an optimization. We use node with
+            # moe_tp_rank = 0 to execute addition, and moe_ep_rank
+            # to slice.
+            token_nums = hidden_states.shape[0]
+            if self.moe_tp_rank == 0:
+                start = self.moe_ep_rank * (token_nums // self.moe_ep_size)
+                end   = (self.moe_ep_rank + 1) * (token_nums // self.moe_ep_size)
+                hidden_states[start:end] += residual[start:end]
+
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 3:
+            reserved_tensors.append(hidden_states)
+            hidden_states = tensor_model_parallel_reduce_scatter(
+                hidden_states,
+                dim=0,
+                tp_group=get_tp_world_group(),
+            )
+            reserved_tensors.append(hidden_states)
+
+            hidden_states = tensor_model_parallel_all_gather_op_v2(
+                hidden_states,
+                dp_params.moe_token_split_list_reduce_scatter,
+                get_tp_group(),
+                self.hidden_size,
+                self.torch_dtype,
+                positions.device,
+            )
+
+            residual = hidden_states
+            return hidden_states, residual, reserved_tensors
+
+    def forward_dp_common_per_stage(
+        self,
+        kwargs: Dict[str, Any],
+    ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
+        stage_id = kwargs["stage_id"]
+        residual = kwargs["residual"]
+        hidden_states = kwargs["hidden_states"]
+        positions = kwargs["positions"]
+        attn_metadata = kwargs["attn_metadata"]
+        dp_params = kwargs["dp_params"]
+        reserved_tensors = []
+
+        if stage_id == 0:
+            if residual is None:
+                residual = hidden_states
+            if dp_params.token_num != 0:
+                hidden_states = self.input_layernorm(hidden_states)
+                kwargs_ = {
+                    "positions": positions,
+                    "hidden_states": hidden_states,
+                    # "attn_metadata": attn_metadata,
+                    "prefill_k_tensor": kwargs["prefill_k_tensor"],
+                    "prefill_v_tensor": kwargs["prefill_v_tensor"],
+                    "req_seq_len": kwargs["req_seq_len"],
+                    "enable_prefill_mcc": True,
+                }
+                hidden_states = partition_utils.execute_with_updated_forward_context(
+                    self.vllm_config,
+                    attn_metadata,
+                    self.self_attn.forward,
+                    kwargs_,
+                )
+
+                # Add residual here.
+                if get_tensor_model_parallel_rank() == 0:
+                    hidden_states = hidden_states + residual
+
+            if dp_params.prefill_pad_to_token_num != -1:
+                # pad hidden_states to use reduce_scatter and global all gather
+                if dp_params.token_num == 0:
+                    hidden_states = torch.empty(
+                        (dp_params.prefill_pad_to_token_num, self.hidden_size),
+                        dtype=self.torch_dtype,
+                        device=positions.device,
+                    )
+                else:
+                    pad_num = dp_params.prefill_pad_to_token_num - dp_params.token_num
+                    if pad_num != 0:
+                        hidden_states = F.pad(hidden_states, (0, 0, 0, pad_num))
+
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 1:
+            if dp_params.prefill_pad_to_token_num != -1:
+                group_coordinator = get_tp_group()
+                reserved_tensors.append(hidden_states)
+                hidden_states = tensor_model_parallel_reduce_scatter(
+                    hidden_states,
+                    dim=0,
+                    tp_group=get_tp_group(),
+                )
+                reserved_tensors.append(hidden_states)
+
+                hidden_states = tensor_model_parallel_all_gather_op_v2(
+                    hidden_states,
+                    dp_params.attn_token_split_list_reduce_scatter,
+                    get_tp_world_group(),
+                    self.hidden_size,
+                    self.torch_dtype,
+                    positions.device,
+                )
+                residual = hidden_states
+                return hidden_states, residual, reserved_tensors
+            else:
+                reserved_tensors.append(hidden_states)
+                if dp_params.token_num != 0:
+                    # All reduce
+                    process_group = get_tp_group()
+                    torch.distributed.all_reduce(hidden_states, group=process_group.device_group, async_op=False)
+                reserved_tensors.append(hidden_states)
+
+                token_split_list = dp_params.token_split_list
+                group_coordinator = get_dp_group()
+
+                hidden_states = tensor_model_parallel_all_gather_op_v2(
+                    hidden_states,
+                    token_split_list,
+                    group_coordinator,
+                    self.hidden_size,
+                    self.torch_dtype,
+                    positions.device,
+                )
+                residual = hidden_states
+                return hidden_states, residual, reserved_tensors
+
+        if stage_id == 2:
+            # Get origin hidden_states for moe compute.
+            if dp_params.prefill_pad_to_token_num != -1:
+                dp_group_tensors = []
+                offset = 0
+                for token_num in dp_params.token_split_list:
+                    dp_group_tensors.append(hidden_states[offset:offset+token_num])
+                    offset += dp_params.prefill_pad_to_token_num
+                if len(dp_group_tensors) == 1:
+                    hidden_states = dp_group_tensors[0]
+                else:
+                    hidden_states = torch.cat(dp_group_tensors)
+                # Update residual.
+                residual = hidden_states
+
+            if sum(dp_params.token_split_list) > 0:
+                # Fully Connected
+                hidden_states = self.post_attention_layernorm(
+                    hidden_states)
+                if isinstance(self.mlp, MLUDeepseekV2MoE):
+                    hidden_states = self.mlp(
+                        hidden_states,
+                        only_compute_routed=False,
+                        # dp_params,
+                        use_tp_weight=True,
+                    )
+                else:
+                    hidden_states = self.mlp(hidden_states)
+                # Add residual here.
+                if get_tp_world_rank() == 0:
+                    hidden_states = hidden_states + residual
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 3:
+            reserved_tensors.append(hidden_states)
+            hidden_states = tensor_model_parallel_all_reduce(
+                hidden_states,
+                tp_group=get_tp_world_group(),
+            )
+            reserved_tensors.append(hidden_states)
+
+            # Slice output.
+            hidden_states = hidden_states[dp_params.token_num_offset:
+                                          dp_params.token_num_offset+dp_params.token_num]
+            residual = hidden_states
+            return hidden_states, residual, reserved_tensors
+
+    def forward_without_dp_per_stage(
+        self,
+        kwargs: Dict[str, Any],
+    ) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:
+        stage_id = kwargs["stage_id"]
+        residual = kwargs["residual"]
+        hidden_states = kwargs["hidden_states"]
+        positions = kwargs["positions"]
+        attn_metadata = kwargs["attn_metadata"]
+        reserved_tensors = []
+
+        if stage_id == 0:
+            if residual is None:
+                residual = hidden_states
+                hidden_states = self.input_layernorm(hidden_states)
+            else:
+                hidden_states, residual = self.input_layernorm(hidden_states, residual)
+            kwargs_ = {
+                "positions": positions,
+                "hidden_states": hidden_states,
+                "prefill_k_tensor": kwargs["prefill_k_tensor"],
+                "prefill_v_tensor": kwargs["prefill_v_tensor"],
+                "req_seq_len": kwargs["req_seq_len"],
+                "enable_prefill_mcc": True,
+            }
+            hidden_states = partition_utils.execute_with_updated_forward_context(
+                self.vllm_config,
+                attn_metadata,
+                self.self_attn.forward,
+                kwargs_,
+            )
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 1:
+            hidden_states = tensor_model_parallel_all_reduce(hidden_states, tp_group=get_tp_group())
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 2:
+            hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
+            hidden_states = self.mlp.forward_compute(hidden_states)
+            return hidden_states, residual, reserved_tensors
+
+        if stage_id == 3:
+            hidden_states = self.mlp.forward_communication(hidden_states)
+            return hidden_states, residual, reserved_tensors
+
+
+class MLUDeepseekV2DecoderLayer(MLUDeepseekV2DecoderLayerWithMCC):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        vllm_config: VllmConfig,
+        model_config: ModelConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__(
+            config=config,
+            prefix=prefix,
+            vllm_config=vllm_config,
+            model_config=model_config,
+            cache_config=cache_config,
+            quant_config=quant_config,
+        )
+
+    def forward(
+        self,
+        positions_list: List[torch.Tensor],
+        hidden_states_list: List[torch.Tensor],
+        residual_list: List[Optional[torch.Tensor]],
+        dp_params_list: List[Optional[DataParallelRuntimeParams]],
+        is_first_layer: bool,
+        is_last_layer: bool,
+        event_list: List[torch.mlu.Event] = None,
+        attn_metadata_list: List[AttentionMetadata] = None,
+        streams: Optional[Dict] = None,
+        compute_stream: Optional[torch.mlu.Stream] = None,
+        communication_stream: Optional[torch.mlu.Stream] = None,
+        bs_parts: int = 1,
+        seq_parts: int = 1,
+        prefill_k_tensor: torch.Tensor = None,
+        prefill_v_tensor: torch.Tensor = None,
+        next_input_layernorm: Any = None,
+    ) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.mlu.Event], List[torch.Tensor]]:
+        num_parts = bs_parts * seq_parts
+        assert num_parts == len(positions_list), \
+            f"num_parts {num_parts} should be equal to the number of positions {len(positions_list)}"
+
+        if num_parts == 1:
+            hidden_states, residual = MLUDeepseekV2DecoderLayerBase.forward(
+                self=self,
+                positions=positions_list[0],
+                hidden_states=hidden_states_list[0],
+                residual=residual_list[0],
+                dp_params=dp_params_list[0],
+                streams=streams,
+                is_first_layer=is_first_layer,
+                is_last_layer=is_last_layer,
+                next_input_layernorm=next_input_layernorm,
+            )
+            return [hidden_states], [residual], event_list, []
+
+        return MLUDeepseekV2DecoderLayerWithMCC.forward(
+            self=self,
+            positions_list=positions_list,
+            hidden_states_list=hidden_states_list,
+            attn_metadata_list=attn_metadata_list,
+            residual_list=residual_list,
+            dp_params_list=dp_params_list,
+            event_list=event_list,
+            is_first_layer=is_first_layer,
+            is_last_layer=is_last_layer,
+            streams=streams,
+            compute_stream=compute_stream,
+            communication_stream=communication_stream,
+            bs_parts=bs_parts,
+            seq_parts=seq_parts,
+            prefill_k_tensor=prefill_k_tensor,
+            prefill_v_tensor=prefill_v_tensor,
+            next_input_layernorm=next_input_layernorm,
+        )
+
+
+@support_torch_compile
+class MLUDeepseekV2ModelBase(nn.Module):
+
+    fall_back_to_pt_during_load = False
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        model_config = vllm_config.model_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+
+        self.vocab_size = config.vocab_size
+        '''
+        =============================
+        Modify by vllm_mlu attn_dp
+        =============================
+        @brief: save attn data parallel configs.
+        '''
+        self.data_parallel_size = vllm_config.parallel_config.data_parallel_size
+        self.data_parallel_rank = vllm_config.parallel_config.data_parallel_rank
+        self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size
+        self.parallelize_shared_expert = vllm_config.mlu_config.dispatch_shared_expert_parallel
+        self.prefill_dispatch_use_RS_AG = (
+            vllm_config.mlu_config.prefill_dispatch_use_RS_AG)
+        if self.parallelize_shared_expert and self.data_parallel_size <= 1:
+            self.parallelize_shared_expert = False
+            logger.info(
+                "Disabling `mlu_config.dispatch_shared_expert_parallel` when data_parallel_size == 1."
+            )
+
+        # NOTE: When quantization is not enabled, parallelizing shared experts is not supported,
+        # because `moe_gen_index` does not support input tensors with stride.
+        if self.parallelize_shared_expert and quant_config is None:
+            self.parallelize_shared_expert = False
+            logger.info(
+                "Disabling `mlu_config.dispatch_shared_expert_parallel` because quant_config is None (non-quant mode is not supported)."
+            )
+
+        self.streams = (
+            {'shared': torch.mlu.Stream(), 'routed': torch.mlu.Stream()}
+            if self.parallelize_shared_expert else None
+        )
+        vllm_config.model_config.parallelize_shared_expert = self.parallelize_shared_expert
+
+        assert vllm_config.model_config.use_mla, "attn data parallel for deepseek must enable mla"
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        if get_pp_group().is_first_rank:
+            self.embed_tokens = DPVocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                prefix=f"{prefix}.embed_tokens",
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUDeepseekV2DecoderLayer(
+                config,
+                prefix,
+                vllm_config=vllm_config,
+                model_config=model_config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+            ),
+            prefix=f"{prefix}.layers")
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def get_input_embeddings(self, input_ids: torch.Tensor,
+                             dp_params: DataParallelRuntimeParams = None) -> torch.Tensor:
+        return self.embed_tokens(input_ids, dp_params)
+
+    def _forward_org(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        # for layer in self.layers[self.start_layer:self.end_layer]:
+        for i in range(self.start_layer, self.end_layer):
+            is_first_layer = (i == 0)
+            is_last_layer = (i == self.end_layer - 1)
+            next_input_layernorm = (
+                None if is_last_layer else self.layers[i+1].input_layernorm)
+            layer = self.layers[i]
+            hidden_states_list, residual_list, _, _ = layer(
+                positions_list=list([positions]),
+                hidden_states_list=list([hidden_states]),
+                residual_list=list([residual]),
+                dp_params_list=list([None]),
+                is_first_layer=is_first_layer,
+                is_last_layer=is_last_layer,
+                streams=self.streams,
+                next_input_layernorm=next_input_layernorm,
+            )
+            hidden_states = hidden_states_list[0]
+            residual = residual_list[0]
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+    def _forward_dp(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        """run model with dp."""
+        if dp_params is None:
+            dp_params = get_dp_metadata(positions.numel(),
+                                        self.data_parallel_size,
+                                        self.data_parallel_rank,
+                                        self.tensor_parallel_size,
+                                        self.prefill_dispatch_use_RS_AG)
+
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids, dp_params)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        for i in range(self.start_layer, self.end_layer):
+            is_first_layer = (i == 0)
+            is_last_layer = (i == self.end_layer - 1)
+            next_input_layernorm = (
+                None if is_last_layer else self.layers[i+1].input_layernorm)
+            layer = self.layers[i]
+            hidden_states_list, residual_list, _, _ = layer(
+                positions_list=list([positions]),
+                hidden_states_list=list([hidden_states]),
+                residual_list=list([residual]),
+                dp_params_list=list([dp_params]),
+                is_first_layer=is_first_layer,
+                is_last_layer=is_last_layer,
+                streams=self.streams,
+                next_input_layernorm=next_input_layernorm,
+            )
+            hidden_states = hidden_states_list[0]
+            residual = residual_list[0]
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        hidden_states = self.norm(hidden_states)
+
+        return hidden_states
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if self.data_parallel_size <= 1:
+            return self._forward_org(input_ids, positions, intermediate_tensors,
+                                     inputs_embeds)
+        else:
+            return self._forward_dp(input_ids, positions, intermediate_tensors,
+                                    inputs_embeds, dp_params)
+
+
+@support_torch_compile
+class MLUDeepseekV2ModelWithMCC(MLUDeepseekV2ModelBase):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        MLUDeepseekV2ModelBase.__init__(
+            self=self,
+            vllm_config=vllm_config,
+            prefix=prefix,
+        )
+        if self.vllm_config.mlu_config.is_dpsk_mcc_enabled:
+            self.compute_stream = torch.mlu.Stream()
+            self.communication_stream = torch.mlu.Stream()
+        else:
+            self.compute_stream = self.communication_stream = None
+
+        self.vllm_config = vllm_config
+        self.layer_num = vllm_config.model_config.hf_config.num_hidden_layers
+
+    def _get_kv_shape(self, seq_len) -> Tuple[int, int]:
+        """Get kv shape."""
+        hf_config = self.vllm_config.model_config.hf_config
+        qk_nope_head_dim = hf_config.qk_nope_head_dim
+        qk_rope_head_dim = hf_config.qk_rope_head_dim
+        qk_head_dim = qk_nope_head_dim + qk_rope_head_dim
+        v_head_dim = hf_config.v_head_dim
+        num_heads = hf_config.num_attention_heads
+        attn_tp_size = get_tensor_model_parallel_world_size()
+        num_local_heads = num_heads // attn_tp_size
+        return [seq_len, num_local_heads * qk_head_dim], [seq_len, num_local_heads * v_head_dim]
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        """run model with dp."""
+        forward_context: ForwardContext = get_forward_context()
+        attn_metadata: AttentionMetadata = forward_context.attn_metadata
+
+        is_in_graph_capture_or_dummy_run_phase = False
+        if enable_data_parallel() and dp_params is None:
+            if positions is None:
+                raise ValueError("positions tensor cannot be None")
+            is_in_graph_capture_or_dummy_run_phase = True
+            dp_params = get_dp_metadata(
+                positions.numel(),
+                self.data_parallel_size,
+                self.data_parallel_rank,
+                self.tensor_parallel_size,
+                self.prefill_dispatch_use_RS_AG,
+            )
+
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = (self.get_input_embeddings(input_ids, dp_params)
+                                 if dp_params is None or dp_params.token_num != 0 else None)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+
+        if is_in_graph_capture_or_dummy_run_phase:
+            bs_parts, seq_parts = 1, 1
+        else:
+            parts_to_split = self.vllm_config.mlu_config.prefill_mcc_parallel_num
+            bs_parts, seq_parts = partition_utils.attn_mcc_plan(attn_metadata, dp_params, parts_to_split)
+
+        # fallback to normal case.
+        if bs_parts * seq_parts == 1:
+            return MLUDeepseekV2ModelBase.forward(
+                self,
+                input_ids,
+                positions,
+                intermediate_tensors,
+                inputs_embeds,
+                dp_params,
+            )
+
+        prefill_k_tensor, prefill_v_tensor = None, None
+        if seq_parts > 1:
+            # Prepare prefix key value tensors.
+            _, layer_metadata = partition_utils.get_common_and_layer_metadata(attn_metadata)
+            if (layer_metadata is not None
+                and hasattr(layer_metadata, 'num_decodes')
+                and layer_metadata.num_decodes == 0):
+                num_tokens = layer_metadata.num_actual_tokens
+            else:
+                num_tokens = 0
+            k_shape, v_shape = self._get_kv_shape(num_tokens)
+            if hidden_states is not None:
+                prefill_k_tensor = torch.empty(k_shape, device=hidden_states.device, dtype=hidden_states.dtype)
+                prefill_v_tensor = torch.empty(v_shape, device=hidden_states.device, dtype=hidden_states.dtype)
+
+            attn_metadata_list = partition_utils.split_attn_metadata(attn_metadata, bs_parts, seq_parts)
+            hidden_states_list = partition_utils.split_input(hidden_states, bs_parts, seq_parts, attn_metadata_list)
+            residual_list = partition_utils.split_input(residual, bs_parts, seq_parts, attn_metadata_list)
+            dp_params_list = partition_utils.split_dp_params(
+                dp_params,
+                bs_parts,
+                seq_parts,
+                self.data_parallel_size,
+                self.tensor_parallel_size,
+                prefill_dispatch_use_RS_AG=self.prefill_dispatch_use_RS_AG,
+                dp_rank_=self.data_parallel_rank,
+            )
+            positions_list = partition_utils.split_positions(positions, bs_parts, seq_parts, attn_metadata)
+        elif seq_parts == 1:
+            # Split prefill data.
+            dp_params_list = partition_utils.split_dp_params(
+                dp_params,
+                bs_parts,
+                seq_parts,
+                self.data_parallel_size,
+                self.tensor_parallel_size,
+                prefill_dispatch_use_RS_AG=self.prefill_dispatch_use_RS_AG,
+                dp_rank_=self.data_parallel_rank,
+            )
+            attn_metadata_list = partition_utils.split_attn_metadata(attn_metadata, bs_parts, seq_parts)
+            hidden_states_list = partition_utils.split_input(hidden_states, bs_parts, seq_parts, attn_metadata_list)
+            residual_list = partition_utils.split_input(residual, bs_parts, seq_parts, attn_metadata_list)
+            positions_list = partition_utils.split_positions(positions, bs_parts, seq_parts, attn_metadata)
+        else:
+            assert False, \
+            "We do not support batch and sequence dimension split concurrently."
+
+        event_list = [None] * (bs_parts * seq_parts)
+        if bs_parts * seq_parts > 1:
+            event_list = [torch.mlu.Event() for _ in range(bs_parts * seq_parts)]
+            for event in event_list:
+                event.record(torch.mlu.current_stream())
+
+        for i in range(self.start_layer, self.end_layer):
+            # Prepare parameter lists.
+            kwargs = {
+                "positions_list": positions_list,
+                "hidden_states_list": hidden_states_list,
+                "attn_metadata_list": attn_metadata_list,
+                "residual_list": residual_list,
+                "dp_params_list": dp_params_list,
+                "streams": self.streams,
+                "bs_parts": bs_parts,
+                "seq_parts": seq_parts,
+                "prefill_k_tensor": prefill_k_tensor,
+                "prefill_v_tensor": prefill_v_tensor,
+                "event_list": event_list,
+                "compute_stream": self.compute_stream,
+                "communication_stream": self.communication_stream,
+                "is_first_layer": (i == 0),
+                "is_last_layer": (i == self.vllm_config.model_config.hf_config.num_hidden_layers - 1),
+            }
+            if kwargs["is_last_layer"]:
+                kwargs["next_input_layernorm"] = None
+            else:
+                kwargs["next_input_layernorm"] = self.layers[i+1].input_layernorm
+
+            # Run decode layer.
+            # Note: Here we need to return the temporary tensors in current decode layer
+            # to avoid memory corruption between adjacent layers.
+            hidden_states_list, residual_list, event_list, _ = self.layers[i](**kwargs)
+
+        # Post-invoke events for sequence dimension mcc.
+        if bs_parts * seq_parts > 1:
+            for event in event_list:
+                event.wait(torch.mlu.current_stream())
+
+        # Combine results.
+        hidden_states = torch.cat(hidden_states_list, dim=0)
+        residual = torch.cat(residual_list, dim=0)
+
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+
+        if not enable_data_parallel():
+            hidden_states, _ = self.norm(hidden_states, residual)
+        else:
+            hidden_states = self.norm(hidden_states)
+
+        return hidden_states
+
+@support_torch_compile
+class MLUDeepseekV2Model(MLUDeepseekV2ModelWithMCC):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        MLUDeepseekV2ModelWithMCC.__init__(
+            self=self,
+            vllm_config=vllm_config,
+            prefix=prefix,
+        )
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if self.vllm_config.mlu_config.is_dpsk_mcc_enabled:
+            return MLUDeepseekV2ModelWithMCC.forward(
+                self,
+                input_ids,
+                positions,
+                intermediate_tensors,
+                inputs_embeds,
+                dp_params,
+            )
+        return MLUDeepseekV2ModelBase.forward(
+            self,
+            input_ids,
+            positions,
+            intermediate_tensors,
+            inputs_embeds,
+            dp_params,
+        )
+
+
+class MLUDeepseekV2ForCausalLM(DeepseekV2ForCausalLM, MLUCausalLM):
+
+    def __init__(
+        self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super(DeepseekV2ForCausalLM, self).__init__()
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUDeepseekV2Model in this file.
+        '''
+        self.vllm_config = vllm_config
+        self.model = MLUDeepseekV2Model(
+            vllm_config=vllm_config,
+            prefix=maybe_prefix(prefix, "model"),
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        if get_pp_group().is_last_rank:
+            self.lm_head = DPParallelLMHead(config.vocab_size,
+                                            config.hidden_size,
+                                            quant_config=quant_config)
+        else:
+            self.lm_head = PPMissingLayer()
+        self.logits_processor = DPLogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def load_weights(
+        self,
+        weights: Iterable[Tuple[str, torch.Tensor]]
+    ) -> Set[str]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params and cal start expert id
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp) or isinstance(m, MLUDeepseekV2MLAAttention):
+                m.pack_params()
+
+        # expert parallel modification start
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        moe_ep_rank = moe_group_info.moe_ep_rank
+        num_total_experts = self.config.n_routed_experts
+        start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+        # expert parallel modification end
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: expert_params_mapping for fp8 block_wise
+        '''
+        is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) and self.quant_config.weight_block_size is not None
+        if is_fp8_block_wise and self.vllm_config.mlu_config.is_dpsk_mcc_enabled:
+            raise NotImplementedError("Deepseek with mcc currently doesn't support block-wise FP8 quantization")
+        is_fp8_smootquant = isinstance(self.quant_config, SmoothQuantConfig) and self.quant_config.is_fp8
+        if is_fp8_smootquant and self.vllm_config.mlu_config.is_dpsk_mcc_enabled:
+            raise NotImplementedError("Deepseek with mcc currently doesn't support SmoothQuant FP8 quantization")
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts) if is_fp8_block_wise else {}
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            pattern = r'layers\.([0-9]*)\.'
+            match = re.search(pattern, name)
+            if match:
+                layer_id = int(match.group(1))
+                if layer_id >= self.config.num_hidden_layers:
+                    continue
+            if "rotary_emb.inv_freq" in name:
+                continue
+
+            spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+            if spec_layer is not None:
+                continue  # skip spec decode layers for main model
+
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace expert_id in weight to named_expert_id in params_dict
+            '''
+            if start_expert_id > 0 and "mlp.experts." in name:
+                expert_str = re.search(r'experts\.\d+', name).group(0)
+                expert_id = int(expert_str.split(".")[1])
+                named_expert_id = expert_id - start_expert_id
+                if named_expert_id < 0:
+                    continue
+                old_expert_name = f"experts.{expert_id}"
+                new_expert_name = f"experts.{named_expert_id}"
+                name = name.replace(old_expert_name, new_expert_name)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                if is_fp8_block_wise and ("mlp.experts." in name) and name not in params_dict:
+                    continue
+                name = name.replace(weight_name, param_name)
+                if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                        and name not in params_dict):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                loaded_weight,
+                                name,
+                                shard_id=shard_id,
+                                expert_id=expert_id)
+                    break
+                else:
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: add expert skiped condition
+                    '''
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+
+                    if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                            and name not in params_dict):
+                        continue
+
+                    # Remapping the name of FP8 kv-scale.
+                    name = maybe_remap_kv_scale_name(name, params_dict)
+                    if name is None:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+                    '''
+                    ==================
+                    End of MLU Hijack
+                    ==================
+                    '''
+            loaded_params.add(name)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params after loading
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp) or isinstance(m, MLUDeepseekV2MLAAttention):
+                m.pack_params_after_loading()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        return loaded_params
+
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add dp params.
+    '''
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                inputs_embeds, dp_params)
+        return hidden_states
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                    sampling_metadata, dp_params=dp_params)
+        return logits
+
+    def get_experts_num(self):
+        return self.model_config.n_routed_experts
+
+    def get_ffn_num(self):
+        return self.model_config.first_k_dense_replace
+
+    def get_moe_num(self):
+        return self.get_layer_num() - self.model_config.first_k_dense_replace
+
+    def get_moe_inner_size(self):
+        return self.model_config.moe_intermediate_size
+
+
+    def get_shared_experts_inner_size(self):
+        if hasattr(self.model_config, "n_shared_experts") and self.model_config.n_shared_experts is not None:
+            return self.model_config.n_shared_experts * self.get_moe_inner_size()
+        else:
+            return 0
+
+    def get_special_config(self):
+        if hasattr(self.model_config, "qk_nope_head_dim") and self.model_config.qk_nope_head_dim is not None:
+            self.qk_nope_head_dim = self.model_config.qk_nope_head_dim
+        if hasattr(self.model_config, "qk_rope_head_dim") and self.model_config.qk_rope_head_dim is not None:
+            self.qk_rope_head_dim = self.model_config.qk_rope_head_dim
+        if hasattr(self.model_config, "q_lora_rank") and self.model_config.q_lora_rank is not None:
+            self.q_lora_rank = self.model_config.q_lora_rank
+        else:
+            self.q_lora_rank = 0
+        if hasattr(self.model_config, "kv_lora_rank") and self.model_config.kv_lora_rank is not None:
+            self.kv_lora_rank = self.model_config.kv_lora_rank
+
+    def get_kv_cache(self, batch_size, input_len, output_len):
+        batch          = batch_size
+        head_num_kv    = self.get_num_kv_heads()
+        head_size      = self.get_head_size()
+        dp_num         = self.data_parallel_size
+        layer_num      = self.get_layer_num()
+        input_seq_len  = input_len
+        output_seq_len = output_len
+        kv_cache_byte_width = data_type_byte_width_map[self.get_kv_cache_dtype()]
+        cla_coeffient  = self.get_cla_coefficient()
+        num_attention_heads = self.get_num_attn_heads()
+        qk_nope_head_dim = self.qk_nope_head_dim
+        qk_rope_head_dim = self.qk_rope_head_dim
+        v_head_dim = self.get_v_head_size()
+
+        kv_cache = batch // dp_num * num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim + v_head_dim) * \
+                (input_seq_len + output_seq_len // 2) * layer_num * kv_cache_byte_width
+        return kv_cache
+
+    def get_context_attn_flops(self, batch_size, input_len, hfu_qk_peak_compute=1.0, hfu_peak_compute=1.0):
+        seq_len          = input_len
+        qk_nope_head_dim = self.qk_nope_head_dim
+        qk_rope_head_dim = self.qk_rope_head_dim
+        v_head_dim       = self.get_v_head_size()
+        head_num         = self.get_num_attn_heads()
+        q_lora_rank      = self.q_lora_rank
+        kv_lora_rank     = self.kv_lora_rank
+        hidden_size = self.get_hidden_size()
+        is_causal = 0.5 if self.is_causal() else 1.0
+        # q_a_proj   weight(hidden_size, q_lora_rank)
+        # q_b_proj   weight(q_lora_rank, self.num_heads * self.qk_head_dim)   (optional)smoothquant
+        # kv_a_proj  weight(hidden_size, (self.kv_lora_rank + self.qk_rope_head_dim))
+        # kv_b_proj  weight(self.kv_lora_rank , self.num_heads * (self.qk_nope_head_dim + self.v_head_dim))
+        # o_proj     weight(num_head* v_head_size, hidden_size)
+        if self.smooth_quant_type == "invalid":
+            context_atn_pre = 2.0 * batch_size * seq_len * \
+                    (hidden_size * q_lora_rank * self.tensor_parallel_size + \
+                    hidden_size * (kv_lora_rank + qk_rope_head_dim) + \
+                    q_lora_rank * head_num * (qk_nope_head_dim + qk_rope_head_dim) + \
+                    kv_lora_rank * head_num * (qk_nope_head_dim + v_head_dim)) / \
+                    hfu_peak_compute
+        else:
+            # for q_a_proj / kv_a_proj / kv_b_proj don't support smooth quant_type, so "* 2" here
+            context_atn_pre = 2.0 * batch_size * seq_len * \
+                    (hidden_size * q_lora_rank * self.tensor_parallel_size * 2 + \
+                    hidden_size * (kv_lora_rank + qk_rope_head_dim) * 2 + \
+                    q_lora_rank * head_num * (qk_nope_head_dim + qk_rope_head_dim) + \
+                    kv_lora_rank * head_num * (qk_nope_head_dim + v_head_dim) * 2) / \
+                    hfu_peak_compute
+        context_atn_qk = 2 * batch_size * is_causal * seq_len * seq_len * head_num * (qk_nope_head_dim + qk_rope_head_dim) / hfu_qk_peak_compute
+        context_atn_qkv = 2 * batch_size * is_causal * seq_len * seq_len * head_num * v_head_dim / hfu_qk_peak_compute
+        context_atn_post = 2 * batch_size * seq_len * head_num * v_head_dim * hidden_size / hfu_peak_compute
+        return context_atn_pre + context_atn_qk + context_atn_qkv + context_atn_post
+
+    def get_attn_param(self):
+        num_attention_heads = self.get_num_attn_heads()
+        qk_nope_head_dim = self.qk_nope_head_dim
+        qk_rope_head_dim = self.qk_rope_head_dim
+        v_head_dim = self.get_v_head_size()
+        q_lora_rank = self.q_lora_rank
+        kv_lora_rank = self.kv_lora_rank
+        hidden_size = self.get_hidden_size()
+        layer_num = self.get_layer_num()
+
+        qa_params = hidden_size * q_lora_rank
+        kva_params = hidden_size * (kv_lora_rank + qk_rope_head_dim)
+        qb_params = q_lora_rank * num_attention_heads * (qk_nope_head_dim + qk_rope_head_dim)
+        kvb_params = kv_lora_rank * num_attention_heads * (qk_nope_head_dim + v_head_dim)
+        o_params = v_head_dim * num_attention_heads * hidden_size
+
+        mla_weight_params = (qa_params + kva_params + qb_params + kvb_params + o_params) * layer_num
+
+        return mla_weight_params

