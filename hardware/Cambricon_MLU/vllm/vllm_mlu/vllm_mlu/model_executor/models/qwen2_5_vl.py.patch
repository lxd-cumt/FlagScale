diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py
new file mode 100644
index 000000000..8896ebbda
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_5_vl.py
@@ -0,0 +1,466 @@
+from functools import cached_property, partial
+from typing import (Callable, Iterable, List, Literal, Mapping, Optional, Set,
+                    Tuple, TypedDict, Union)
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn import Module
+from einops import rearrange, repeat
+from transformers import BatchFeature
+from transformers.models.qwen2_5_vl import Qwen2_5_VLProcessor
+from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import (
+    Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig)
+
+from vllm.config import VllmConfig
+from vllm.distributed import parallel_state, tensor_model_parallel_all_gather
+from vllm.distributed import utils as dist_utils
+from vllm.logger import init_logger
+from vllm.model_executor import SamplingMetadata
+from vllm.model_executor.layers.activation import _ACTIVATION_REGISTRY
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.gptq import GPTQConfig
+from vllm.model_executor.layers.quantization.gptq_marlin import (
+    GPTQMarlinConfig)
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.module_mapping import MultiModelKeys
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import MultiModalFieldConfig
+from vllm.platforms import _Backend
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.config import uses_mrope
+
+from vllm.model_executor.models.interfaces import (
+    MultiModalEmbeddings, SupportsLoRA, SupportsMultiModal, SupportsPP)
+from vllm.model_executor.models.qwen2_vl import (
+    Qwen2VLDummyInputsBuilder as Qwen2_5_VLDummyInputsBuilder)
+from vllm.model_executor.models.qwen2_vl import (
+    Qwen2VLMultiModalProcessor, Qwen2VLProcessingInfo,
+    apply_rotary_pos_emb_vision)
+from vllm.model_executor.models.qwen2_5_vl import (
+    Qwen2_5_VisionMLP, Qwen2_5_VisionPatchEmbed,
+    Qwen2_5_VisionRotaryEmbedding, Qwen2_5_VisionPatchMerger,
+    Qwen2_5_VisionAttention, Qwen2_5_VisionBlock,
+    Qwen2_5_VisionTransformer, Qwen2_5_VLForConditionalGeneration,
+    Qwen2_5_VLMultiModalProcessor, Qwen2_5_VLProcessingInfo)
+from vllm.model_executor.models.utils import (
+    AutoWeightsLoader, WeightsMapper, cast_overflow_tensors,
+    init_vllm_registered_model, maybe_prefix, merge_multimodal_embeddings)
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.models.layer_utils import is_smoothquant
+from vllm_mlu.model_executor.models.utils import set_attn_compute_dtype
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu import _mlu_ops as mlu_ops
+
+logger = init_logger(__name__)
+
+class MLUQwen2_5_VisionAttention(Qwen2_5_VisionAttention):
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        cu_seqlens: torch.Tensor,
+        rotary_pos_emb: torch.Tensor,
+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention
+        seqlens: Optional[list[int]] = None,  # Only used for xFormers
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ):
+        # [s, b, c] --> [s, b, 3 * head * head_dim]
+        x, _ = self.qkv(x, smooth_quant_scale)
+
+        # [s, b, 3 * head * head_dim] -> 3 * [s, b, head, head_dim]
+        q, k, v = self.split_qkv(x)
+        batch_size = q.shape[1]
+
+        q, k, v = (rearrange(x, "s b ... -> b s ...") for x in (q, k, v))
+        head_dim = q.shape[-1]
+        if rotary_pos_emb is not None:
+            sin = rotary_pos_emb.sin
+            cos = rotary_pos_emb.cos
+            q = mlu_ops.rotary_embedding(
+                q, sin, cos, None, None, False, False, False, q.shape[1]
+            )
+            k = mlu_ops.rotary_embedding(
+                k, sin, cos, None, None, False, False, False, k.shape[1]
+            )
+        q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+        # fa use half for better performance
+        output = mlu_ops.flash_attention(q,
+                                         k,
+                                         v,
+                                         out=None,
+                                         cu_seq_lens_q=cu_seqlens,
+                                         cu_seq_lens_kv=cu_seqlens,
+                                         max_seq_len_q=max_seqlen,
+                                         max_seq_len_kv=max_seqlen,
+                                         alibi_slope=None,
+                                         attn_bias=None,
+                                         softmax_scale=head_dim ** -0.5,
+                                         compute_dtype=torch.half,
+                                         is_causal=False)
+        context_layer = rearrange(output, "(b s) h d -> s b (h d)", b=batch_size)
+
+        output, _ = self.proj(context_layer, residual)
+        return output
+
+class MLUQwen2_5_VisionBlock(Qwen2_5_VisionBlock):
+
+    def __init__(
+        self,
+        dim: int,
+        num_heads: int,
+        mlp_hidden_dim: int,
+        act_layer: str,
+        norm_layer: Optional[Callable[[int], nn.Module]] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        Module.__init__(self)
+        if norm_layer is None:
+            norm_layer = partial(nn.LayerNorm, eps=1e-6)
+        self.norm1 = norm_layer(dim)
+        self.norm2 = norm_layer(dim)
+        self.attn = MLUQwen2_5_VisionAttention(embed_dim=dim,
+                                               num_heads=num_heads,
+                                               projection_size=dim,
+                                               quant_config=quant_config,
+                                               prefix=f"{prefix}.attn")
+        self.mlp = FeedForward(hidden_size=dim,
+                               intermediate_size=mlp_hidden_dim,
+                               hidden_act=act_layer,
+                               up_proj_name='gate_up_proj',
+                               is_gated=True,
+                               down_proj_name='down_proj',
+                               bias=True,
+                               quant_config=quant_config,
+                               prefix=f"{prefix}.mlp")
+        # prepare to perf per-tensor sq cases if suitable;
+        # bypass residual in matmul logics if fp8_block_wise quantization;
+        self.is_per_token_sq_perf_cases = False
+
+    def forward_norm(self, norm_layer, x):
+        '''
+        forward layernorm and return sq_scale if per_token_sq
+        '''
+        if self.is_per_token_sq_perf_cases:
+            layernorm_output, smooth_quant_scale = norm_layer(x)
+        else:
+            layernorm_output = norm_layer(x)
+            smooth_quant_scale = None
+        return layernorm_output, smooth_quant_scale
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        cu_seqlens: torch.Tensor,
+        rotary_pos_emb: torch.Tensor,
+        max_seqlen: Optional[int] = None,
+        seqlens: Optional[list[int]] = None
+    ):
+        layernorm_output, smooth_quant_scale = self.forward_norm(self.norm1, x)
+        residual = x
+        attn_output = self.attn(
+                layernorm_output,
+                cu_seqlens,
+                rotary_pos_emb,
+                max_seqlen,
+                seqlens,
+                residual,
+                smooth_quant_scale)
+        layernorm_output, smooth_quant_scale = self.forward_norm(self.norm2, attn_output)
+        residual = attn_output
+        org_shape = layernorm_output.shape
+        hidden_states = self.mlp(layernorm_output, residual, smooth_quant_scale)
+        return hidden_states.view(org_shape)
+
+class MLUQwen2_5_VisionPatchMerger(Qwen2_5_VisionPatchMerger):
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.ln_q(x)
+        x = x.view(-1, self.hidden_size)
+
+        mlp_fc1, mlp_act, mlp_fc2 = self.mlp
+        x_parallel, _ = mlp_fc1(x)
+        x_parallel = mlu_ops.active(x_parallel, "gelu", is_gated=False)
+        out, _ = mlp_fc2(x_parallel)
+        return out
+
+class MLUQwen2_5_VisionTransformer(Qwen2_5_VisionTransformer):
+
+    def __init__(
+        self,
+        vision_config: Qwen2_5_VLVisionConfig,
+        norm_eps: float = 1e-6,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        Module.__init__(self)
+        patch_size = vision_config.patch_size
+        temporal_patch_size = vision_config.temporal_patch_size
+        in_channels = vision_config.in_channels
+        depth = vision_config.depth
+        self.hidden_size = vision_config.hidden_size
+        self.num_heads = vision_config.num_heads
+
+        # args for get_window_index_thw
+        self.window_size = vision_config.window_size
+        self.patch_size = vision_config.patch_size
+        self.spatial_merge_size = vision_config.spatial_merge_size
+        self.fullatt_block_indexes = vision_config.fullatt_block_indexes
+        self.spatial_merge_unit = self.spatial_merge_size**2
+
+        self.patch_embed = Qwen2_5_VisionPatchEmbed(
+            patch_size=patch_size,
+            temporal_patch_size=temporal_patch_size,
+            in_channels=in_channels,
+            hidden_size=self.hidden_size,
+        )
+
+        norm_layer = partial(RMSNorm, eps=norm_eps)
+        head_dim = self.hidden_size // self.num_heads
+        self.rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding(head_dim // 2)
+
+        self.blocks = nn.ModuleList([
+            MLUQwen2_5_VisionBlock(
+                dim=self.hidden_size,
+                num_heads=self.num_heads,
+                mlp_hidden_dim=vision_config.intermediate_size,
+                act_layer=vision_config.hidden_act,
+                norm_layer=norm_layer,
+                quant_config=quant_config,
+                prefix=f"{prefix}.blocks.{layer_idx}")
+            for layer_idx in range(depth)
+        ])
+        self.merger = MLUQwen2_5_VisionPatchMerger(
+            d_model=vision_config.out_hidden_size,
+            context_dim=self.hidden_size,
+            norm_layer=norm_layer,
+            spatial_merge_size=self.spatial_merge_size,
+            quant_config=quant_config,
+            prefix=f"{prefix}.merger",
+        )
+        self.attn_backend = _Backend.FLASH_ATTN
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        grid_thw: torch.Tensor
+    ):
+        # patchify
+        seq_len, _ = x.size()
+        rotary_pos_emb = []
+        window_index: list = []
+        cu_window_seqlens: list = [torch.tensor([0], dtype=torch.int32)]
+        cu_seqlens: list = []
+
+        hidden_states = x.to(device=self.device, dtype=self.dtype)
+        hidden_states = self.patch_embed(hidden_states)
+
+        window_index_id = 0
+        cu_window_seqlens_last = 0
+        for t, h, w in grid_thw:
+            t, h, w = int(t), int(h), int(w)
+            llm_h = h // self.spatial_merge_size
+            llm_w = w // self.spatial_merge_size
+
+            (
+                rotary_pos_emb_thw,
+                window_index_thw,
+                cu_seqlens_window_thw,
+                cu_seqlens_thw,
+            ) = self.get_rope_by_thw(t, h, w)
+
+            window_index.append(window_index_thw + window_index_id)
+            window_index_id += (t * llm_h * llm_w)
+
+            cu_seqlens_window_thw = (cu_seqlens_window_thw +
+                                     cu_window_seqlens_last)
+            cu_window_seqlens_last = cu_seqlens_window_thw[-1]
+            cu_window_seqlens.append(cu_seqlens_window_thw)
+
+            rotary_pos_emb.append(rotary_pos_emb_thw)
+
+            cu_seqlens.append(cu_seqlens_thw)
+
+        rotary_pos_emb = torch.cat(rotary_pos_emb)
+        window_index = torch.cat(window_index)
+        cu_window_seqlens = torch.cat(cu_window_seqlens)
+        cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)
+        cu_seqlens = torch.cat(cu_seqlens)
+        cu_seqlens = torch.cumsum(cu_seqlens, dim=0, dtype=torch.int32)
+        cu_seqlens = F.pad(cu_seqlens, (1, 0), "constant", 0)
+
+        # transformers
+        # pre-compute seqlens for window/full attn to reduce cuMemcpy operations
+        max_seqlen_full, seqlens_full = self.compute_attn_mask_seqlen(
+            cu_seqlens)
+        max_seqlen_window, seqlens_window = self.compute_attn_mask_seqlen(
+            cu_window_seqlens)
+
+        cu_seqlens = cu_seqlens.to(device=self.device, non_blocking=True)
+        cu_window_seqlens = cu_window_seqlens.to(device=self.device,
+                                                 non_blocking=True)
+        rotary_pos_emb = rotary_pos_emb.to(device=self.device,
+                                           non_blocking=True)
+        window_index = window_index.to(device=hidden_states.device,
+                                       non_blocking=True)
+
+        hidden_states = hidden_states.reshape(
+            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
+        hidden_states = hidden_states[window_index, :, :]
+        hidden_states = hidden_states.reshape(seq_len, -1)
+
+        hidden_states = hidden_states.unsqueeze(1)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: compute cos and sin once for rope
+        '''
+        # compute cos sin for apply_rope
+        cos = rotary_pos_emb.cos()
+        sin = rotary_pos_emb.sin()
+        cos = repeat(cos, "... d -> ... (2 d)")
+        sin = repeat(sin, "... d -> ... (2 d)")
+        rotary_pos_emb.cos = cos.to(hidden_states.dtype)
+        rotary_pos_emb.sin = sin.to(hidden_states.dtype)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        for layer_num, blk in enumerate(self.blocks):
+            if layer_num in self.fullatt_block_indexes:
+                cu_seqlens_now = cu_seqlens
+                max_seqlen_now = max_seqlen_full
+                seqlens_now = seqlens_full
+            else:
+                cu_seqlens_now = cu_window_seqlens
+                max_seqlen_now = max_seqlen_window
+                seqlens_now = seqlens_window
+
+            hidden_states = blk(
+                hidden_states,
+                cu_seqlens=cu_seqlens_now,
+                rotary_pos_emb=rotary_pos_emb,
+                max_seqlen=max_seqlen_now,
+                seqlens=seqlens_now,
+            )
+
+        # For Qwen2.5-VL-3B, float16 will overflow at last block
+        # for long visual tokens sequences.
+        if hidden_states.dtype == torch.float16:
+            hidden_states = cast_overflow_tensors(hidden_states)
+
+        # adapter
+        hidden_states = self.merger(hidden_states)
+        reverse_indices = torch.argsort(window_index)
+        hidden_states = hidden_states[reverse_indices, :]
+        return hidden_states
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add gate_up_proj info to adapt to FeedForward layer
+        '''
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("attn.qkv.", "attn.q.", "q"),
+            ("attn.qkv.", "attn.k.", "k"),
+            ("attn.qkv.", "attn.v.", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        params_dict = dict(self.named_parameters(remove_duplicate=False))
+        loaded_params: set[str] = set()
+
+        for name, loaded_weight in weights:
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
+
+
+@MULTIMODAL_REGISTRY.register_processor(
+    Qwen2_5_VLMultiModalProcessor,
+    info=Qwen2_5_VLProcessingInfo,
+    dummy_inputs=Qwen2_5_VLDummyInputsBuilder)
+class MLUQwen2_5_VLForConditionalGeneration(
+            Qwen2_5_VLForConditionalGeneration, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        Module.__init__(self)
+        SupportsMultiModal.__init__(self)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        SupportsLoRA.__init__(self)
+        SupportsPP.__init__(self)
+        config: Qwen2_5_VLConfig = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        multimodal_config = vllm_config.model_config.multimodal_config
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+
+        self.visual = MLUQwen2_5_VisionTransformer(
+            config.vision_config,
+            norm_eps=getattr(config, "rms_norm_eps", 1e-6),
+            quant_config=self._maybe_ignore_quant_config(quant_config),
+            prefix=maybe_prefix(prefix, "visual"),
+        )
+
+        self.language_model = init_vllm_registered_model(
+            vllm_config=vllm_config,
+            prefix=maybe_prefix(prefix, "language_model"),
+            architectures=["Qwen2ForCausalLM"],
+        )
+
+        self.make_empty_intermediate_tensors = (
+            self.language_model.make_empty_intermediate_tensors)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        **kwargs: object,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        # TODO: FA may standardize on half precision computation in the future
+        #       set_attn_compute_dtype might be deprecated and removed
+        set_attn_compute_dtype(torch.half)
+        return Qwen2_5_VLForConditionalGeneration.forward(
+                self,
+                input_ids,
+                positions,
+                intermediate_tensors,
+                inputs_embeds,
+                **kwargs)
+

