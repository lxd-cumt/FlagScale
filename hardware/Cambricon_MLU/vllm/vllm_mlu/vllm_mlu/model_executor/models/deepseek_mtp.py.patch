diff --git a/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py
new file mode 100644
index 000000000..a25c644f3
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/deepseek_mtp.py
@@ -0,0 +1,539 @@
+# SPDX-License-Identifier: Apache-2.0
+from typing import Iterable, Set, Tuple, Optional, Any
+import re
+
+import torch
+
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.deepseek_v2 import get_spec_layer_idx_from_weight_name, DeepseekV2MLAAttention
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp, MoeGroupInfo
+from transformers import PretrainedConfig
+from vllm.config import CacheConfig, ModelConfig, VllmConfig, ParallelConfig, MluConfig
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.models.deepseek_mtp import (
+    DeepSeekMTP, DeepSeekMultiTokenPredictor, DeepSeekMultiTokenPredictorLayer, SharedHead)
+from vllm.model_executor.models.deepseek_v2 import DeepseekV2DecoderLayer
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+from vllm_mlu.model_executor.layers.dp_vocab_parallel_embedding import (
+    DPParallelLMHead, DPVocabParallelEmbedding)
+from vllm_mlu.model_executor.layers.dp_logits_processor import DPLogitsProcessor
+from vllm_mlu.model_executor.models.deepseek_v2 import MLUDeepseekV2DecoderLayer
+from vllm_mlu.model_executor.models.dp_utils import (
+    get_dp_metadata, DataParallelRuntimeParams)
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__load_weights(
+        self, weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack params and cal start expert id
+    '''
+    for name, m in self.model.named_modules():
+        if isinstance(m, SparseMoeMlp) or isinstance(m, DeepseekV2MLAAttention):
+            m.pack_params()
+
+    # expert parallel modification start
+    moe_group_info = MoeGroupInfo()
+    moe_ep_size = moe_group_info.moe_ep_size
+    moe_ep_rank = moe_group_info.moe_ep_rank
+    num_total_experts = self.config.n_routed_experts
+    start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+    # expert parallel modification end
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    stacked_params_mapping = [
+        ("gate_up_proj", "gate_proj", 0),
+        ("gate_up_proj", "up_proj", 1),
+    ]
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: delete expert_params_mapping for useless
+    '''
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    params_dict = dict(self.named_parameters())
+    loaded_params: Set[str] = set()
+    for name, loaded_weight in weights:
+        if "rotary_emb.inv_freq" in name:
+            continue
+        spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+        if spec_layer is None:
+            continue
+        name = self._rewrite_spec_layer_name(spec_layer, name)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace expert_id in weight to named_expert_id in params_dict
+        '''
+        if start_expert_id > 0 and "mlp.experts." in name:
+            expert_str = re.search(r'experts\.\d+', name).group(0)
+            expert_id = int(expert_str.split(".")[1])
+            named_expert_id = expert_id - start_expert_id
+            if named_expert_id < 0:
+                continue
+            old_expert_name = f"experts.{expert_id}"
+            new_expert_name = f"experts.{named_expert_id}"
+            name = name.replace(old_expert_name, new_expert_name)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        for (param_name, weight_name, shard_id) in stacked_params_mapping:
+            # Skip non-stacked layers and experts (experts handled below).
+            if weight_name not in name:
+                continue
+            # We have mlp.experts[0].gate_proj in the checkpoint.
+            # Since we handle the experts below in expert_params_mapping,
+            # we need to skip here BEFORE we update the name, otherwise
+            # name will be updated to mlp.experts[0].gate_up_proj, which
+            # will then be updated below in expert_params_mapping
+            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+            '''
+            name = name.replace(weight_name, param_name)
+            if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+
+            param = params_dict[name]
+            weight_loader = param.weight_loader
+            weight_loader(param, loaded_weight, shard_id)
+            break
+        else:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: delete expert_params_mapping weight load
+            '''
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            # Skip loading extra bias for GPTQ models.
+            if name.endswith(".bias") and name not in params_dict:
+                continue
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add expert skiped condition
+            '''
+            if (("mlp.experts." in name or "mlp.shared_experts." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                continue
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+            param = params_dict[name]
+            weight_loader = getattr(param, "weight_loader",
+                                    default_weight_loader)
+            weight_loader(param, loaded_weight)
+
+        loaded_params.add(name)
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: pack_params after loading
+    '''
+    for name, m in self.model.named_modules():
+        if isinstance(m, SparseMoeMlp) or isinstance(m, DeepseekV2MLAAttention):
+            m.pack_params_after_loading()
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    return loaded_params
+
+def vllm__module_executor__models__deepseek_mtp__SharedHead__init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super(SharedHead, self).__init__()
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.head = DPParallelLMHead(config.vocab_size,
+                                     config.hidden_size,
+                                     quant_config=quant_config)
+
+class MLUDeepSeekMultiTokenPredictorLayer(torch.nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        prefix: str,
+        model_config: ModelConfig,
+        parallel_config: ParallelConfig,
+        mlu_config: MluConfig,
+        vllm_config: VllmConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ) -> None:
+        super().__init__()
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace VocabParallelEmbedding with DPVocabParallelEmbedding.
+        '''
+        self.embed_tokens = DPVocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+        )
+        '''
+        =================
+        End of MLU Hijack
+        =================
+        '''
+
+        self.enorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.hnorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.eh_proj = torch.nn.Linear(config.hidden_size * 2,
+                                 config.hidden_size,
+                                 bias=False)
+        self.shared_head = SharedHead(config=config, quant_config=quant_config)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace DeepseekV2DecoderLayer with MLUDeepseekV2DecoderLayer.
+        '''
+        self.mtp_block = MLUDeepseekV2DecoderLayer(
+            config=config,
+            prefix=prefix,
+            vllm_config=vllm_config,
+            model_config=model_config,
+            cache_config=cache_config,
+            quant_config=quant_config,
+        )
+        '''
+        =================
+        End of MLU Hijack
+        =================
+        '''
+
+        self.data_parallel_size = parallel_config.data_parallel_size
+        self.data_parallel_rank = parallel_config.data_parallel_rank
+        self.tensor_parallel_size = parallel_config.tensor_parallel_size
+        self.prefill_dispatch_use_RS_AG = mlu_config.prefill_dispatch_use_RS_AG
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_index: int = 0,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> torch.Tensor:
+        if self.data_parallel_size > 1:
+            return self.forward_dp(
+                input_ids=input_ids,
+                positions=positions,
+                previous_hidden_states=previous_hidden_states,
+                inputs_embeds=inputs_embeds,
+                spec_step_index=spec_step_index,
+                dp_params=dp_params,
+            )
+        else:
+            return self.forward_without_dp(
+                input_ids=input_ids,
+                positions=positions,
+                previous_hidden_states=previous_hidden_states,
+                inputs_embeds=inputs_embeds,
+                spec_step_index=spec_step_index)
+
+    def forward_without_dp(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_index: int = 0,
+    ) -> torch.Tensor:
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+        assert inputs_embeds is not None
+        # masking inputs at position 0, as not needed by MTP
+        inputs_embeds[positions == 0] = 0
+        inputs_embeds = self.enorm(inputs_embeds)
+        previous_hidden_states = self.hnorm(previous_hidden_states)
+
+        hidden_states = self.eh_proj(
+            torch.cat([inputs_embeds, previous_hidden_states], dim=-1))
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: update mtp block parameters to list.
+        '''
+        hidden_states_list, residual_list, _, _ = self.mtp_block(
+            positions_list=[positions],
+            hidden_states_list=[hidden_states],
+            residual_list=[None],
+            dp_params_list =[None],
+            is_first_layer=True,
+            is_last_layer=True,
+        )
+        hidden_states = hidden_states_list[0]
+        residual = residual_list[0]
+        '''
+        =================
+        End of MLU Hijack
+        =================
+        '''
+        hidden_states = residual + hidden_states
+        return hidden_states
+
+
+    def forward_dp(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        previous_hidden_states: torch.Tensor,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        spec_step_index: int = 0,
+        dp_params: Optional[DataParallelRuntimeParams] = None,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add dp_params to record data parallel info.
+        '''
+        if dp_params is None:
+            dp_params = get_dp_metadata(positions.numel(),
+                                        self.data_parallel_size,
+                                        self.data_parallel_rank,
+                                        self.tensor_parallel_size,
+                                        self.prefill_dispatch_use_RS_AG)
+        '''
+        =================
+        End of MLU Hijack
+        =================
+        '''
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids, dp_params)
+
+        assert inputs_embeds is not None
+        # masking inputs at position 0, as not needed by MTP
+        inputs_embeds[positions == 0] = 0
+        inputs_embeds = self.enorm(inputs_embeds)
+        previous_hidden_states = self.hnorm(previous_hidden_states)
+
+        hidden_states = self.eh_proj(
+            torch.cat([inputs_embeds, previous_hidden_states], dim=-1))
+
+        hidden_states_list, residual_list, _, _ = self.mtp_block(
+            positions_list=[positions],
+            hidden_states_list=[hidden_states],
+            residual_list=[None],
+            dp_params_list=[dp_params],
+            is_first_layer=True,
+            is_last_layer=True,
+        )
+        hidden_states = hidden_states_list[0]
+        residual = residual_list[0]
+        if residual is not None:
+            hidden_states = residual + hidden_states
+        return hidden_states
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor____init__(
+        self, *, vllm_config: VllmConfig, prefix: str = ""):
+    super(DeepSeekMultiTokenPredictor, self).__init__()
+    config = vllm_config.model_config.hf_config
+    self.mtp_start_layer_idx = config.num_hidden_layers
+    self.num_mtp_layers = config.num_nextn_predict_layers
+
+    '''
+    =============================
+    Modify by vllm_mlu attn_dp
+    =============================
+    @brief: save attn data parallel configs.
+    '''
+    self.data_parallel_size = vllm_config.parallel_config.data_parallel_size
+    self.data_parallel_rank = vllm_config.parallel_config.data_parallel_rank
+    self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size
+
+    self.prefill_dispatch_use_RS_AG = (
+            vllm_config.mlu_config.prefill_dispatch_use_RS_AG)
+
+    assert vllm_config.model_config.use_mla, "attn data parallel for deepseek requires MLA to be enabled for proper distributed attention support"
+
+    # to map the exact layer index from weights
+    self.layers = torch.nn.ModuleDict({
+        str(idx):
+        MLUDeepSeekMultiTokenPredictorLayer(
+            config,
+            f"{prefix}.layers.{idx}",
+            model_config=vllm_config.model_config,
+            parallel_config=vllm_config.parallel_config,
+            mlu_config=vllm_config.mlu_config,
+            vllm_config=vllm_config,
+            cache_config=vllm_config.cache_config,
+            quant_config=vllm_config.quant_config,
+        )
+        for idx in range(self.mtp_start_layer_idx,
+                            self.mtp_start_layer_idx + self.num_mtp_layers)
+    })
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    self.logits_processor = DPLogitsProcessor(config.vocab_size)
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor__forward(
+    self,
+    input_ids: torch.Tensor,
+    positions: torch.Tensor,
+    previous_hidden_states: torch.Tensor,
+    inputs_embeds: Optional[torch.Tensor] = None,
+    spec_step_idx: int = 0,
+    dp_params: Optional[DataParallelRuntimeParams] = None,
+) -> torch.Tensor:
+    current_step_idx = (spec_step_idx % self.num_mtp_layers)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add dp_params.
+    '''
+    kwargs = dict()
+    if dp_params is not None:
+        kwargs['dp_params'] = dp_params
+    return self.layers[str(self.mtp_start_layer_idx + current_step_idx)](
+        input_ids,
+        positions,
+        previous_hidden_states,
+        inputs_embeds,
+        current_step_idx,
+        **kwargs,
+    )
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__forward(
+    self,
+    input_ids: torch.Tensor,
+    positions: torch.Tensor,
+    previous_hidden_states: torch.Tensor,
+    intermediate_tensors: Optional[IntermediateTensors] = None,
+    inputs_embeds: Optional[torch.Tensor] = None,
+    spec_step_idx: int = 0,
+    dp_params: Optional[DataParallelRuntimeParams] = None,
+):
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add dp_params.
+    '''
+    hidden_states = self.model(input_ids, positions,
+                               previous_hidden_states, inputs_embeds,
+                               spec_step_idx, dp_params=dp_params)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return hidden_states
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor__compute_logits(
+    self,
+    hidden_states: torch.Tensor,
+    sampling_metadata: SamplingMetadata,
+    spec_step_idx: int = 0,
+    dp_params: Optional[DataParallelRuntimeParams] = None,
+) -> torch.Tensor:
+    current_step_idx = (spec_step_idx % self.num_mtp_layers)
+    mtp_layer = self.layers[str(self.mtp_start_layer_idx +
+                                current_step_idx)]
+    logits = self.logits_processor(mtp_layer.shared_head.head,
+                                   mtp_layer.shared_head(hidden_states),
+                                   sampling_metadata,
+                                   dp_params=dp_params)
+    return logits
+
+
+def vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__compute_logits(
+    self,
+    hidden_states: torch.Tensor,
+    sampling_metadata: SamplingMetadata,
+    spec_step_idx: int = 0,
+    dp_params: Optional[DataParallelRuntimeParams] = None,
+) -> Optional[torch.Tensor]:
+    return self.model.compute_logits(hidden_states, sampling_metadata,
+                                     spec_step_idx, dp_params)
+
+
+MluHijackObject.apply_hijack(DeepSeekMTP,
+                             DeepSeekMTP.load_weights,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__load_weights)
+
+MluHijackObject.apply_hijack(SharedHead,
+                             SharedHead.__init__,
+                             vllm__module_executor__models__deepseek_mtp__SharedHead__init__)
+MluHijackObject.apply_hijack(DeepSeekMultiTokenPredictor,
+                             DeepSeekMultiTokenPredictor.__init__,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor____init__)
+MluHijackObject.apply_hijack(DeepSeekMultiTokenPredictor,
+                             DeepSeekMultiTokenPredictor.forward,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor__forward)
+MluHijackObject.apply_hijack(DeepSeekMultiTokenPredictor,
+                             DeepSeekMultiTokenPredictor.compute_logits,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMultiTokenPredictor__compute_logits)
+MluHijackObject.apply_hijack(DeepSeekMTP,
+                             DeepSeekMTP.forward,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__forward)
+MluHijackObject.apply_hijack(DeepSeekMTP,
+                             DeepSeekMTP.compute_logits,
+                             vllm__module_executor__models__deepseek_mtp__DeepSeekMTP__compute_logits)

