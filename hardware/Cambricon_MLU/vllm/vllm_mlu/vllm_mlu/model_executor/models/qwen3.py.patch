diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen3.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen3.py
new file mode 100644
index 000000000..508be2c26
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen3.py
@@ -0,0 +1,367 @@
+from typing import Optional, Tuple, Union, Iterable
+
+import torch
+from torch import nn
+from transformers import Qwen3Config
+
+from vllm.attention import AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group
+from vllm.logger import init_logger
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+from vllm.model_executor.layers.sampler import get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
+from vllm.sequence import IntermediateTensors
+
+from vllm.model_executor.models.interfaces import SupportsLoRA, SupportsPP
+from vllm.model_executor.models.qwen2 import Qwen2Model
+from vllm.model_executor.models.utils import (
+    AutoWeightsLoader, PPMissingLayer, maybe_prefix)
+
+from vllm.model_executor.models.qwen3 import Qwen3Attention, Qwen3ForCausalLM
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.models.utils import is_tie_word_embeddings
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+
+logger = init_logger(__name__)
+
+class MLUQwen3Attention(Qwen3Attention):
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # qwen3
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                            self.head_dim)
+        q_by_head = self.q_norm(q_by_head)
+        q = q_by_head.view(q.shape)
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                            self.head_dim)
+        k_by_head = self.k_norm(k_by_head)
+        k = k_by_head.view(k.shape)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        '''
+        qk = torch.cat([q, k], dim=-1)
+        self.rotary_emb(positions, qk.view(-1, self.num_heads + self.num_kv_heads, self.head_dim))
+        q, k = qk.split([self.q_size, self.kv_size], dim=-1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        attn_output = self.attn(q, k, v)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add residual
+        '''
+        output, _ = self.o_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+class MLUQwen3DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: Qwen3Config,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        # By default, Qwen3 uses causal attention as it is a decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        # (e.g. Alibaba-NLP/gte-Qwen3-7B-instruct)
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        self.self_attn = MLUQwen3Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use FeedForward instead of MLP
+        '''
+        self.mlp = FeedForward(hidden_size=config.hidden_size,
+                                intermediate_size=config.intermediate_size,
+                                hidden_act='silu',
+                                up_proj_name='gate_up_proj',
+                                is_gated=True,
+                                down_proj_name='down_proj',
+                                bias=False,
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.mlp")
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                        eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: 
+            prepare to perf per-tensor sq cases if suitable;
+            bypass residual in matmul logics if fp8_block_wise quantization;
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            self.input_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.self_attn.qkv_proj)
+            self.post_attention_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.mlp.gate_up_proj)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        self.is_fp8_block_wise = isinstance(quant_config, Fp8Config) \
+            and quant_config.weight_block_size is not None
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        if self.is_fp8_block_wise:
+            # Self Attention
+            if residual is None:
+                residual = hidden_states
+                hidden_states = self.input_layernorm(hidden_states)
+            else:
+                hidden_states, residual = self.input_layernorm(
+                    hidden_states, residual)
+            hidden_states = self.self_attn(
+                positions=positions,
+                hidden_states=hidden_states,
+            )
+
+            # Fully Connected
+            hidden_states, residual = self.post_attention_layernorm(
+                hidden_states, residual)
+            hidden_states = self.mlp(hidden_states)
+            return hidden_states, residual
+
+        return decoder_layer_forward_base(
+            positions=positions,
+            hidden_states=hidden_states,
+            input_layernorm=self.input_layernorm,
+            self_attn=self.self_attn,
+            post_layernorm=self.post_attention_layernorm,
+            mlp=self.mlp,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+ALL_DECODER_LAYER_TYPES = {
+    "attention": MLUQwen3DecoderLayer,
+}
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
+        # otherwise (seq_len, ).
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class MLUQwen3Model(Qwen2Model):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config,
+                         prefix=prefix,
+                         decoder_layer_type=MLUQwen3DecoderLayer)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        '''
+        is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) \
+            and self.quant_config.weight_block_size is not None
+        if is_fp8_block_wise:
+            if get_pp_group().is_first_rank:
+                if inputs_embeds is not None:
+                    hidden_states = inputs_embeds
+                else:
+                    hidden_states = self.get_input_embeddings(input_ids)
+                residual = None
+            else:
+                assert intermediate_tensors is not None
+                hidden_states = intermediate_tensors["hidden_states"]
+                residual = intermediate_tensors["residual"]
+            for layer in self.layers[self.start_layer:self.end_layer]:
+                hidden_states, residual = layer(
+                    positions,
+                    hidden_states,
+                    residual,
+                )
+            if not get_pp_group().is_last_rank:
+                return IntermediateTensors({
+                    "hidden_states": hidden_states,
+                    "residual": residual
+                })
+            hidden_states, _ = self.norm(hidden_states, residual)
+            return hidden_states
+
+        return decoder_model_forward_base_pp(
+            input_ids=input_ids,
+            positions=positions,
+            intermediate_tensors=intermediate_tensors,
+            layers=self.layers,
+            start_layer=self.start_layer,
+            end_layer=self.end_layer,
+            get_input_embeddings=self.embed_tokens,
+            norm=self.norm,
+            inputs_embeds=inputs_embeds
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+class MLUQwen3ForCausalLM(Qwen3ForCausalLM, MLUCausalLM):
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        nn.Module.__init__(self)
+        SupportsLoRA.__init__(self)
+        SupportsPP.__init__(self)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = MLUQwen3Model(vllm_config=vllm_config,
+                                   prefix=maybe_prefix(prefix, "model"))
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: workaround for internvl3.5 multimodal models for
+          is_tie_word_embeddings config
+        '''
+        self.tie_word_embeddings = is_tie_word_embeddings(
+            vllm_config.model_config, config.tie_word_embeddings)
+
+        if get_pp_group().is_last_rank:
+            if self.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: change self.config.tie_word_embeddings to self.is_tie_word_embeddings
+        '''
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.tie_word_embeddings else None),
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return loader.load_weights(weights)

