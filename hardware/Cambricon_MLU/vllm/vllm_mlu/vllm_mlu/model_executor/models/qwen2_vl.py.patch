diff --git a/vllm_mlu/vllm_mlu/model_executor/models/qwen2_vl.py b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_vl.py
new file mode 100644
index 000000000..196f8d9b9
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/qwen2_vl.py
@@ -0,0 +1,378 @@
+from collections.abc import Iterable, Mapping, Sequence
+from functools import cached_property, partial
+from typing import (Any, Callable, Literal, Optional, Set, Tuple, TypedDict,
+                    Union)
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn import Module
+from einops import rearrange, repeat
+from transformers import BatchFeature
+from transformers.models.qwen2_vl import (Qwen2VLImageProcessor,
+                                          Qwen2VLProcessor)
+from transformers.models.qwen2_vl.configuration_qwen2_vl import (
+    Qwen2VLConfig, Qwen2VLVisionConfig)
+from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize
+
+from vllm.config import VllmConfig
+from vllm.distributed import parallel_state, tensor_model_parallel_all_gather
+from vllm.distributed import utils as dist_utils
+from vllm.logger import init_logger
+from vllm.model_executor import SamplingMetadata
+from vllm.model_executor.layers.activation import QuickGELU
+from vllm.model_executor.layers.linear import (ColumnParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.quantization.gptq import GPTQConfig
+from vllm.model_executor.layers.quantization.gptq_marlin import (
+    GPTQMarlinConfig)
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.models.module_mapping import MultiModelKeys
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.multimodal.inputs import (ImageItem, ModalityData,
+                                    MultiModalFieldConfig, MultiModalKwargs,
+                                    VideoItem)
+from vllm.multimodal.parse import (DictEmbeddingItems, ImageSize,
+                                   ModalityDataItems, MultiModalDataItems,
+                                   MultiModalDataParser)
+from vllm.multimodal.processing import (BaseMultiModalProcessor,
+                                        BaseProcessingInfo, PromptReplacement,
+                                        PromptUpdate)
+from vllm.multimodal.profiling import BaseDummyInputsBuilder, ProcessorInputs
+from vllm.platforms import _Backend
+from vllm.sequence import IntermediateTensors
+from vllm.transformers_utils.config import uses_mrope
+from vllm.transformers_utils.processor import (
+    cached_image_processor_from_config)
+
+from vllm.model_executor.models.interfaces import (
+    MultiModalEmbeddings, SupportsLoRA,
+    SupportsMultiModal, SupportsPP)
+from vllm.model_executor.models.utils import (
+    AutoWeightsLoader, WeightsMapper,
+    init_vllm_registered_model, maybe_prefix,
+    merge_multimodal_embeddings)
+from vllm.model_executor.models.vision import get_vit_attn_backend
+from vllm.model_executor.models.qwen2_vl import (
+    Qwen2VisionMLP, Qwen2VisionAttention, Qwen2VisionBlock,
+    Qwen2VisionTransformer, Qwen2VisionPatchEmbed,
+    Qwen2VisionRotaryEmbedding, Qwen2VisionPatchMerger,
+    Qwen2VLMultiModalProcessor, Qwen2VLProcessingInfo,
+    Qwen2VLDummyInputsBuilder, Qwen2VLForConditionalGeneration
+)
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.models.layer_utils import is_smoothquant
+from vllm_mlu.model_executor.models.utils import set_attn_compute_dtype
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu import _mlu_ops as mlu_ops
+
+logger = init_logger(__name__)
+
+class MLUQwen2VisionAttention(Qwen2VisionAttention):
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        cu_seqlens: torch.Tensor,
+        rotary_pos_emb: torch.Tensor,
+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention
+        seqlens: Optional[list[int]] = None,  # Only used for xFormers
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        # [s, b, c] --> [s, b, 3 * head * head_dim]
+        x, _ = self.qkv(x, smooth_quant_scale)
+
+        # [s, b, 3 * head * head_dim] -> 3 * [s, b, head, head_dim]
+        q, k, v = self.split_qkv(x)
+        batch_size = q.shape[1]
+
+        q, k, v = (rearrange(x, "s b ... -> b s ...") for x in (q, k, v))
+
+        head_dim = q.shape[-1]
+        if rotary_pos_emb is not None:
+            sin = rotary_pos_emb.sin
+            cos = rotary_pos_emb.cos
+            q = mlu_ops.rotary_embedding(
+                q, sin, cos, None, None, False, False, False, q.shape[1]
+            )
+            k = mlu_ops.rotary_embedding(
+                k, sin, cos, None, None, False, False, False, k.shape[1]
+            )
+        q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+        # fa use half for better performance
+        output = mlu_ops.flash_attention(q,
+                                         k,
+                                         v,
+                                         out=None,
+                                         cu_seq_lens_q=cu_seqlens,
+                                         cu_seq_lens_kv=cu_seqlens,
+                                         max_seq_len_q=max_seqlen,
+                                         max_seq_len_kv=max_seqlen,
+                                         alibi_slope=None,
+                                         attn_bias=None,
+                                         softmax_scale=head_dim ** -0.5,
+                                         compute_dtype=torch.half,
+                                         is_causal=False)
+        context_layer = rearrange(output, "(b s) h d -> s b (h d)", b=batch_size)
+
+        output, _ = self.proj(context_layer, residual)
+        return output
+
+class MLUQwen2VisionBlock(Qwen2VisionBlock):
+
+    def __init__(
+        self,
+        dim: int,
+        num_heads: int,
+        mlp_ratio: float,
+        act_layer: type[nn.Module] = QuickGELU,
+        norm_layer: Optional[Callable[[int], nn.Module]] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        Module.__init__(self)
+        if norm_layer is None:
+            norm_layer = partial(nn.LayerNorm, eps=1e-6)
+        self.norm1 = norm_layer(dim)
+        self.norm2 = norm_layer(dim)
+        mlp_hidden_dim = int(dim * mlp_ratio)
+
+        self.attn = MLUQwen2VisionAttention(embed_dim=dim,
+                                            num_heads=num_heads,
+                                            projection_size=dim,
+                                            quant_config=quant_config,
+                                            prefix=f"{prefix}.attn")
+        # For TMO-2464 support reasons, temporarily use gelu instead of quick_gelu
+        if is_smoothquant(quant_config):
+            hidden_act = 'gelu'
+        else:
+            hidden_act = 'quick_gelu'
+        self.mlp = FeedForward(hidden_size=dim,
+                               intermediate_size=mlp_hidden_dim,
+                               hidden_act=hidden_act,
+                               up_proj_name='fc1',
+                               is_gated=False,
+                               down_proj_name='fc2',
+                               bias=True,
+                               quant_config=quant_config,
+                               prefix=f"{prefix}.mlp")
+        # prepare to perf per-tensor sq cases if suitable;
+        # bypass residual in matmul logics if fp8_block_wise quantization;
+        self.is_per_token_sq_perf_cases = False
+
+    def forward_norm(self, norm_layer, x):
+        '''
+        forward layernorm and return sq_scale if per_token_sq
+        '''
+        if self.is_per_token_sq_perf_cases:
+            layernorm_output, smooth_quant_scale = norm_layer(x)
+        else:
+            layernorm_output = norm_layer(x)
+            smooth_quant_scale = None
+        return layernorm_output, smooth_quant_scale
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        cu_seqlens: torch.Tensor,
+        rotary_pos_emb: torch.Tensor,
+        max_seqlen: Optional[int] = None,
+        seqlens: Optional[list[int]] = None
+    ):
+        layernorm_output, smooth_quant_scale = self.forward_norm(self.norm1, x)
+        residual = x
+        attn_output = self.attn(
+                layernorm_output,
+                cu_seqlens,
+                rotary_pos_emb,
+                max_seqlen,
+                seqlens,
+                residual,
+                smooth_quant_scale)
+        layernorm_output, smooth_quant_scale = self.forward_norm(self.norm2, attn_output)
+        residual = attn_output
+        org_shape = layernorm_output.shape
+        hidden_states = self.mlp(layernorm_output, residual, smooth_quant_scale)
+        return hidden_states.view(org_shape)
+
+class MLUQwen2VisionPatchMerger(Qwen2VisionPatchMerger):
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        x = self.ln_q(x)
+        x = x.view(-1, self.hidden_size)
+
+        mlp_fc1, mlp_act, mlp_fc2 = self.mlp
+        x_parallel, _ = mlp_fc1(x)
+        x_parallel = mlu_ops.active(x_parallel, "gelu", is_gated=False)
+        out, _ = mlp_fc2(x_parallel)
+        return out
+
+class MLUQwen2VisionTransformer(Qwen2VisionTransformer):
+
+    def __init__(
+        self,
+        vision_config: Qwen2VLVisionConfig,
+        norm_eps: float = 1e-6,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        Module.__init__(self)
+
+        patch_size = vision_config.patch_size
+        temporal_patch_size = vision_config.temporal_patch_size
+        spatial_merge_size = vision_config.spatial_merge_size
+        in_channels = vision_config.in_channels
+        hidden_size = vision_config.hidden_size
+        embed_dim = vision_config.embed_dim
+        depth = vision_config.depth
+        num_heads = vision_config.num_heads
+        mlp_ratio = vision_config.mlp_ratio
+
+        self.spatial_merge_size = spatial_merge_size
+        self.num_heads = num_heads
+        self.embed_dim = embed_dim
+
+        self.patch_embed = Qwen2VisionPatchEmbed(
+            patch_size=patch_size,
+            temporal_patch_size=temporal_patch_size,
+            in_channels=in_channels,
+            embed_dim=embed_dim,
+        )
+
+        norm_layer = partial(nn.LayerNorm, eps=norm_eps)
+        head_dim = embed_dim // num_heads
+        self.rotary_pos_emb = Qwen2VisionRotaryEmbedding(head_dim // 2)
+
+        self.blocks = nn.ModuleList([
+            MLUQwen2VisionBlock(dim=embed_dim,
+                             num_heads=num_heads,
+                             mlp_ratio=mlp_ratio,
+                             norm_layer=norm_layer,
+                             quant_config=quant_config,
+                             prefix=f"{prefix}.blocks.{layer_idx}")
+            for layer_idx in range(depth)
+        ])
+        self.merger = MLUQwen2VisionPatchMerger(
+            d_model=hidden_size,
+            context_dim=embed_dim,
+            norm_layer=norm_layer,
+            quant_config=quant_config,
+            prefix=f"{prefix}.merger",
+        )
+        self.attn_backend = _Backend.FLASH_ATTN
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        grid_thw: torch.Tensor
+    ):
+        # patchify
+        x = x.to(device=self.device, dtype=self.dtype)
+        x = self.patch_embed(x)
+
+        # compute position embedding
+        rotary_pos_emb = self.rot_pos_emb(grid_thw)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        '''
+        # compute cos sin for apply_rope
+        cos = rotary_pos_emb.cos()
+        sin = rotary_pos_emb.sin()
+        cos = repeat(cos, "... d -> ... (2 d)")
+        sin = repeat(sin, "... d -> ... (2 d)")
+        rotary_pos_emb.cos = cos.to(x.dtype)
+        rotary_pos_emb.sin = sin.to(x.dtype)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        # compute cu_seqlens
+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
+                                             grid_thw[:, 0]).cumsum(
+                                                dim=0, dtype=torch.int32)
+        cu_seqlens = F.pad(cu_seqlens, (1, 0), "constant", 0)
+
+        # transformers
+        x = x.unsqueeze(1)
+
+        # pre-compute seqlens for attn mask to reduce cuMemcpy operations
+        max_seqlen, seqlens = self.compute_attn_mask_seqlen(cu_seqlens)
+        for blk in self.blocks:
+            x = blk(
+                x,
+                cu_seqlens=cu_seqlens,
+                rotary_pos_emb=rotary_pos_emb,
+                max_seqlen=max_seqlen,
+                seqlens=seqlens,
+            )
+
+        # adapter
+        x = self.merger(x)
+
+        return x
+
+
+@MULTIMODAL_REGISTRY.register_processor(Qwen2VLMultiModalProcessor,
+                                        info=Qwen2VLProcessingInfo,
+                                        dummy_inputs=Qwen2VLDummyInputsBuilder)
+class MLUQwen2VLForConditionalGeneration(Qwen2VLForConditionalGeneration, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        Module.__init__(self)
+        SupportsMultiModal.__init__(self)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        SupportsLoRA.__init__(self)
+        SupportsPP.__init__(self)
+        config: Qwen2VLConfig = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        multimodal_config = vllm_config.model_config.multimodal_config
+
+        self.config = config
+        self.multimodal_config = multimodal_config
+
+        self.visual = MLUQwen2VisionTransformer(
+            config.vision_config,
+            norm_eps=getattr(config, "rms_norm_eps", 1e-6),
+            quant_config=quant_config,
+            prefix=maybe_prefix(prefix, "visual"),
+        )
+
+        self.language_model = init_vllm_registered_model(
+            vllm_config=vllm_config,
+            prefix=maybe_prefix(prefix, "language_model"),
+            architectures=["Qwen2ForCausalLM"],
+        )
+
+        self.make_empty_intermediate_tensors = (
+            self.language_model.make_empty_intermediate_tensors)
+
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        **kwargs: object,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        # TODO: FA may standardize on half precision computation in the future
+        #       set_attn_compute_dtype might be deprecated and removed
+        set_attn_compute_dtype(torch.half)
+        return Qwen2VLForConditionalGeneration.forward(
+                self,
+                input_ids,
+                positions,
+                intermediate_tensors,
+                inputs_embeds,
+                **kwargs)
+

