diff --git a/vllm_mlu/vllm_mlu/model_executor/models/baichuan.py b/vllm_mlu/vllm_mlu/model_executor/models/baichuan.py
new file mode 100644
index 000000000..3604f0d4e
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/baichuan.py
@@ -0,0 +1,366 @@
+import torch
+from torch import nn
+from torch.nn import Module
+from typing import List, Optional, Union
+from transformers import PretrainedConfig
+from vllm.attention import Attention
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import (get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size)
+from vllm.sequence import IntermediateTensors
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.models.baichuan import (
+    _get_alibi_slopes, BaiChuanModel, BaiChuanBaseForCausalLM)
+from vllm.model_executor.models.interfaces import SupportsLoRA, SupportsPP, SupportsQuant
+from vllm.model_executor.models.utils import (
+    make_empty_intermediate_tensors_factory, make_layers)
+
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+from vllm.logger import init_logger
+
+
+logger = init_logger(__name__)
+
+
+class MLUBaiChuanAttention(nn.Module):
+    """Multi-headed attention from 'Attention Is All You Need' paper"""
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        position_embedding: str,
+        rope_theta: float = 10000,
+        max_position_embeddings: int = 8192,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.hidden_size = hidden_size
+        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size(
+        )
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tensor_model_parallel_world_size == 0
+        self.num_heads = (self.total_num_heads //
+                          tensor_model_parallel_world_size)
+        self.head_dim = hidden_size // self.total_num_heads
+        self.postion_embedding = position_embedding
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+
+        # pylint: disable=invalid-name
+        self.W_pack = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_heads,
+            bias=False,
+            quant_config=quant_config,
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+        )
+        # Create the alibi slopes and slice them.
+        if self.postion_embedding == "ALIBI":
+            tp_rank = get_tensor_model_parallel_rank()
+            head_start = tp_rank * self.num_heads
+            head_end = (tp_rank + 1) * self.num_heads
+            alibi_slopes = _get_alibi_slopes(self.total_num_heads)
+            alibi_slopes = alibi_slopes[head_start:head_end].tolist()
+
+            scaling = self.head_dim**-0.5
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: add cache_config to support kv8
+            '''
+            self.attn = Attention(self.num_heads,
+                                  self.head_dim,
+                                  scaling,
+                                  alibi_slopes=alibi_slopes,
+                                  cache_config=cache_config,
+                                  quant_config=quant_config,
+                                  prefix=f"{prefix}.attn")
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+        else:
+            self.rotary_emb = get_rope(
+                self.head_dim,
+                rotary_dim=self.head_dim,
+                max_position=self.max_position_embeddings,
+                base=self.rope_theta,
+            )
+            self.scaling = self.head_dim**-0.5
+            self.attn = Attention(self.num_heads,
+                                  self.head_dim,
+                                  self.scaling,
+                                  cache_config=cache_config,
+                                  quant_config=quant_config,
+                                  prefix=f"{prefix}.attn")
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.W_pack(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.chunk(chunks=3, dim=-1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        @brief: o_proj add residual
+        '''
+        qk, _ = qkv.split([self.num_heads * self.head_dim * 2,
+                           self.num_heads * self.head_dim], dim=-1)
+        if self.postion_embedding != "ALIBI":
+            self.rotary_emb(positions, qk.view(
+                -1, self.num_heads + self.num_heads, self.head_dim))
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+
+class MLUBaiChuanDecoderLayer(nn.Module):
+
+    def __init__(self,
+                 config: PretrainedConfig,
+                 position_embedding: str,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUBaiChuanAttention and FeedForward
+        '''
+        self.self_attn = MLUBaiChuanAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            position_embedding=position_embedding,
+            rope_theta=rope_theta,
+            max_position_embeddings=max_position_embeddings,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+        self.mlp = FeedForward(
+            hidden_size=config.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act='silu',
+            up_proj_name='gate_up_proj',
+            is_gated=True,
+            down_proj_name='down_proj',
+            bias=False,
+            quant_config=quant_config
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: prepare to perf per-tensor sq cases if suitable
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            self.input_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.self_attn.W_pack)
+            self.post_attention_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.mlp.gate_up_proj)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return decoder_layer_forward_base(
+            positions=positions,
+            hidden_states=hidden_states,
+            input_layernorm=self.input_layernorm,
+            self_attn=self.self_attn,
+            post_layernorm=self.post_attention_layernorm,
+            mlp=self.mlp,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+@support_torch_compile
+class MLUBaiChuanModel(BaiChuanModel):
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        prefix: str = "",
+        position_embedding: str = "ROPE",
+    ) -> None:
+        Module.__init__(self)
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.config = config
+        self.vocab_size = config.vocab_size
+
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+        )
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUBaiChuanDecoderLayer(config,
+                                                position_embedding,
+                                                cache_config,
+                                                quant_config,
+                                                prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        return decoder_model_forward_base_pp(
+            input_ids=input_ids,
+            positions=positions,
+            intermediate_tensors=intermediate_tensors,
+            inputs_embeds=inputs_embeds,
+            layers=self.layers,
+            start_layer=self.start_layer,
+            end_layer=self.end_layer,
+            get_input_embeddings=self.embed_tokens,
+            norm=self.norm
+        )
+
+
+class MLUBaiChuanBaseForCausalLM(BaiChuanBaseForCausalLM):
+
+    def __init__(
+        self,
+        *,
+        vllm_config: VllmConfig,
+        prefix: str = "",
+        position_embedding: str = "ROPE",
+    ):
+        Module.__init__(self)
+        SupportsLoRA.__init__(self)
+        SupportsPP.__init__(self)
+        SupportsQuant.__init__(self)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+        self.config = config
+        self.lora_config = lora_config
+        self.tp_size = get_tensor_model_parallel_world_size()
+        self.quant_config = quant_config
+        self.model = MLUBaiChuanModel(vllm_config=vllm_config,
+                                      prefix=prefix,
+                                      position_embedding=position_embedding)
+        self.lm_head = ParallelLMHead(config.vocab_size,
+                                      config.hidden_size,
+                                      quant_config=quant_config)
+        self.lm_head.weight.weight_loader = self.lm_head_weight_loader
+        if self.config.tie_word_embeddings:
+            self.lm_head.weight = self.model.embed_tokens.weight
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+
+class MLUBaichuanForCausalLM(MLUBaiChuanBaseForCausalLM):
+    """Baichuan 13B and Baichuan2 7B/13B.
+    NOTE: the class name has a lower case 'c'.
+    """
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        config = vllm_config.model_config.hf_config
+        if config.hidden_size == 4096:  # baichuan2 7b
+            super().__init__(vllm_config=vllm_config,
+                             prefix=prefix,
+                             position_embedding="ROPE")
+        else:  # baichuan 13b, baichuan2 13b
+            super().__init__(vllm_config=vllm_config,
+                             prefix=prefix,
+                             position_embedding="ALIBI")
+
+
+class MLUBaiChuanForCausalLM(MLUBaiChuanBaseForCausalLM):
+    """Baichuan 7B.
+    NOTE: the class name has an upper case 'C'.
+    """
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config,
+                         prefix=prefix,
+                         position_embedding="ROPE")
\ No newline at end of file

