diff --git a/vllm_mlu/vllm_mlu/model_executor/models/intern_vit.py b/vllm_mlu/vllm_mlu/model_executor/models/intern_vit.py
new file mode 100644
index 000000000..93290b35f
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/intern_vit.py
@@ -0,0 +1,233 @@
+from typing import Optional
+
+import torch
+import torch.nn as nn
+from transformers import PretrainedConfig
+
+from vllm.distributed import get_tensor_model_parallel_world_size
+from vllm.model_executor.models.intern_vit import (
+    InternVisionModel, InternVisionEncoder, InternVisionEncoderLayer,
+    InternParallelAttention, InternSdpaAttention, InternVisionEmbeddings)
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm_mlu import _mlu_ops as mlu_ops
+
+
+def _flash_attn(
+    q: torch.Tensor,
+    k: torch.Tensor,
+    v: torch.Tensor,
+    scale: float,
+) -> torch.Tensor:
+    """
+    flash attn for pad mode.
+
+    q shape  : [bs, seq, num_heads, head_size]
+    k shape  : [bs, seq, num_kv_heads, head_size]
+    v shape  : [bs, seq, num_kv_heads, head_size]
+    out shape: [bs, seq, num_heads, head_size]
+    """
+    sqe_len_q  = q.shape[1]
+    sqe_len_kv = k.shape[1]
+
+    out = torch.empty_like(q)
+
+    mlu_ops.flash_attention(
+        q=q,
+        k=k,
+        v=v,
+        out=out,
+        cu_seq_lens_q=None,
+        cu_seq_lens_kv=None,
+        alibi_slope=None,
+        attn_bias=None,
+        max_seq_len_q=sqe_len_q,
+        max_seq_len_kv=sqe_len_kv,
+        softmax_scale=scale,
+        is_causal=False,
+    )
+
+    return out
+
+
+class MLUInternParallelAttention(InternParallelAttention):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        *,
+        num_dummy_heads: int = 0,
+        prefix: str = "",
+    ) -> None:
+        super().__init__(config=config,
+                         quant_config=quant_config,
+                         num_dummy_heads=num_dummy_heads,
+                         prefix=prefix)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        B, N, _ = x.shape
+        qkv, _ = self.qkv(x)
+        q, k, v = qkv.chunk(3, dim=-1)
+
+        if self.qk_normalization:
+            q, k = self._apply_qk_norm(q, k)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace MHA with flash attn.
+        '''
+        q = q.view(q.shape[0], q.shape[1], -1, self.head_dim)
+        k = k.view(k.shape[0], k.shape[1], -1, self.head_dim)
+        v = v.view(v.shape[0], v.shape[1], -1, self.head_dim)
+
+        out = _flash_attn(q, k, v, self.scale)
+
+        out = out.view(B, N, -1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        out, _ = self.proj(out)
+        return out
+
+
+class MLUInternSdpaAttention(InternSdpaAttention):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        *,
+        num_dummy_heads: int = 0,
+    ) -> None:
+        super().__init__(config=config,
+                         num_dummy_heads=num_dummy_heads)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        B, N, C = x.shape
+        qkv = self.qkv(x)
+        q, k, v = qkv.chunk(3, dim=-1)
+
+        q = q.view(B, N, self.num_heads, self.head_dim)
+        k = k.view(B, N, self.num_heads, self.head_dim)
+        v = v.view(B, N, self.num_heads, self.head_dim)
+
+        if self.qk_normalization:
+            B_, N_, H_, D_ = q.shape
+            q = self.q_norm(q.flatten(-2, -1)).view(B_, N_, H_, D_)
+            k = self.k_norm(k.flatten(-2, -1)).view(B_, N_, H_, D_)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace SDPA with flash attn.
+        '''
+        # q = q.transpose(1, 2)
+        # k = k.transpose(1, 2)
+        # v = v.transpose(1, 2)
+
+        x = _flash_attn(q, k, v, self.scale)
+
+        # x = x.transpose(1, 2).reshape(B, N, -1)
+        x = x.reshape(B, N, -1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        x = self.proj(x)
+        return x
+
+
+class MLUInternVisionEncoderLayer(InternVisionEncoderLayer):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        *,
+        num_dummy_heads: int = 0,
+        prefix: str = "",
+    ) -> None:
+        super().__init__(config=config,
+                         quant_config=quant_config,
+                         num_dummy_heads=num_dummy_heads,
+                         prefix=prefix)
+
+    def _init_attn(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig],
+        *,
+        num_dummy_heads: int,
+        prefix: str = "",
+    ):
+        # fallback to sdpa attention if tp unavailable
+        tp_size = get_tensor_model_parallel_world_size()
+        num_heads = config.num_attention_heads
+
+        if (num_heads + num_dummy_heads) % tp_size == 0:
+            return MLUInternParallelAttention(config,
+                                              quant_config=quant_config,
+                                              num_dummy_heads=num_dummy_heads,
+                                              prefix=prefix)
+
+        return MLUInternSdpaAttention(config, num_dummy_heads=num_dummy_heads)
+
+
+class MLUInternVisionEncoder(InternVisionEncoder):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        *,
+        num_hidden_layers_override: Optional[int] = None,
+        num_dummy_heads: int = 0,
+        prefix: str = "",
+    ):
+        nn.Module.__init__(self)
+
+        self.config = config
+
+        if num_hidden_layers_override is None:
+            num_hidden_layers = config.num_hidden_layers
+        else:
+            num_hidden_layers = num_hidden_layers_override
+
+        self.layers = nn.ModuleList([
+            MLUInternVisionEncoderLayer(config,
+                                        quant_config,
+                                        num_dummy_heads=num_dummy_heads,
+                                        prefix=f"{prefix}.layers.{layer_idx}")
+            for layer_idx in range(num_hidden_layers)
+        ])
+
+
+class MLUInternVisionModel(InternVisionModel):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        *,
+        num_hidden_layers_override: Optional[int] = None,
+        num_dummy_heads: int = 0,
+        prefix: str = "",
+    ) -> None:
+        nn.Module.__init__(self)
+
+        self.config = config
+
+        self.embeddings = InternVisionEmbeddings(config)
+        self.encoder = MLUInternVisionEncoder(
+            config=config,
+            quant_config=quant_config,
+            num_hidden_layers_override=num_hidden_layers_override,
+            num_dummy_heads=num_dummy_heads,
+            prefix=f"{prefix}.encoder",
+        )

