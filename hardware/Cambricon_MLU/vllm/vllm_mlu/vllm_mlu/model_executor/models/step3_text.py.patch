diff --git a/vllm_mlu/vllm_mlu/model_executor/models/step3_text.py b/vllm_mlu/vllm_mlu/model_executor/models/step3_text.py
new file mode 100644
index 000000000..5b1a0c8dc
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/step3_text.py
@@ -0,0 +1,528 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+"""Inference-only Jurassic model."""
+from collections.abc import Iterable
+from typing import Any, Optional
+
+import torch
+from torch.nn import Module
+
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, ModelConfig, VllmConfig, get_current_vllm_config
+from vllm.distributed import (get_pp_group, get_tp_world_group,
+                              tensor_model_parallel_all_reduce)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.layernorm import RMSNorm
+
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig)
+from vllm.model_executor.layers.sampler import get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+
+from vllm.model_executor.models.interfaces import SupportsPP
+from vllm.model_executor.models.utils import (PPMissingLayer, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers)
+
+from vllm.model_executor.models.step3_text import (FusedMoEBlock, Step3TextMLP, Step3TextModel,
+                                                   Step3TextAttention, Step3TextDecoderLayer,
+                                                   Step3TextForCausalLM)
+
+from vllm_mlu.model_executor.models.dp_utils import enable_data_parallel
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp, MoeGroupInfo
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+
+logger = init_logger(__name__)
+
+
+'''
+=============================
+Modify by vllm_mlu
+=============================
+@brief: Use SparseMoeMlp as base class 
+    to replace FusedMoE for ep
+'''
+class MLUStep3MoeSparseMoeBlock(SparseMoeMlp):
+
+    def __init__(self,
+                 config: ModelConfig,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = ""):
+        tp_group = get_tp_world_group() if enable_data_parallel() else None
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        super().__init__(num_experts=config.moe_num_experts,
+                         top_k=config.moe_top_k,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True,
+                         tp_group=tp_group,
+                         scoring_func="sigmoid")
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        orig_shape = hidden_states.shape
+        hidden_dim = hidden_states.shape[-1]
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.forward_experts(hidden_states, router_logits)
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+
+        return final_hidden_states.view(orig_shape)
+'''
+==================
+End of MLU Hijack
+==================
+'''
+
+
+class MLUStep3TextAttention(Step3TextAttention):
+
+    def forward(self, positions: torch.Tensor,
+                hidden_states: torch.Tensor) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q = self.inter_norm(q)
+        q = self.wq(q)[0]
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use concat qk on RoPE
+        '''
+        qk = torch.cat([q, k], dim=-1)
+
+        self.rotary_emb(positions, qk.view(
+            -1, self.num_heads + self.num_kv_heads, self.head_dim))
+
+        q, k = qk.split([q.shape[-1], self.kv_size], dim=-1)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        attn_output = self.attn(q, k, v)
+        residual, _ = self.o_proj(attn_output)
+        return residual
+
+
+class MLUStep3TextDecoderLayer(Step3TextDecoderLayer):
+
+    def __init__(self,
+                 config: ModelConfig,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 prefix: str = "") -> None:
+        Module.__init__(self)
+        config = config.hf_config
+        self.hidden_size = config.hidden_size
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use hijack attention
+        '''
+        self.self_attn = MLUStep3TextAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=1,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            norm_eps=config.rms_norm_eps,
+            max_position_embedding=config.max_position_embedding,
+            head_dim=config.head_dim,
+            share_q_dim=config.share_q_dim,
+            rope_theta=config.rope_theta,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn")
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        layer_idx = int(prefix.split("layers.")[1].split(".")[0])
+        moe_layers_enum = getattr(config, "moe_layers_enum", None)
+        if moe_layers_enum is not None:
+            moe_layers_idx = [
+                int(i) for i in moe_layers_enum.strip().split(',')
+            ]
+        else:
+            # Default to 1dense.
+            moe_layers_idx = [i for i in range(1, config.num_hidden_layers)]
+
+        if layer_idx in moe_layers_idx:
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: use sparse_moe when use ep
+            '''
+            moe_group_info = MoeGroupInfo()
+            vllm_config = get_current_vllm_config()
+            use_ep = (vllm_config.parallel_config.enable_expert_parallel
+                    and moe_group_info.tp_size * moe_group_info.dp_size > 1)
+            if use_ep:
+                self.moe = MLUStep3MoeSparseMoeBlock(config=config,
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.moe")
+            else:
+                self.moe = FusedMoEBlock(config=config,
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.moe")
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+            self.share_expert = Step3TextMLP(
+                hidden_size=self.hidden_size,
+                intermediate_size=config.share_expert_dim,
+                hidden_act="silu",
+                quant_config=quant_config,
+                prefix=f"{prefix}.share_expert")
+            self.use_moe = True
+        else:
+            self.mlp = Step3TextMLP(hidden_size=config.hidden_size,
+                                    intermediate_size=config.intermediate_size,
+                                    hidden_act="silu",
+                                    quant_config=quant_config,
+                                    prefix=f"{prefix}.mlp")
+            self.use_moe = False
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+
+@support_torch_compile
+class MLUStep3TextModel(Step3TextModel):
+
+    def __init__(self, vllm_config: VllmConfig, prefix: str = "") -> None:
+        Module.__init__(self)
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        self.vocab_size = config.vocab_size
+        self.config = config
+
+        if get_pp_group().is_first_rank or (config.tie_word_embeddings
+                                            and get_pp_group().is_last_rank):
+            self.embed_tokens = VocabParallelEmbedding(
+                self.vocab_size,
+                config.hidden_size,
+            )
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use hijack decoder layer
+        '''
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUStep3TextDecoderLayer(config=vllm_config.
+                                                 model_config,
+                                                 cache_config=cache_config,
+                                                 quant_config=quant_config,
+                                                 prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+    
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(["hidden_states"],
+                                                    config.hidden_size))
+
+
+class MLUStep3TextForCausalLM(Step3TextForCausalLM, MLUCausalLM):
+
+    def __init__(
+        self,
+        *,
+        vllm_config: VllmConfig,
+        prefix: str = "",
+    ):
+        Module.__init__(self)
+        SupportsPP.__init__(self)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        config = vllm_config.model_config.hf_config
+        lora_config = vllm_config.lora_config
+        self.config = config
+        self.vllm_config = vllm_config
+
+        self.model = MLUStep3TextModel(vllm_config=vllm_config, prefix=prefix)
+
+        if get_pp_group().is_last_rank:
+            self.unpadded_vocab_size = config.vocab_size
+            if lora_config:
+                self.unpadded_vocab_size += lora_config.lora_extra_vocab_size
+            self.lm_head = ParallelLMHead(
+                self.unpadded_vocab_size,
+                config.hidden_size,
+                org_num_embeddings=config.vocab_size,
+                padding_size=DEFAULT_VOCAB_PADDING_SIZE
+                if not lora_config else lora_config.lora_vocab_padding_size,
+            )
+            self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,
+                                                    config.vocab_size)
+            self.sampler = get_sampler()
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def load_weights(self, weights: Iterable[tuple[str,
+                                                   torch.Tensor]]) -> set[str]:
+        qkv_params_mapping = [
+            # (param_name, shard_name, relative_start_idx, relative_end_idx)
+            (".qkv_proj", ".q_proj", 0, self.config.share_q_dim /
+             (self.config.share_q_dim + self.config.head_dim * 2)),
+            (".qkv_proj", ".k_proj", self.config.share_q_dim /
+             (self.config.share_q_dim + self.config.head_dim * 2),
+             (self.config.share_q_dim + self.config.head_dim) /
+             (self.config.share_q_dim + self.config.head_dim * 2)),
+            (".qkv_proj", ".v_proj",
+             (self.config.share_q_dim + self.config.head_dim) /
+             (self.config.share_q_dim + self.config.head_dim * 2),
+             (self.config.share_q_dim + self.config.head_dim * 2) /
+             (self.config.share_q_dim + self.config.head_dim * 2)),
+        ]
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            (".gate_up_proj", ".gate_proj", 0),
+            (".gate_up_proj", ".up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: 1 add pack_params before loading weights
+            2 use different params_dict when use ep
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params()
+
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        moe_ep_rank = moe_group_info.moe_ep_rank
+        num_total_experts = self.vllm_config.model_config.hf_config.moe_num_experts
+        num_experts_per_rank = (num_total_experts + moe_ep_size - 1) // moe_ep_size
+        use_ep = (self.vllm_config.parallel_config.enable_expert_parallel
+                and moe_group_info.tp_size * moe_group_info.dp_size > 1)
+
+        expert_params_mapping = []
+        if use_ep:
+            expert_params_mapping = [
+                (".moe.experts.gate_up_proj.weight", ".moe.gate_proj.weight", 0),
+                (".moe.experts.gate_up_proj.weight", ".moe.up_proj.weight", 1),
+                (".moe.experts.down_proj.weight", ".moe.down_proj.weight", None)
+            ]
+        else:
+            expert_params_mapping = [
+            (".moe.experts.w13_weight", ".moe.gate_proj.weight", "w1"),
+            (".moe.experts.w13_weight", ".moe.up_proj.weight", "w3"),
+            (".moe.experts.w2_weight", ".moe.down_proj.weight", "w2")
+        ]
+
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        disable_moe_stacked_params = [
+            data[1] for data in expert_params_mapping
+        ]
+
+        for name, loaded_weight in weights:
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                if any(disable_moe_stacked_param in name
+                    for disable_moe_stacked_param in
+                    disable_moe_stacked_params):
+                    continue
+                name = name.replace(weight_name, param_name)
+                if is_pp_missing_parameter(name, self):
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: use hf "language_model" prefix in ep
+                '''
+                if use_ep:
+                    param = params_dict[name.replace("language_model.", "")]
+                else:
+                    param = params_dict[name]
+                '''
+                =============================
+                End of MLU Hijack
+                =============================
+                '''
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                loaded_params.add(name)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: 1 use hf "language_model" prefix in ep
+                        2 mapping expert_id to local expert id in ep_rank
+                    '''
+                    if use_ep:
+                        if moe_ep_rank + 1 == moe_ep_size and num_total_experts % moe_ep_size:
+                            num_experts_per_rank = num_total_experts % moe_ep_size
+
+                        # Handle expert parallel, use residual expert id as local expert id in ep_rank
+                        start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+                        end_expert_id = start_expert_id + num_experts_per_rank
+
+                        for expert_id in range(start_expert_id, end_expert_id):
+                            residual_expert_id = expert_id % num_experts_per_rank
+                            rename = name.replace("moe.experts", f"moe.experts.{residual_expert_id}").\
+                                replace("language_model.", "")
+                            param = params_dict[rename]
+
+                            if "gate" in name or "up" in name:
+                                weight_loader = param.weight_loader
+                                weight_loader(param,
+                                            loaded_weight[expert_id],
+                                            loaded_shard_id=shard_id)
+                            else:
+                                weight_loader = param.weight_loader
+                                weight_loader(param, loaded_weight[expert_id])
+                            rename = rename.replace("model", "language_model.model")
+                            loaded_params.add(rename)
+                        break
+                    else:
+                        param = params_dict[name]
+                        weight_loader = param.weight_loader
+                        for expert_id in range(loaded_weight.shape[0]):
+                            loaded_weight_expert = loaded_weight[expert_id]
+                            weight_loader(param,
+                                        loaded_weight_expert,
+                                        name,
+                                        shard_id=shard_id,
+                                        expert_id=expert_id)
+                        loaded_params.add(name)
+                        break
+                    '''
+                    =============================
+                    End of MLU Hijack
+                    =============================
+                    '''
+                else:
+                    for (param_name, weight_name, start_idx,
+                        end_idx) in qkv_params_mapping:
+                        if weight_name not in name:
+                            continue
+                        name = name.replace(weight_name, param_name)
+                        if is_pp_missing_parameter(name, self):
+                            continue
+                        '''
+                        =============================
+                        Modify by vllm_mlu
+                        =============================
+                        @brief: use hf "language_model" prefix in ep
+                        '''
+                        if use_ep:
+                            param = params_dict[name.replace("language_model.", "")]
+                        else:
+                            param = params_dict[name]
+                        '''
+                        =============================
+                        End of MLU Hijack
+                        =============================
+                        '''
+                        dim = param.shape[param.output_dim]
+                        begin_idx = int(start_idx * dim)
+                        end_idx = int(end_idx * dim)
+                        param_slice = param.narrow(param.output_dim, begin_idx,
+                                                end_idx - begin_idx)
+                        param_slice.copy_(loaded_weight)
+                        loaded_params.add(name)
+                        break
+                    else:
+                        if is_pp_missing_parameter(name, self):
+                            continue
+                        '''
+                        =============================
+                        Modify by vllm_mlu
+                        =============================
+                        @brief: use hf "language_model" prefix in ep
+                        '''
+                        if use_ep:
+                            param = params_dict[name.replace("language_model.", "")]
+                        else:
+                            param = params_dict[name]
+                        '''
+                        =============================
+                        End of MLU Hijack
+                        =============================
+                        '''
+                        weight_loader = getattr(param, "weight_loader",
+                                                default_weight_loader)
+                        weight_loader(param, loaded_weight)
+                        loaded_params.add(name)
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: add pack_params_after_loading
+        '''
+        for name, m in self.model.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params_after_loading()  
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return loaded_params

