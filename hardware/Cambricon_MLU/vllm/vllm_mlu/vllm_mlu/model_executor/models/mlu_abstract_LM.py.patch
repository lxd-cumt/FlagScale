diff --git a/vllm_mlu/vllm_mlu/model_executor/models/mlu_abstract_LM.py b/vllm_mlu/vllm_mlu/model_executor/models/mlu_abstract_LM.py
new file mode 100644
index 000000000..911018cbd
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/mlu_abstract_LM.py
@@ -0,0 +1,502 @@
+import os
+from vllm.logger import init_logger
+from vllm.config import CacheConfig, VllmConfig
+from vllm_mlu.mlu_hijack_utils import get_is_gated, TypedDict
+import json
+from vllm.transformers_utils.config import get_config
+from vllm.entrypoints.llm import LLM
+from vllm_mlu._mlu_utils import VLLM_DUMP_MLU_INFO_EN, VLLM_DUMP_MLU_INFO_DEBUG
+from vllm.distributed import get_pp_group, get_tp_group
+logger = init_logger(__name__)
+data_type_byte_width_map = {
+                            "int8":1,
+                            "int4":0.5,
+                            "float16":2,
+                            "float32":4,
+                            "int32":4,
+                            "bfloat16":2,
+                            "fp8_e4m3":1,
+                            "fp8_e4m3fn":1
+}
+
+HFUInfo = {
+    "context_hfu":0.0,
+    "decoder_hfu":0.0
+    }
+FlopsInfo = {
+    "context_flops":0.0,
+    "decoder_flops":0.0
+    }
+
+class MLUCausalLM:
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        self.tensor_parallel_size = vllm_config.parallel_config.tensor_parallel_size
+        self.data_parallel_size = vllm_config.parallel_config.data_parallel_size
+        self.parallel_size = self.tensor_parallel_size * self.data_parallel_size
+        self.mlu_dtype = str(vllm_config.model_config.dtype).split(".")[-1]
+        self.mlu_dtype = "float16" if self.mlu_dtype == "auto" else self.mlu_dtype
+        self.kv_cache_dtype = self.mlu_dtype if vllm_config.cache_config.cache_dtype == "auto" else vllm_config.cache_config.cache_dtype
+        self.quantization = vllm_config.model_config.quantization
+        self.model_config = get_config(vllm_config.model_config.model, vllm_config.model_config.trust_remote_code)
+        self.hfu_info = None
+        self.flops_info = None
+        self.hfu_model_config = TypedDict
+        self.io_efficiency = 0
+        self.model_path = vllm_config.model_config.model
+        self.trust_remote_code = vllm_config.model_config.trust_remote_code
+        if self.quantization != None:
+            config = None
+            if os.path.exists(self.model_path + "/quantize_config.json"):
+                with open(self.model_path + "/quantize_config.json", 'r') as file:
+                    config = json.load(file)
+            elif os.path.exists(self.model_path + "/config.json"):
+                with open(self.model_path + "/config.json", 'r') as file:
+                    config = json.load(file).get("quantization_config", None)
+            if config is not None:
+                quant_mode = config.get("quant_mode", None)
+                self.smooth_quant_type = "SmoothQuant" if quant_mode == "SmoothQuant" else "invalid"
+                if self.quantization == 'fp8':
+                    self.filter_dtype = f"fp8_{config.get('fmt', 'e4m3')}"
+                else:
+                    self.filter_dtype = f"int{config.get('bits', 8)}"
+        else:
+            self.smooth_quant_type = "invalid"
+            self.filter_dtype = self.mlu_dtype
+        self.get_special_config()
+        if VLLM_DUMP_MLU_INFO_DEBUG:
+            self.print_model_info()
+
+    def print_model_info(self):
+        logger.info(f"************ MODEL_CONFIG_LIST ********************")
+        logger.info(f"hidden_size                {self.get_hidden_size()}")
+        logger.info(f"vocab_size                 {self.get_vocab_size()}")
+        logger.info(f"cla_coeffient              {self.get_cla_coefficient()}")
+        logger.info(f"dtype                      {self.get_dtype()}")
+        logger.info(f"quant_type                 {self.get_quant_type()}")
+        logger.info(f"is_gated                   {self.get_is_gated()}")
+
+        logger.info(f"************ ffn moe layer info ********************")
+        logger.info(f"ffn_inner_size             {self.get_ffn_inner_size()}")
+        logger.info(f"moe_inner_size             {self.get_moe_inner_size()}")
+        logger.info(f"layer_num                  {self.get_layer_num()}")
+        logger.info(f"ffn_num                    {self.get_ffn_num()}")
+        logger.info(f"moe_num                    {self.get_moe_num()}")
+        logger.info(f"topk_num                   {self.get_topk_num()}")
+        logger.info(f"experts_num                {self.get_experts_num()}")
+        logger.info(f"shared_experts_num         {self.get_shared_experts_num()}")
+        logger.info(f"shared_experts_inner_size  {self.get_shared_experts_inner_size()}")
+
+        logger.info(f"************ attn layer info ********************")
+        logger.info(f"num_attn_heads             {self.get_num_attn_heads()}")
+        logger.info(f"num_kv_heads               {self.get_num_kv_heads()}")
+        logger.info(f"kv_cache_dtype             {self.get_kv_cache_dtype()}")
+        logger.info(f"head_size                  {self.get_head_size()}")
+        logger.info(f"v_head_size                {self.get_v_head_size()}")
+        logger.info(f"check if model config info match with 'path_to_model/config.json' ********************")
+
+    def get_special_config(self):
+        return
+
+    def get_decoder_io_efficiency(self, batch_size, input_len, output_len):
+        try:
+            from device_info import get_device_attribute
+            model_param = self.get_model_param(batch_size)
+            model_byte_width = data_type_byte_width_map[self.filter_dtype]
+            param_scale = model_param * model_byte_width
+            kv_cache = self.get_kv_cache(batch_size, input_len, output_len)
+            unit = 1000.0
+            _, _, _, bandwidth = get_device_attribute()
+            memory_MB = (param_scale + kv_cache) / unit / unit
+            self.io_efficiency = memory_MB / bandwidth * unit / self.parallel_size
+        except Exception:
+            logger.info("Unsupport io_efficiency get_decoder_io_efficiency function, this will turn to error performance info.")
+
+    def collect_hfu_io_effciency_info(self, batch_size, input_len, output_len):
+        if VLLM_DUMP_MLU_INFO_EN:
+            self.hfu_info = HFUInfo
+            self.get_flops_(True, batch_size, input_len, output_len)
+            self.get_decoder_io_efficiency(batch_size, input_len, output_len)
+        else:
+            self.flops_info = FlopsInfo
+            self.get_flops_(False, batch_size, input_len, output_len)
+
+    def has_information_dump(self):
+        if VLLM_DUMP_MLU_INFO_EN:
+            try:
+                import device_info
+                return True
+            except:
+                return False
+        return False
+
+    def get_flops_(self, get_hfu, batch_size, input_len, output_len):
+        total_peak_compute = [1.0, 1.0, 1.0]
+        if get_hfu:
+            try:
+                from device_info import get_hfu_peak_flops
+                each_peak_compute = [1.0, 1.0, 1.0]
+                get_hfu_peak_flops(self.mlu_dtype, self.filter_dtype, each_peak_compute)
+                tmp = [f"{num:.5e}" for num in each_peak_compute]
+                total_peak_compute = each_peak_compute
+                for i in range(len(total_peak_compute)):
+                    total_peak_compute[i] = self.parallel_size * each_peak_compute[i] + 1e-6
+            except Exception:
+                logger.info("Unsupport io_efficiency get_decoder_io_efficiency function, this will turn to error performance info.")
+        if self.get_kv_cache_dtype() != "int8":
+            attn_qkv_peak_compute     = total_peak_compute[1]
+        else:
+            attn_qkv_peak_compute     = total_peak_compute[0]
+        if self.smooth_quant_type == "invalid":
+           moe_peak_compute           = total_peak_compute[1]
+           attn_pre_post_peak_compute = total_peak_compute[1]
+        else:
+           moe_peak_compute           = total_peak_compute[2]
+           attn_pre_post_peak_compute = total_peak_compute[2]
+
+
+        layer_attn_num = self.get_layer_num()
+        layer_ffn_num  = self.get_ffn_num()
+        layer_moe_num  = self.get_moe_num()
+        context_flops  = layer_attn_num * self.get_context_attn_flops(batch_size, input_len, attn_qkv_peak_compute, attn_pre_post_peak_compute) +\
+                         layer_ffn_num  * self.get_context_ffn_flops(batch_size, input_len, moe_peak_compute)  +\
+                         layer_moe_num  * self.get_context_moe_flops(batch_size, input_len, moe_peak_compute) + \
+                         1              * self.get_context_others_flops(batch_size, input_len, total_peak_compute[1])
+        decoder_flops  = layer_attn_num * self.get_decoder_attn_flops(batch_size, input_len, output_len, attn_qkv_peak_compute, attn_pre_post_peak_compute) +\
+                         layer_ffn_num  * self.get_decoder_ffn_flops(batch_size, output_len, moe_peak_compute)  +\
+                         layer_moe_num  * self.get_decoder_moe_flops(batch_size, output_len, moe_peak_compute)  +\
+                         1              * self.get_decoder_others_flops(batch_size, output_len, total_peak_compute[1])
+        if get_hfu:
+            self.hfu_info["context_hfu"] = context_flops
+            self.hfu_info["decoder_hfu"] = decoder_flops 
+        else:
+            self.flops_info["context_flops"] = context_flops
+            self.flops_info["decoder_flops"] = decoder_flops 
+        return
+
+    def get_hidden_size(self):
+        if hasattr(self.model_config, "hidden_size"):
+            return self.model_config.hidden_size
+        elif hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "d_model"):
+            return self.model_config.audio_config.d_model
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "hidden_size"):
+            return self.model_config.text_config.hidden_size
+        else:
+            logger.error("The model's config.json does not contain hidden_size or audio_config.d_model.")
+            return
+    def get_vocab_size(self):
+        if hasattr(self.model_config, "vocab_size"):
+            return self.model_config.vocab_size
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "vocab_size"):
+            return self.model_config.text_config.vocab_size
+        else:
+            logger.error("The model's config.json does not contain vocab_size.")
+            return
+    def get_cla_coefficient(self):
+        return 1.0
+    def is_causal(self):
+        if hasattr(self.model_config, "is_causal"):
+            return self.model_config.is_causal
+        return True
+
+    def get_ffn_inner_size(self):
+        possible_keys_ffn_size = [
+            # chatglm3-6b-32k
+            "ffn_hidden_size",
+            # llama3-8b-hf
+            "intermediate_size",
+        ]
+        ffn_size=None
+        for key in possible_keys_ffn_size:
+            ffn_size = getattr(self.model_config, key, None)
+            if ffn_size is not None:
+                break
+        if ffn_size is None:
+            logger.warning("The model's config.json does not contain any of the following"
+                        "keys to determine the ffn_size or moe_size: "
+                        f"{possible_keys_ffn_size}. ")
+        return 0 if ffn_size is None else ffn_size
+    
+    def get_moe_inner_size(self):
+        moe_size=None
+        if getattr(self.model_config, "moe_intermediate_size", None):
+            moe_size = getattr(self.model_config, "moe_intermediate_size", None)
+        elif hasattr(self.model_config, "text_config") and getattr(self.model_config.text_config, "moe_intermediate_size", None):
+            moe_size = getattr(self.model_config.text_config, "moe_intermediate_size", None)
+        return   0 if moe_size is None else moe_size
+
+    def get_num_kv_heads(self):
+        possible_kv_heads = [
+            # chatglm3-6b-32k
+            "multi_query_group_num",
+            # llama3-8b-hf
+            "num_key_value_heads",
+            # falcon-180B-chat
+            "num_kv_heads",
+        ]
+        for key in possible_kv_heads:
+            kv_heads = getattr(self.model_config, key, None)
+            if kv_heads is not None:
+                break
+
+        if kv_heads is None:
+            logger.warning("The model's config.json does not contain any of the following"
+                        "keys to determine the kv_heads: "
+                        f"{possible_kv_heads}, use num_attention_heads to replace")
+            kv_heads = self.get_num_attn_heads()
+        return kv_heads
+
+    def get_kv_cache(self, batch_size, input_len, output_len):
+        batch          = batch_size
+        head_num_kv    = self.get_num_kv_heads()
+        head_size      = self.get_head_size()
+        tp_num         = self.tensor_parallel_size 
+        layer_num      = self.get_layer_num()
+        input_seq_len  = input_len
+        output_seq_len = output_len
+        kv_cache_byte_width = data_type_byte_width_map[self.get_kv_cache_dtype()]
+        cla_coeffient  = self.get_cla_coefficient()
+        kv_cache = cla_coeffient * batch * max(head_num_kv, tp_num) * head_size * (input_seq_len + output_seq_len / 2) * 2 * layer_num * kv_cache_byte_width
+        return kv_cache
+
+    def get_num_attn_heads(self):
+        possible_num_attention_heads = [
+            "num_attention_heads",
+            "n_heads",
+        ]
+        for key in possible_num_attention_heads:
+            num_attention_heads = getattr(self.model_config, key, None)
+            if num_attention_heads is not None:
+                break
+            
+        if num_attention_heads is None:
+            if hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "encoder_attention_heads"):
+                num_attention_heads = self.model_config.audio_config.encoder_attention_heads
+            elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "num_attention_heads"):
+                num_attention_heads = self.model_config.text_config.num_attention_heads
+            else:
+                logger.error("The model's config.json does not contain any of the following"
+                            "keys to determine the num_attention_heads: "
+                            f"{possible_num_attention_heads}. ")
+        return num_attention_heads
+
+    def get_ffn_num(self):
+        return self.get_layer_num()
+    def get_moe_num(self):
+        return self.get_layer_num()
+
+    def get_layer_num(self):
+        if getattr(self.model_config, "num_hidden_layers", None):
+            num_hidden_layers = getattr(self.model_config, "num_hidden_layers", None)
+        elif hasattr(self.model_config, "audio_config") and hasattr(self.model_config.audio_config, "encoder_layers"):
+            num_hidden_layers = self.model_config.audio_config.encoder_layers
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "num_hidden_layers"):
+            num_hidden_layers = self.model_config.text_config.num_hidden_layers
+        else:
+            logger.error("The model's config.json does not contain num_hidden_layers or audio_config.encoder_layers.")
+        return num_hidden_layers
+
+    def get_head_size(self):
+        return self.get_hidden_size() / self.get_num_attn_heads()
+
+    def get_v_head_size(self):
+        if hasattr(self.model_config, "v_head_dim") and self.model_config.v_head_dim is not None:
+            return self.model_config.v_head_dim
+        return self.get_head_size()
+
+    def get_shared_experts_num(self):
+        return 1
+
+    def get_shared_experts_inner_size(self):
+        # shared_expert_intermediate_size defined in qwen2_moe
+        if hasattr(self.model_config, "shared_expert_intermediate_size") and self.model_config.shared_expert_intermediate_size is not None:
+            return self.model_config.shared_expert_intermediate_size
+        else:
+            return 0
+
+    def get_is_gated(self):
+        return get_is_gated()
+
+    def get_topk_num(self):
+        if hasattr(self.model_config, "num_experts_per_tok"):
+            if self.model_config.num_experts_per_tok == None:
+                return 0
+            return self.model_config.num_experts_per_tok
+        else:
+            return 0
+
+    def get_experts_num(self):
+        if hasattr(self.model_config, "num_experts") and self.model_config.num_experts is not None:
+            return  self.model_config.num_experts
+        elif hasattr(self.model_config, "num_local_experts"):
+            return self.model_config.num_local_experts
+        elif hasattr(self.model_config, "n_routed_experts"):
+            return self.model_config.n_routed_experts
+        else:
+            return 0
+
+    def get_dtype(self):
+        return self.mlu_dtype
+
+    def get_quant_type(self):
+        if self.quantization == None:
+            return self.get_dtype()
+        try:
+            with open(self.model_path + "/quantize_config.json", 'r') as file:
+                config = json.load(file)
+            if config["quant_mode"] == "SmoothQuant":
+                return b"SmoothQuant"
+            else:
+                return "invalid"
+            return "fp8" if self.quantization == 'fp8' else f"int{config['bits']}"
+        except Exception as e:
+            raise RuntimeError(f"Failed to get HFU:quant_type info: {str(e)}")
+
+    def get_kv_cache_dtype(self):
+        return self.kv_cache_dtype
+
+    def get_context_others_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        # 存在优化导致seq实际按1计算，因此为 IO 瓶颈，context阶段占比较小 忽略相关计算量。
+        seq_len = 1
+        hidden_size = self.get_hidden_size()
+        voc_size = self.get_vocab_size()
+        context_lm_head = 2 * batch_size * seq_len * hidden_size * voc_size / hfu_peak_compute
+        return context_lm_head
+
+    def get_context_ffn_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        seq_len = input_len
+        hidden_size = self.get_hidden_size()
+        ffn_size = self.get_ffn_inner_size()
+        model_byte_width = data_type_byte_width_map[self.filter_dtype]
+        coeffient = 6 if self.get_is_gated() else 4
+        context_ffn = coeffient * batch_size * seq_len * hidden_size * ffn_size / hfu_peak_compute
+        if VLLM_DUMP_MLU_INFO_DEBUG:
+            logger.info(f"context_ffn estimated time is : {context_ffn}")
+        return context_ffn
+
+    def get_context_moe_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        seq_len = input_len
+        hidden_size = self.get_hidden_size()
+        coeffient = 6 if self.get_is_gated() else 4
+        moe_size = self.get_moe_inner_size()
+        shared_expert_intermediate_size = self.get_shared_experts_inner_size()
+        topk=self.get_topk_num()
+        expert_num = self.get_experts_num()
+        if self.smooth_quant_type == "invalid":
+            context_gate = batch_size * seq_len * hidden_size * 2 * self.get_experts_num() * self.tensor_parallel_size / hfu_peak_compute
+        else:
+            context_gate = batch_size * seq_len * hidden_size * 2 * 2 * self.get_experts_num() * self.tensor_parallel_size / hfu_peak_compute
+        context_shared_expert = batch_size * seq_len * hidden_size * coeffient * shared_expert_intermediate_size / hfu_peak_compute
+        context_moe = batch_size * seq_len * hidden_size * coeffient * moe_size * self.get_topk_num() / hfu_peak_compute
+        if VLLM_DUMP_MLU_INFO_DEBUG:
+            logger.info(f"context_moe groupgemm estimated time is : {context_moe}")
+            logger.info(f"context_moe gate estimated time is : {context_gate}")
+            logger.info(f"context_moe shared_expert estimated time is : {context_shared_expert}")
+        return context_moe + context_gate + context_shared_expert
+
+    def get_context_attn_flops(self, batch_size, input_len, hfu_qk_peak_compute=1.0, hfu_peak_compute=1.0):
+        seq_len = input_len
+        hidden_size = self.get_hidden_size()
+        head_num       = self.get_num_attn_heads()
+        head_num_kv    = self.get_num_kv_heads()
+        head_size      = self.get_head_size()
+        v_head_size      = self.get_v_head_size()
+        bsh2 = batch_size * seq_len * hidden_size * hidden_size
+        cla_coeffient = self.get_cla_coefficient()
+        is_causal = 0.5 if self.is_causal() else 1.0
+        context_atn_pre = (2 * batch_size * seq_len * (head_num * head_size + 2 * head_num_kv * v_head_size) * hidden_size * cla_coeffient) / hfu_peak_compute
+        context_atn_qk = 2 * is_causal * batch_size * seq_len * seq_len * head_num * head_size / hfu_qk_peak_compute
+        context_atn_qkv = 2 * is_causal * batch_size * seq_len * seq_len * head_num * v_head_size / hfu_qk_peak_compute
+        context_atn_post = 2 * batch_size * seq_len * head_num * head_size * hidden_size / hfu_peak_compute
+        if VLLM_DUMP_MLU_INFO_DEBUG:
+            logger.info(f"context_self_attn FlashAttn estimated time is : {context_atn_qk + context_atn_qkv}")
+            logger.info(f"context_self_attn pre linear estimated time is : {context_atn_pre}")
+            logger.info(f"context_self_attn post linear estimated time is : {context_atn_post}")
+        ret = (context_atn_qk + context_atn_qkv + context_atn_pre + context_atn_post)
+        return ret
+
+    def get_decoder_others_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        seq_len = 1
+        hidden_size = self.get_hidden_size()
+        voc_size = self.get_vocab_size()
+        context_lm_head = 2 * batch_size * seq_len * hidden_size * voc_size / hfu_peak_compute
+        return context_lm_head
+
+    def get_decoder_ffn_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        seq_len = 1
+        hidden_size = self.get_hidden_size()
+        ffn_size = self.get_ffn_inner_size()
+        model_byte_width = data_type_byte_width_map[self.filter_dtype]
+        coeffient = 6 if self.get_is_gated() else 4
+        context_ffn = coeffient * batch_size * seq_len * hidden_size * ffn_size / hfu_peak_compute
+        return context_ffn
+
+    def get_decoder_moe_flops(self, batch_size, input_len, hfu_peak_compute=1.0):
+        seq_len = 1
+        hidden_size = self.get_hidden_size()
+        coeffient = 6 if self.get_is_gated() else 4
+        moe_size = self.get_moe_inner_size()
+        shared_expert_intermediate_size = self.get_shared_experts_inner_size()
+        topk=self.get_topk_num()
+        expert_num = self.get_experts_num()
+        if self.smooth_quant_type == "invalid":
+            context_moe = batch_size * hidden_size * (coeffient * (moe_size * self.get_topk_num() + shared_expert_intermediate_size) +\
+                                                                2 * self.get_experts_num() * self.tensor_parallel_size) / hfu_peak_compute
+        else:
+            context_moe = batch_size  * hidden_size * (coeffient * (moe_size * self.get_topk_num() + shared_expert_intermediate_size) +\
+                                                                2 * 2 * self.get_experts_num() * self.tensor_parallel_size) / hfu_peak_compute
+        return context_moe
+
+    def get_decoder_attn_flops(self, batch_size, input_len, output_len, hfu_qk_peak_compute=1.0, hfu_peak_compute=1.0):
+        seq_len_decode = input_len + output_len / 2
+
+        hidden_size    = self.get_hidden_size()
+        head_num       = self.get_num_attn_heads()
+        head_num_kv    = self.get_num_kv_heads()
+        head_size      = self.get_head_size()
+        v_head_size    = self.get_v_head_size()
+        cla_coeffient = self.get_cla_coefficient()
+        is_causal = 0.5 if self.is_causal() else 1.0
+        context_atn_pre = (2 * batch_size *  (head_num * head_size + 2 * head_num_kv * v_head_size) * hidden_size * cla_coeffient) / hfu_peak_compute
+        context_atn_qk = 2 * is_causal * batch_size * seq_len_decode * head_num * head_size / hfu_qk_peak_compute
+        context_atn_qkv = 2 * is_causal * batch_size * seq_len_decode * head_num * v_head_size / hfu_qk_peak_compute
+        context_atn_post = 2 * batch_size * head_num * head_size * hidden_size / hfu_peak_compute
+        ret = (context_atn_qk + context_atn_qkv + context_atn_pre + context_atn_post)
+        return ret
+
+    def get_moe_param(self, batch_size):
+        batch = batch_size
+        hidden_size = self.get_hidden_size()
+        moe_inner_size = self.get_moe_inner_size()
+        r = self.get_num_attn_heads() / self.get_num_kv_heads()
+        layer_moe_num = self.get_moe_num()
+        experts_num = self.get_experts_num()
+        topk_num = self.get_topk_num()
+        shared_expert_inner_size = self.get_shared_experts_inner_size()
+        expert_num = min(experts_num, batch * topk_num)
+        ffn_num = 3.0 if self.get_is_gated() else 2.0
+        expert_param = layer_moe_num * ffn_num * hidden_size * (expert_num * moe_inner_size + shared_expert_inner_size)
+        return expert_param 
+
+    def get_ffn_param(self):
+        hidden_size = self.get_hidden_size()
+        ffn_inner_size = self.get_ffn_inner_size()
+        moe_inner_size = self.get_moe_inner_size()
+        layer_ffn_num = self.get_ffn_num()
+        ffn_num = 3.0 if self.get_is_gated() else 2.0
+        ffn_param = layer_ffn_num * ffn_num * hidden_size * ffn_inner_size
+        return ffn_param
+
+    def get_attn_param(self):
+        hidden_size = self.get_hidden_size()
+        layer_num = self.get_layer_num()
+        r = self.get_num_attn_heads() / self.get_num_kv_heads()
+        cla_coeffient = self.get_cla_coefficient()
+        attn_param = (2.0 + 2.0 / r * cla_coeffient) * layer_num * hidden_size * hidden_size
+        return attn_param
+
+    def get_model_param(self, batch_size):
+        expert_param = self.get_moe_param(batch_size)
+        ffn_param = self.get_ffn_param() 
+        attn_param = self.get_attn_param() 
+        return expert_param + ffn_param + attn_param
+

