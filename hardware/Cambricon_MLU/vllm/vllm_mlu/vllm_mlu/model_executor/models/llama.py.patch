diff --git a/vllm_mlu/vllm_mlu/model_executor/models/llama.py b/vllm_mlu/vllm_mlu/model_executor/models/llama.py
new file mode 100644
index 000000000..b725c0aa6
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/llama.py
@@ -0,0 +1,243 @@
+import torch
+from torch import nn
+
+from typing import Dict, Optional, Union, Any, Type
+from transformers import LlamaConfig
+
+from vllm.attention import AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.models.llama import (LlamaAttention, LlamaDecoderLayer,
+                                              LlamaModel, LlamaForCausalLM)
+from vllm.sequence import IntermediateTensors
+from vllm.logger import init_logger
+
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+logger = init_logger(__name__)
+
+
+class MLULlamaAttention(LlamaAttention):
+
+    def __init__(
+        self,
+        config: LlamaConfig,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        quant_config: Optional[QuantizationConfig] = None,
+        bias: bool = False,
+        bias_o_proj: bool = False,
+        cache_config: Optional[CacheConfig] = None,
+        prefix: str = "",
+        attn_type: str = AttentionType.DECODER,
+    ) -> None:
+        super().__init__(
+            config, hidden_size, num_heads,
+            num_kv_heads, rope_theta, rope_scaling,
+            max_position_embeddings, quant_config,
+            bias, bias_o_proj, cache_config, prefix,
+			attn_type
+        )
+        self.rope_scaling = rope_scaling
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        @brief: o_proj add residual
+        '''
+        if self.rope_scaling is not None and self.rope_scaling["rope_type"] == "longrope":
+            q, k = self.rotary_emb(positions, q, k)
+        else:
+            qk, _ = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)
+            self.rotary_emb(positions, qk.view(
+                -1, self.num_heads + self.num_kv_heads, self.head_dim))
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+
+class MLULlamaDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: LlamaConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        if rope_scaling is not None and getattr(
+                config, "original_max_position_embeddings", None):
+            rope_scaling["original_max_position_embeddings"] = (
+                config.original_max_position_embeddings)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        # Support abacusai/Smaug-72B-v0.1 with attention_bias
+        # Support internlm/internlm-7b with bias
+        attention_bias = getattr(config, "attention_bias", False) or getattr(
+            config, "bias", False)
+        bias_o_proj = attention_bias
+        # support internlm/internlm3-8b with qkv_bias
+        if hasattr(config, 'qkv_bias'):
+            attention_bias = config.qkv_bias
+
+        # By default, Llama uses causal attention as it is a decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        # (e.g. parasail-ai/GritLM-7B-vllm)
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLULlamaAttention
+        @brief: use FeedForward
+        '''
+        self.self_attn = MLULlamaAttention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=getattr(config, "num_key_value_heads",
+                                 config.num_attention_heads),
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            quant_config=quant_config,
+            bias=attention_bias,
+            bias_o_proj=bias_o_proj,
+            cache_config=cache_config,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        self.mlp = FeedForward(hidden_size=config.hidden_size,
+                                intermediate_size=config.intermediate_size,
+                                hidden_act='silu',
+                                up_proj_name='gate_up_proj',
+                                is_gated=True,
+                                down_proj_name='down_proj',
+                                bias=getattr(config, "mlp_bias", False),
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.mlp")
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: prepare to perf sq cases if suitable
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            self.input_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.self_attn.qkv_proj)
+            self.post_attention_layernorm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.mlp.gate_up_proj)
+            self.is_per_token_sq_perf_cases = self.input_layernorm.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return decoder_layer_forward_base(
+            positions, hidden_states,
+            self.input_layernorm,
+            self.self_attn,
+            self.post_attention_layernorm,
+            self.mlp,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+@support_torch_compile
+class MLULlamaModel(LlamaModel):
+
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor],
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors],
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        return decoder_model_forward_base_pp(
+            input_ids, positions, intermediate_tensors,
+            self.layers, self.start_layer, self.end_layer,
+            self.get_input_embeddings,
+            self.norm,
+            inputs_embeds
+        )
+
+
+class MLULlamaForCausalLM(LlamaForCausalLM, MLUCausalLM):
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        LlamaForCausalLM.__init__(self, vllm_config=vllm_config, prefix=prefix)
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+
+    def _init_model(self,
+                    vllm_config: VllmConfig,
+                    prefix: str = "",
+                    layer_type: type[nn.Module] = LlamaDecoderLayer):
+        return MLULlamaModel(vllm_config=vllm_config,
+                             prefix=prefix,
+                             layer_type=MLULlamaDecoderLayer)

