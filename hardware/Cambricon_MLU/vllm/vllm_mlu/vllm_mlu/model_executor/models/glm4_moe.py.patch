diff --git a/vllm_mlu/vllm_mlu/model_executor/models/glm4_moe.py b/vllm_mlu/vllm_mlu/model_executor/models/glm4_moe.py
new file mode 100644
index 000000000..9dbd87784
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/glm4_moe.py
@@ -0,0 +1,706 @@
+# SPDX-License-Identifier: Apache-2.0
+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
+
+# Copyright 2025 The ZhipuAI Team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only GLM-4.5 model compatible with HuggingFace weights."""
+import typing
+import re
+from collections.abc import Callable, Iterable
+from typing import Any, Optional, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig
+
+from vllm.attention import Attention
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig, get_current_vllm_config
+from vllm.distributed import (get_ep_group, get_pp_group,
+                              get_tensor_model_parallel_world_size)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               ReplicatedLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import (
+    default_weight_loader, maybe_remap_kv_scale_name)
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from vllm.model_executor.models.interfaces import SupportsPP
+from vllm.model_executor.models.utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+from vllm.model_executor.models.glm4_moe import (Glm4MoE,
+    Glm4MoeAttention, Glm4MoeModel, Glm4MoeMLP, Glm4MoeDecoderLayer,
+    get_spec_layer_idx_from_weight_name, Glm4MoeForCausalLM)
+
+from vllm.model_executor.layers.quantization.fp8 import Fp8Config
+
+from vllm.distributed import (
+    get_dp_group,
+    get_pp_group,
+    get_tp_group,
+    get_tensor_model_parallel_world_size,
+    get_data_parallel_group_rank,
+    get_data_parallel_group_world_size,
+    get_dense_mlp_tp_group,
+    get_dense_mlp_tp_world_size,
+    get_dense_mlp_tp_rank,
+    get_tp_world_rank,
+    get_tensor_model_parallel_rank,
+    get_tp_world_world_size,
+    get_tp_world_group,
+    get_parallel_rank_with_group,
+)
+
+from vllm_mlu.model_executor.models.dp_utils import (
+    DataParallelRuntimeParams,
+    enable_data_parallel,
+    get_dp_metadata,
+    process_post_attention_communication,
+    tensor_model_parallel_all_gather_dp,
+    tensor_model_parallel_all_gather_op_v2,
+)
+from vllm_mlu.model_executor.layers.sparse_moe_mlp import SparseMoeMlp, MoeGroupInfo
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+from vllm_mlu._mlu_utils import *
+from vllm_mlu.model_executor.models.mlu_abstract_LM import MLUCausalLM, data_type_byte_width_map
+
+
+def vllm__module_executor__models__glm4_moe__Glm4MoeAttention____forward(
+    self,
+    positions: torch.Tensor,
+    hidden_states: torch.Tensor,
+) -> torch.Tensor:
+    qkv, _ = self.qkv_proj(hidden_states)
+    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+    if self.use_qk_norm:
+        q = self.q_norm(q.reshape(-1, self.num_heads,
+                                    self.head_dim)).reshape(q.shape)
+        k = self.k_norm(k.reshape(-1, self.num_kv_heads,
+                                    self.head_dim)).reshape(k.shape)
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: rehspae q/k for rotary emb
+    '''
+    q = q.reshape(-1, self.num_heads, self.head_dim)
+    k = k.reshape(-1, self.num_kv_heads, self.head_dim)
+    num_tokens = q.shape[0]
+    self.rotary_emb(positions, q)
+    self.rotary_emb(positions, k)
+    q = q.reshape(num_tokens, self.q_size)
+    k = k.reshape(num_tokens, self.kv_size)
+    '''
+    =============================
+    End of MLU Hijack
+    =============================
+    '''
+    attn_output = self.attn(q, k, v)
+    output, _ = self.o_proj(attn_output)
+    return output
+
+class MLUGlm4SparseMoE(SparseMoeMlp):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        parallelize_shared_expert: bool = False,
+    ):
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: create sparse moe layer
+        '''
+        # set tp_world_group to mlp
+        tp_group = get_tp_world_group() if enable_data_parallel() else None
+        super().__init__(num_experts=config.n_routed_experts,
+                         top_k=config.num_experts_per_tok,
+                         hidden_size=config.hidden_size,
+                         intermediate_size=config.moe_intermediate_size,
+                         up_proj_name="gate_up_proj",
+                         is_gated=True,
+                         down_proj_name="down_proj",
+                         has_bias=False,
+                         skip_bias_add=False,
+                         renormalize=config.norm_topk_prob,
+                         hidden_act=config.hidden_act,
+                         params_dtype=None,
+                         quant_config=quant_config,
+                         is_use_fused_moe=True,
+                         expert_group=config.n_group,
+                         topk_group=config.topk_group,
+                         scoring_func="sigmoid",
+                         topk_method="noaux_tc",
+                         routed_scaling_factor=config.routed_scaling_factor,
+                         tp_group=tp_group)
+        
+        self.config = config
+        self.n_shared_experts = config.n_shared_experts
+        self.routed_scaling_factor = config.routed_scaling_factor
+        self.tp_size = get_tp_world_world_size()
+        if self.moe_tp_size > config.n_routed_experts:
+            raise ValueError(
+                f"Moe Tensor parallel size {self.moe_tp_size} is greater than "
+                f"the number of experts {config.n_routed_experts}.")
+
+        if config.hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {config.hidden_act}. "
+                             "Only silu is supported for now.")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.n_routed_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+        self.gate.e_score_correction_bias = nn.Parameter(
+            torch.empty(config.n_routed_experts))
+        
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+        if config.n_shared_experts is not None:
+            intermediate_size = (config.moe_intermediate_size *
+                                 config.n_shared_experts)
+            self.shared_experts = Glm4MoeMLP(
+                hidden_size=config.hidden_size,
+                intermediate_size=intermediate_size,
+                hidden_act=config.hidden_act,
+                quant_config=quant_config,
+                reduce_results=False,
+                prefix=f"{prefix}.shared_experts",
+            )
+
+        if self.is_fp8_block_wise:
+            self.experts.e_score_correction_bias = self.gate.e_score_correction_bias
+    
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        num_tokens, hidden_dim = hidden_states.shape
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        if self.n_shared_experts is not None:
+            shared_output = self.shared_experts(hidden_states)
+        router_logits, _ = self.gate(hidden_states)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: replace experts() with forward_experts, which defined by SparseMoeMlp.
+        '''      
+        final_hidden_states = self.forward_experts(
+            hidden_states, router_logits, shared_output=shared_output)
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+        if self.tp_size > 1:
+            final_hidden_states = self.reduce_results(final_hidden_states)
+        return final_hidden_states.view(num_tokens, hidden_dim)
+
+
+class MLUGlm4MoeDecoderLayer(Glm4MoeDecoderLayer):
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+        enable_eplb: bool = False,
+    ) -> None:
+        super(Glm4MoeDecoderLayer, self).__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                            131072)
+        # DecoderLayers are created with `make_layers` which passes the prefix
+        # with the layer's index.
+        layer_idx = int(prefix.split(sep='.')[-1])
+        self.layer_idx = layer_idx
+
+        self.self_attn = Glm4MoeAttention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            head_dim=config.head_dim,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=config.attention_bias,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+            use_qk_norm=config.use_qk_norm,
+        )
+
+        if (config.n_routed_experts is not None
+                and layer_idx >= config.first_k_dense_replace):
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: use mlu sparse moe
+            '''
+            # TODO(zhaoxuesong): support parallel shared expert
+            parallelize_shared_expert = False
+            self.mlp = MLUGlm4SparseMoE(config=config,
+                                            quant_config=quant_config,
+                                            prefix=f"{prefix}.mlp",
+                                            parallelize_shared_expert=parallelize_shared_expert,
+                                            )
+            '''
+            =============================
+            End of MLU Hijack
+            =============================
+            '''
+        else:
+            self.mlp = Glm4MoeMLP(hidden_size=config.hidden_size,
+                                    intermediate_size=config.intermediate_size,
+                                    hidden_act=config.hidden_act,
+                                    quant_config=quant_config,
+                                    prefix=f"{prefix}.mlp")
+
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                        eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.routed_scaling_factor = config.routed_scaling_factor
+
+
+@support_torch_compile
+class MLUGlm4MoeModel(Glm4MoeModel):
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super(Glm4MoeModel, self).__init__()
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+        self.quant_config = quant_config
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: disable eplb and set self.config for load_weights
+        '''
+        enable_eplb = False
+        self.config = config
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+        self.vocab_size = config.vocab_size
+
+        if get_pp_group().is_first_rank:
+            self.embed_tokens = VocabParallelEmbedding(
+                config.vocab_size,
+                config.hidden_size,
+                quant_config=quant_config,
+                prefix=f"{prefix}.embed_tokens")
+        else:
+            self.embed_tokens = PPMissingLayer()
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUGlm4MoeDecoderLayer
+        '''
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: MLUGlm4MoeDecoderLayer(
+                config=config,
+                cache_config=cache_config,
+                quant_config=quant_config,
+                prefix=prefix,
+                enable_eplb=enable_eplb,
+            ),
+            prefix=f"{prefix}.layers")
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+        if get_pp_group().is_last_rank:
+            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        else:
+            self.norm = PPMissingLayer()
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params and cal start expert id
+        '''
+        for name, m in self.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params()
+
+        # expert parallel modification start
+        moe_group_info = MoeGroupInfo()
+        moe_ep_size = moe_group_info.moe_ep_size
+        moe_ep_rank = moe_group_info.moe_ep_rank
+        num_total_experts = self.config.n_routed_experts
+        start_expert_id = moe_ep_rank * ((num_total_experts + moe_ep_size - 1) // moe_ep_size)
+        # expert parallel modification end
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: expert_params_mapping for fp8 block_wise
+        '''
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        is_fp8_block_wise = isinstance(self.quant_config, Fp8Config) and \
+                self.quant_config.weight_block_size is not None
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.n_routed_experts) if is_fp8_block_wise else {}
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+        for name, loaded_weight in weights:
+            spec_layer = get_spec_layer_idx_from_weight_name(self.config, name)
+            if spec_layer is not None:
+                continue
+            
+            '''
+            =============================
+            Modify by vllm_mlu
+            =============================
+            @brief: replace expert_id in weight to named_expert_id in params_dict
+            '''
+            if start_expert_id > 0 and "mlp.experts." in name:
+                expert_str = re.search(r'experts\.\d+', name).group(0)
+                expert_id = int(expert_str.split(".")[1])
+                named_expert_id = expert_id - start_expert_id
+                if named_expert_id < 0:
+                    continue
+                old_expert_name = f"experts.{expert_id}"
+                new_expert_name = f"experts.{named_expert_id}"
+                name = name.replace(old_expert_name, new_expert_name)
+            '''
+            ==================
+            End of MLU Hijack
+            ==================
+            '''
+
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                if is_fp8_block_wise and ("mlp.experts." in name):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: add expert skiped condition and delete useless if name not in params_dict: continue condition
+                '''
+                name = name.replace(weight_name, param_name)
+                if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                    and name not in params_dict):
+                    continue
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                is_expert_weight = False
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+
+                    # Anyway, this is an expert weight and should not be
+                    # attempted to load as other weights later
+                    is_expert_weight = True
+
+                    # Do not modify `name` since the loop may continue here
+                    # Instead, create a new variable
+                    name_mapped = name.replace(weight_name, param_name)
+
+                    if is_pp_missing_parameter(name_mapped, self):
+                        continue
+
+                    param = params_dict[name_mapped]
+                    # We should ask the weight loader to return success or not
+                    # here since otherwise we may skip experts with other
+                    # available replicas.
+                    weight_loader = typing.cast(Callable[..., bool],
+                                                param.weight_loader)
+                    '''
+                    =============================
+                    Modify by vllm_mlu
+                    =============================
+                    @brief: del enable eplb code
+                    '''
+                    weight_loader(param,
+                                loaded_weight,
+                                name_mapped,
+                                shard_id=shard_id,
+                                expert_id=expert_id,
+                                )
+                    name = name_mapped
+                    break
+                    '''
+                    =============================
+                    End of MLU Hijack
+                    =============================
+                    '''
+                else:
+                    if is_expert_weight:
+                        # We've checked that this is an expert weight
+                        # However it's not mapped locally to this rank
+                        # So we simply skip it
+                        continue
+
+                    # Skip loading extra bias for GPTQ models.
+                    if name.endswith(".bias") and name not in params_dict:
+                        continue
+
+                    # Remapping the name of FP8 kv-scale.
+                    name = maybe_remap_kv_scale_name(name, params_dict)
+                    if name is None:
+                        continue
+
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                
+                    # Skip experts that are not assigned to this worker.
+                    if (("mlp.experts." in name or "mlp.shared_expert." in name or "mlp.shared_expert_gate." in name)
+                            and name not in params_dict):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack params after loading
+        '''
+        for name, m in self.named_modules():
+            if isinstance(m, SparseMoeMlp):
+                m.pack_params_after_loading()
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+        return loaded_params
+
+
+class MLUGlm4MoeForCausalLM(Glm4MoeForCausalLM, MLUCausalLM):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""
+    ):
+        super(Glm4MoeForCausalLM, self).__init__()
+        MLUCausalLM.__init__(self, vllm_config=vllm_config)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUGlm4MoeModel for repalce Glm4MoeModel.
+        '''
+        self.model = MLUGlm4MoeModel(vllm_config=vllm_config,
+                                    prefix=maybe_prefix(prefix, "model"))
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        if get_pp_group().is_last_rank:
+            self.lm_head = ParallelLMHead(config.vocab_size,
+                                            config.hidden_size,
+                                            quant_config=quant_config)
+        else:
+            self.lm_head = PPMissingLayer()
+        if self.config.tie_word_embeddings:
+            self.lm_head.weight = self.model.embed_tokens.weight
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+        self.expert_weights = []
+
+        # Set MoE hyperparameters
+        self.num_moe_layers = (config.num_hidden_layers -
+                                config.first_k_dense_replace)
+        self.num_expert_groups = config.n_group
+
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: disable eplb for glm4 moe, will support eplb in the future
+        '''
+        # self.moe_layers: list[FusedMoE] = []
+        # for layer in self.model.layers:
+        #     assert isinstance(layer, Glm4MoeDecoderLayer)
+        #     if isinstance(layer.mlp, MLUGlm4SparseMoE):
+        #         self.moe_layers.append(layer.mlp.experts)
+
+        # # Pick last one layer since the first ones may be dense layers.
+        # example_moe = typing.cast(
+        #     Glm4MoE, self.model.layers[config.num_hidden_layers - 1].mlp)
+        # self.num_logical_experts = example_moe.n_logical_experts
+        # self.num_physical_experts = example_moe.n_physical_experts
+        # self.num_local_physical_experts = example_moe.n_local_physical_experts
+        # self.num_routed_experts = example_moe.n_routed_experts
+        # self.num_shared_experts = example_moe.n_shared_experts
+        # self.num_redundant_experts = example_moe.n_redundant_experts
+        '''
+        =============================
+        End of MLU Hijack
+        =============================
+        '''
+
+    def get_experts_num(self):
+        if hasattr(self.model_config, "n_routed_experts"):
+            return self.model_config.n_routed_experts
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "n_routed_experts"):
+            return self.model_config.text_config.n_routed_experts
+        else:
+            return 0
+
+    def get_ffn_num(self):
+        if hasattr(self.model_config, "first_k_dense_replace"):
+            return self.model_config.first_k_dense_replace
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "first_k_dense_replace"):
+            return self.model_config.text_config.first_k_dense_replace
+        else:
+            return 0
+
+    def get_moe_num(self):
+        if hasattr(self.model_config, "first_k_dense_replace"):
+            return self.get_layer_num() - self.model_config.first_k_dense_replace
+        elif hasattr(self.model_config, "text_config") and hasattr(self.model_config.text_config, "first_k_dense_replace"):
+            return self.get_layer_num() - self.model_config.text_config.first_k_dense_replace
+        else:
+            return 0
+
+    def get_shared_experts_inner_size(self):
+        if hasattr(self.model_config, "n_shared_experts") and self.model_config.n_shared_experts is not None:
+            return self.model_config.n_shared_experts * self.get_moe_inner_size()
+        elif (hasattr(self.model_config, "text_config") and 
+         hasattr(self.model_config.text_config, "n_shared_experts") and self.model_config.text_config.n_shared_experts is not None):
+            return self.model_config.text_config.n_shared_experts * self.get_moe_inner_size()
+        else:
+            return 0
+    
+    def get_head_size(self):
+        if hasattr(self.model_config, "head_dim") and self.model_config.head_dim is not None:
+            return self.model_config.head_dim
+        elif (hasattr(self.model_config, "text_config") and \
+         hasattr(self.model_config.text_config, "head_dim") and self.model_config.text_config.head_dim is not None):
+            return self.model_config.text_config.head_dim
+        else:
+            return 0
+
+MluHijackObject.apply_hijack(Glm4MoeAttention,
+                             Glm4MoeAttention.forward,
+                             vllm__module_executor__models__glm4_moe__Glm4MoeAttention____forward)
+

