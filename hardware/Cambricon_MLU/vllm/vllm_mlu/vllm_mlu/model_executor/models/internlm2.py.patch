diff --git a/vllm_mlu/vllm_mlu/model_executor/models/internlm2.py b/vllm_mlu/vllm_mlu/model_executor/models/internlm2.py
new file mode 100644
index 000000000..8e0ffa9ba
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/model_executor/models/internlm2.py
@@ -0,0 +1,293 @@
+import torch
+from torch import nn
+from torch.nn import Module
+
+from typing import Optional, Tuple, Iterable, Union, Set, Type
+from transformers import PretrainedConfig
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.models.internlm2 import (
+    InternLM2Attention, InternLMDecoderLayer, InternLM2ForCausalLM, InternLM2Model)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.sequence import IntermediateTensors
+from vllm.model_executor.models.interfaces import SupportsLoRA, SupportsPP
+from vllm.model_executor.models.utils import (is_pp_missing_parameter,
+                                              maybe_prefix)
+from vllm.logger import init_logger
+
+from vllm_mlu.model_executor.models.layer_utils import (
+    decoder_layer_forward_base, decoder_model_forward_base_pp,
+    is_smoothquant)
+from vllm_mlu.model_executor.layers.feed_forward import FeedForward
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+from vllm_mlu.model_executor.layers.layernorm import QuantFusionRMSNorm
+logger = init_logger(__name__)
+
+
+class MLUInternLM2Attention(InternLM2Attention):
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor] = None,
+        smooth_quant_scale: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        qkv, _ = self.wqkv(hidden_states, smooth_quant_scale)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: pack q & k to fit tmo.apply_rotary
+        @brief: o_proj add residual
+        '''
+        qk, v = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)
+        self.rotary_emb(positions, qk.view(
+            -1, self.num_heads + self.num_kv_heads, self.head_dim))
+        attn_output = self.attn(q, k, v)
+        output, _ = self.wo(attn_output, residual)
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        return output
+
+
+class MLUInternLMDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: use MLUInternLM2Attention and FeedForward
+        '''
+        self.attention = MLUInternLM2Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.attention",
+        )
+        self.feed_forward = FeedForward(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            up_proj_name='gate_up_proj',
+            is_gated=True,
+            down_proj_name='w2',
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.feed_forward",
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+        self.attention_norm = RMSNorm(config.hidden_size,
+                                      eps=config.rms_norm_eps)
+        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: prepare to perf per-tensor sq cases if suitable
+        '''
+        self.is_per_token_sq_perf_cases = False
+        if is_smoothquant(quant_config):
+            self.attention_norm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.attention.wqkv)
+            self.ffn_norm = QuantFusionRMSNorm(
+                config.hidden_size, config.rms_norm_eps,
+                self.feed_forward.gate_up_proj)
+            self.is_per_token_sq_perf_cases = self.attention_norm.dynamic_quant
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        '''
+        =============================
+        Modify by vllm_mlu
+        =============================
+        @brief: perf model by:
+        1) add residual in matmul;
+        2) fuse quantization in layernorm in per-tensor sq case;
+        '''
+        return decoder_layer_forward_base(
+            positions, hidden_states,
+            self.attention_norm,
+            self.attention,
+            self.ffn_norm,
+            self.feed_forward,
+            input_norm_fuse_en=self.is_per_token_sq_perf_cases,
+            post_norm_fuse_en=self.is_per_token_sq_perf_cases
+        )
+        '''
+        ==================
+        End of MLU Hijack
+        ==================
+        '''
+
+
+@support_torch_compile
+class MLUInternLM2Model(InternLM2Model):
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        return decoder_model_forward_base_pp(
+            input_ids, positions, intermediate_tensors,
+            self.layers, self.start_layer, self.end_layer,
+            self.tok_embeddings,
+            self.norm,
+            inputs_embeds
+        )
+
+
+class MLUInternLM2ForCausalLM(InternLM2ForCausalLM):
+
+    def __init__(self,
+                 *,
+                 vllm_config: VllmConfig,
+                 prefix: str = "",
+                 model_type: type[MLUInternLM2Model] = MLUInternLM2Model):
+        Module.__init__(self)
+        SupportsPP.__init__(self)
+        SupportsLoRA.__init__(self)
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.quant_config = quant_config
+        self.lora_config = lora_config
+
+        self.model = model_type(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"),
+                                layer_type=MLUInternLMDecoderLayer)
+        self.output = ParallelLMHead(config.vocab_size,
+                                     config.hidden_size,
+                                     quant_config=quant_config,
+                                     prefix=maybe_prefix(prefix, "output"))
+        if self.config.tie_word_embeddings:
+            self.output.weight = self.model.tok_embeddings.weight
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def load_weights(
+        self,
+        weights: Iterable[tuple[str, torch.Tensor]]
+    ) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "w1", 0),
+            ("gate_up_proj", "w3", 1),
+        ]
+        params_dict = dict(self.named_parameters())
+        loaded_params: set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                '''
+                =============================
+                Modify by vllm_mlu
+                =============================
+                @brief: support load quant weights and params
+                '''
+                if "wqkv" in name and 'smooth' not in name and 'scale_to_int' not in name:
+                    config = self.config
+                    kv_groups = (config.num_attention_heads //
+                                    config.num_key_value_heads)
+                    head_dim = config.hidden_size // config.num_attention_heads
+                    if 'weight' in name:
+                        loaded_weight = loaded_weight.view(-1, 2 + kv_groups,
+                                                            head_dim,
+                                                            loaded_weight.shape[-1])
+                        wq, wk, wv = torch.split(loaded_weight, [kv_groups, 1, 1],
+                                                    dim=1)
+                        wq = wq.reshape(-1, wq.shape[-1])
+                        wk = wk.reshape(-1, wk.shape[-1])
+                        wv = wv.reshape(-1, wv.shape[-1])
+                    elif 'scale' in name:
+                        loaded_weight = loaded_weight.view(-1, 2 + kv_groups, head_dim)
+                        wq, wk, wv = torch.split(loaded_weight, [kv_groups, 1, 1],
+                                                    dim=1)
+                        wq = wq.reshape(-1)
+                        wk = wk.reshape(-1)
+                        wv = wv.reshape(-1)
+                    else:
+                        logger.error(f"unsupport internlm2 quant param: {name}, shape: {loaded_weight.shape}")
+                    weight_loader = param.weight_loader
+                    weight_loader(param, wq, 'q')
+                    weight_loader(param, wk, 'k')
+                    weight_loader(param, wv, 'v')
+                else:
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+                '''
+                ==================
+                End of MLU Hijack
+                ==================
+                '''
+            loaded_params.add(name)
+        return loaded_params
\ No newline at end of file

