diff --git a/vllm_mlu/vllm_mlu/engine/arg_utils.py b/vllm_mlu/vllm_mlu/engine/arg_utils.py
new file mode 100644
index 000000000..9a9bad7cc
--- /dev/null
+++ b/vllm_mlu/vllm_mlu/engine/arg_utils.py
@@ -0,0 +1,473 @@
+from typing import Optional, get_args
+import threading
+
+import torch
+
+import vllm.envs as envs
+from vllm.config import (ModelConfig, LoadFormat, VllmConfig,
+                         SchedulerConfig, GuidedDecodingBackendV1,
+                         ParallelConfig, CacheDType)
+from vllm.engine.arg_utils import (EngineArgs, _raise_or_fallback,
+                                   _warn_or_fallback)
+from vllm.logger import init_logger
+from vllm.usage.usage_lib import UsageContext
+from vllm.utils import GiB_bytes
+
+import vllm_mlu._mlu_utils as mlu_envs
+from vllm_mlu.mlu_hijack_utils import MluHijackObject
+
+logger = init_logger(__name__)
+
+
+vllm__engine__arg_utils__EngineArgs__create_engine_config_org = EngineArgs.create_engine_config
+
+
+def vllm__engine__arg_utils__EngineArgs___is_v1_supported_oracle(
+    self, model_config: ModelConfig
+) -> bool:
+    """Oracle for whether to use V0 or V1 Engine by default."""
+
+    #############################################################
+    # Unsupported Feature Flags on V1.
+
+    if self.load_format == LoadFormat.SHARDED_STATE.value:
+        _raise_or_fallback(
+            feature_name=f"--load_format {self.load_format}",
+            recommend_to_remove=False)
+        return False
+
+    if (self.logits_processor_pattern
+            != EngineArgs.logits_processor_pattern):
+        _raise_or_fallback(feature_name="--logits-processor-pattern",
+                            recommend_to_remove=False)
+        return False
+
+    if self.preemption_mode != SchedulerConfig.preemption_mode:
+        _raise_or_fallback(feature_name="--preemption-mode",
+                            recommend_to_remove=True)
+        return False
+
+    if (self.disable_async_output_proc
+            != EngineArgs.disable_async_output_proc):
+        _raise_or_fallback(feature_name="--disable-async-output-proc",
+                            recommend_to_remove=True)
+        return False
+
+    if self.scheduling_policy != SchedulerConfig.policy:
+        _raise_or_fallback(feature_name="--scheduling-policy",
+                            recommend_to_remove=False)
+        return False
+
+    if self.num_scheduler_steps != SchedulerConfig.num_scheduler_steps:
+        _raise_or_fallback(feature_name="--num-scheduler-steps",
+                            recommend_to_remove=True)
+        return False
+
+    if self.scheduler_delay_factor != SchedulerConfig.delay_factor:
+        _raise_or_fallback(feature_name="--scheduler-delay-factor",
+                            recommend_to_remove=True)
+        return False
+
+    if self.guided_decoding_backend not in get_args(
+            GuidedDecodingBackendV1):
+        _raise_or_fallback(
+            feature_name=
+            f"--guided-decoding-backend={self.guided_decoding_backend}",
+            recommend_to_remove=False)
+        return False
+
+    # Need at least Ampere for now (FA support required).
+    # Skip this check if we are running on a non-GPU platform,
+    # or if the device capability is not available
+    # (e.g. in a Ray actor without GPUs).
+    from vllm.platforms import CpuArchEnum, current_platform
+    if (current_platform.is_cuda()
+            and current_platform.get_device_capability()
+            and current_platform.get_device_capability().major < 8):
+        _raise_or_fallback(feature_name="Compute Capability < 8.0",
+                            recommend_to_remove=False)
+        return False
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: support mlu kv_cache_dtype=int8/fp8/fp8_e4m3
+    '''
+    # Fp8/Fp8_e4m3 KV cache only support on mlu 6.0+
+    if self.kv_cache_dtype != "auto":
+        if self.kv_cache_dtype == "int8" or self.kv_cache_dtype.startswith("fp8"):
+            if current_platform.is_out_of_tree():
+                logger.info(f"MLU-V1 use kv_cache_dtype={self.kv_cache_dtype}.")
+            else:
+                _raise_or_fallback(feature_name="--kv-cache-dtype",
+                                    recommend_to_remove=False)
+                return False
+        else:
+            fp8_attention = self.kv_cache_dtype.startswith("fp8")
+            will_use_fa = (
+                current_platform.is_cuda()
+                and not envs.is_set("VLLM_ATTENTION_BACKEND")
+            ) or envs.VLLM_ATTENTION_BACKEND == "FLASH_ATTN_VLLM_V1"
+            supported = False
+            if current_platform.is_rocm():
+                supported = True
+            elif fp8_attention and will_use_fa:
+                from vllm.attention.utils.fa_utils import (
+                    flash_attn_supports_fp8)
+                supported = flash_attn_supports_fp8()
+            if not supported:
+                _raise_or_fallback(feature_name="--kv-cache-dtype",
+                                    recommend_to_remove=False)
+                return False
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    # No Prompt Adapter so far.
+    if self.enable_prompt_adapter:
+        _raise_or_fallback(feature_name="--enable-prompt-adapter",
+                            recommend_to_remove=False)
+        return False
+
+    # No text embedding inputs so far.
+    if self.enable_prompt_embeds:
+        _raise_or_fallback(feature_name="--enable-prompt-embeds",
+                            recommend_to_remove=False)
+        return False
+
+    # Only Fp16 and Bf16 dtypes since we only support FA.
+    V1_SUPPORTED_DTYPES = [torch.bfloat16, torch.float16]
+    if model_config.dtype not in V1_SUPPORTED_DTYPES:
+        _raise_or_fallback(feature_name=f"--dtype {model_config.dtype}",
+                            recommend_to_remove=False)
+        return False
+
+    # No Embedding Models so far.
+    if model_config.task not in ["generate"]:
+        _raise_or_fallback(feature_name=f"--task {model_config.task}",
+                            recommend_to_remove=False)
+        return False
+
+    # No Mamba or Encoder-Decoder so far.
+    if not model_config.is_v1_compatible:
+        _raise_or_fallback(feature_name=model_config.architectures,
+                            recommend_to_remove=False)
+        return False
+
+    # No Concurrent Partial Prefills so far.
+    if (self.max_num_partial_prefills
+            != SchedulerConfig.max_num_partial_prefills
+            or self.max_long_partial_prefills
+            != SchedulerConfig.max_long_partial_prefills):
+        _raise_or_fallback(feature_name="Concurrent Partial Prefill",
+                            recommend_to_remove=False)
+        return False
+
+    # No OTLP observability so far.
+    if (self.otlp_traces_endpoint or self.collect_detailed_traces):
+        _raise_or_fallback(feature_name="--otlp-traces-endpoint",
+                            recommend_to_remove=False)
+        return False
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: check spec_decode support
+    '''
+    # V1 supports N-gram, Medusa, and Eagle speculative decoding.
+    is_ngram_enabled = False
+    is_eagle_enabled = False
+    is_medusa_enabled = False
+    is_mtp_enabled = False
+    if self.speculative_config is not None:
+        # This is supported but experimental (handled below).
+        speculative_method = self.speculative_config.get("method")
+        if speculative_method:
+            if speculative_method in ("ngram", "[ngram]"):
+                is_ngram_enabled = True
+            elif speculative_method == "medusa":
+                is_medusa_enabled = True
+            elif speculative_method in ("eagle", "eagle3", "deepseek_mtp"):
+                is_eagle_enabled = True
+        else:
+            speculative_model = self.speculative_config.get("model")
+            if speculative_model in ("ngram", "[ngram]"):
+                is_ngram_enabled = True
+        if not (is_ngram_enabled or is_eagle_enabled or is_medusa_enabled):
+            num_speculative_tokens = self.speculative_config.get(
+                "num_speculative_tokens", 0)
+            if model_config.is_deepseek_mla and num_speculative_tokens > 0:
+                is_mtp_enabled = True
+        if not (is_ngram_enabled or is_eagle_enabled or is_medusa_enabled
+                or is_mtp_enabled):
+            # Other speculative decoding methods are not supported yet.
+            _raise_or_fallback(feature_name="Speculative Decoding",
+                                recommend_to_remove=False)
+            return False
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    # No XFormers so far.
+    V1_BACKENDS = [
+        "FLASH_ATTN_VLLM_V1",
+        "FLASH_ATTN",
+        "PALLAS",
+        "PALLAS_VLLM_V1",
+        "TRITON_ATTN_VLLM_V1",
+        "TRITON_MLA",
+        "CUTLASS_MLA_VLLM_V1",
+        "FLASHMLA",
+        "FLASHINFER",
+        "FLASHINFER_VLLM_V1",
+        "ROCM_AITER_MLA",
+        "TORCH_SDPA_VLLM_V1",
+        "FLEX_ATTENTION",
+    ]
+    if (envs.is_set("VLLM_ATTENTION_BACKEND")
+            and envs.VLLM_ATTENTION_BACKEND not in V1_BACKENDS):
+        name = f"VLLM_ATTENTION_BACKEND={envs.VLLM_ATTENTION_BACKEND}"
+        _raise_or_fallback(feature_name=name, recommend_to_remove=True)
+        return False
+
+    # Platforms must decide if they can support v1 for this model
+    if not current_platform.supports_v1(model_config=model_config):
+        _raise_or_fallback(
+            feature_name=f"device type={current_platform.device_type}",
+            recommend_to_remove=False)
+        return False
+    #############################################################
+    # Experimental Features - allow users to opt in.
+
+    # Signal Handlers requires running in main thread.
+    if (threading.current_thread() != threading.main_thread()
+            and _warn_or_fallback("Engine in background thread")):
+        return False
+
+    if (self.pipeline_parallel_size > 1
+            and self.distributed_executor_backend
+            not in (ParallelConfig.distributed_executor_backend, "ray",
+                    "mp", "external_launcher")):
+        name = "Pipeline Parallelism without Ray distributed executor " \
+                "or multiprocessing executor or external launcher"
+        _raise_or_fallback(feature_name=name, recommend_to_remove=False)
+        return False
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: set v1 on by default at oot platform
+    '''
+    # Non-[CUDA, TPU] may be supported on V1, but off by default for now.
+    v0_hardware = not any(
+        (current_platform.is_cuda(), current_platform.is_tpu(),
+            (current_platform.is_cpu()
+             and current_platform.get_cpu_architecture() == CpuArchEnum.X86),
+             current_platform.is_out_of_tree()))
+    if v0_hardware and _warn_or_fallback(  # noqa: SIM103
+            current_platform.device_name):
+        return False
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    #############################################################
+
+    return True
+
+
+def vllm__engine__arg_utils__EngineArgs___set_default_args_v1(
+    self, usage_context: UsageContext
+) -> None:
+    """Set Default Arguments for V1 Engine."""
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: mlu-v1 default use unchunked scheduler
+    '''
+    # V1 always uses chunked prefills.
+    if self.enable_chunked_prefill is None:
+        if mlu_envs.VLLM_V1_USE_UNCHUNK_SCHED:
+            self.enable_chunked_prefill = False
+        else:
+            self.enable_chunked_prefill = True
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    # V1 enables prefix caching by default.
+    if self.enable_prefix_caching is None:
+        self.enable_prefix_caching = True
+
+    # V1 should use the new scheduler by default.
+    # Swap it only if this arg is set to the original V0 default
+    if self.scheduler_cls == EngineArgs.scheduler_cls:
+        self.scheduler_cls = "vllm.v1.core.sched.scheduler.Scheduler"
+
+    # When no user override, set the default values based on the usage
+    # context.
+    # Use different default values for different hardware.
+
+    # Try to query the device name on the current platform. If it fails,
+    # it may be because the platform that imports vLLM is not the same
+    # as the platform that vLLM is running on (e.g. the case of scaling
+    # vLLM with Ray) and has no GPUs. In this case we use the default
+    # values for non-H100/H200 GPUs.
+    from vllm.platforms import current_platform
+    try:
+        device_memory = current_platform.get_device_total_memory()
+        device_name = current_platform.get_device_name().lower()
+    except Exception:
+        # This is only used to set default_max_num_batched_tokens
+        device_memory = 0
+
+    # NOTE(Kuntai): Setting large `max_num_batched_tokens` for A100 reduces
+    # throughput, see PR #17885 for more details.
+    # So here we do an extra device name check to prevent such regression.
+    if device_memory >= 70 * GiB_bytes and "a100" not in device_name:
+        # For GPUs like H100 and MI300x, use larger default values.
+        default_max_num_batched_tokens = {
+            UsageContext.LLM_CLASS: 16384,
+            UsageContext.OPENAI_API_SERVER: 8192,
+        }
+        default_max_num_seqs = 1024
+    else:
+        # TODO(woosuk): Tune the default values for other hardware.
+        default_max_num_batched_tokens = {
+            UsageContext.LLM_CLASS: 8192,
+            UsageContext.OPENAI_API_SERVER: 2048,
+        }
+        default_max_num_seqs = 256
+
+    # tpu specific default values.
+    if current_platform.is_tpu():
+        default_max_num_batched_tokens_tpu = {
+            UsageContext.LLM_CLASS: {
+                'V6E': 2048,
+                'V5E': 1024,
+                'V5P': 512,
+            },
+            UsageContext.OPENAI_API_SERVER: {
+                'V6E': 1024,
+                'V5E': 512,
+                'V5P': 256,
+            }
+        }
+
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: only set max_num_batched_tokens when enable chunked_prefill
+    '''
+    use_context_value = usage_context.value if usage_context else None
+    if (self.max_num_batched_tokens is None
+            and usage_context in default_max_num_batched_tokens
+            and self.enable_chunked_prefill):
+        if current_platform.is_tpu():
+            chip_name = current_platform.get_device_name()
+            if chip_name in default_max_num_batched_tokens_tpu[
+                    usage_context]:
+                self.max_num_batched_tokens = \
+                    default_max_num_batched_tokens_tpu[
+                        usage_context][chip_name]
+            else:
+                self.max_num_batched_tokens = \
+                    default_max_num_batched_tokens[usage_context]
+        else:
+            self.max_num_batched_tokens = default_max_num_batched_tokens[
+                usage_context]
+        logger.debug(
+            "Setting max_num_batched_tokens to %d for %s usage context.",
+            self.max_num_batched_tokens, use_context_value)
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+
+    if self.max_num_seqs is None:
+        self.max_num_seqs = default_max_num_seqs
+
+        logger.debug("Setting max_num_seqs to %d for %s usage context.",
+                        self.max_num_seqs, use_context_value)
+
+
+def vllm__engine__arg_utils__EngineArgs__create_engine_config(
+    self,
+    usage_context: Optional[UsageContext] = None
+) -> VllmConfig:
+    '''
+    =============================
+    Modify by vllm_mlu
+    =============================
+    @brief: add data parallel params to parallel config.
+    '''
+    if self.mlu_config and "decoder_attn_dtype" in self.mlu_config:
+        if self.mlu_config.get("decoder_attn_dtype") in ["int8", "fp8", "fp8_e4m3"]:
+            self.kv_cache_dtype = self.mlu_config.get("decoder_attn_dtype")
+
+    engine_config = vllm__engine__arg_utils__EngineArgs__create_engine_config_org(
+        self, usage_context)
+
+    world_size = engine_config.parallel_config.world_size_across_dp
+    tensor_parallel_size = engine_config.parallel_config.tensor_parallel_size
+    embedding_tp_size = engine_config.mlu_config.layer_embedding_logit_tp_size
+    if embedding_tp_size:
+        assert embedding_tp_size >= tensor_parallel_size and embedding_tp_size <= world_size, (
+            f"embedding_tp_size = {embedding_tp_size} out of bounds. "
+            f"Require {tensor_parallel_size} ≤ size ≤ {world_size}")
+    dense_mlp_tp_size = engine_config.mlu_config.layer_dense_mlp_tp_size
+    if dense_mlp_tp_size:
+        assert dense_mlp_tp_size >= 1 and dense_mlp_tp_size <= world_size, (
+            f"dense_mlp_tp_size = {dense_mlp_tp_size} out of bounds. Require 1 ≤ size ≤ {world_size}")
+        if dense_mlp_tp_size != world_size:
+            assert not engine_config.mlu_config.is_dpsk_mcc_enabled, (
+                "dense_mlp_tp_size is not supported when dpsk mcc is enabled.")
+
+    if ((engine_config.parallel_config.data_parallel_size > 1 or engine_config.speculative_config is not None)
+        and engine_config.mlu_config.prefill_enable_mlugraph):
+        logger.info("Data parallel or speculative is enabled, forcing context mlugraph to be disabled.")
+        engine_config.mlu_config.prefill_enable_mlugraph = False
+    if engine_config.mlu_config.decoder_attn_dtype:
+        if engine_config.mlu_config.decoder_attn_dtype not in get_args(CacheDType):
+            raise ValueError(f"MLU backend does not support {engine_config.mlu_config.decoder_attn_dtype} "
+                            f"decoder_attn_dtype for now")
+        if not engine_config.model_config.is_deepseek_mla and engine_config.mlu_config.decoder_attn_dtype != "auto":
+            raise ValueError(f"mlu_config.decoder_attn_dtype only support deepseek_mla model")
+        if engine_config.mlu_config.decoder_attn_dtype.startswith("fp8"):
+            from vllm.platforms import current_platform
+            if current_platform.is_out_of_tree() and current_platform.get_device_capability().major < 6:
+                raise ValueError(f"MLU backend does not support {engine_config.mlu_config.decoder_attn_dtype} "
+                                f"decoder_attn_dtype for now")
+            if engine_config.mlu_config.decoder_attn_dtype == "fp8_e5m2":
+                raise ValueError("MLU backend does not support fp8_e5m2 decoder_attn_dtype for now")
+    '''
+    ==================
+    End of MLU Hijack
+    ==================
+    '''
+    return engine_config
+
+
+MluHijackObject.apply_hijack(EngineArgs,
+                             EngineArgs._is_v1_supported_oracle,
+                             vllm__engine__arg_utils__EngineArgs___is_v1_supported_oracle)
+MluHijackObject.apply_hijack(EngineArgs,
+                             EngineArgs._set_default_args_v1,
+                             vllm__engine__arg_utils__EngineArgs___set_default_args_v1)
+MluHijackObject.apply_hijack(EngineArgs,
+                             EngineArgs.create_engine_config,
+                             vllm__engine__arg_utils__EngineArgs__create_engine_config)

