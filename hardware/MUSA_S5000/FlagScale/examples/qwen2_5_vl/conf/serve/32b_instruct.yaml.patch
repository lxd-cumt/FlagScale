diff --git a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
index 0f317685..19347576 100644
--- a/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
+++ b/examples/qwen2_5_vl/conf/serve/32b_instruct.yaml
@@ -1,14 +1,12 @@
 - serve_id: vllm_model
   engine: vllm
   engine_args:
-    model: /share/Qwen2.5-VL-32B-Instruct # should be customized
-    served_model_name: qwenvl32-nv-flagos
+    model: ${oc.env:QWEN3_PATH}
     tensor_parallel_size: 8
     max_model_len: 32768
     pipeline_parallel_size: 1
     max_num_seqs: 8 # Even at full 32,768 context usage, 8 concurrent operations won't trigger OOM
     gpu_memory_utilization: 0.9
-    limit_mm_per_prompt: '{"image": 18}' # should be customized, 18 images/request is enough for most scenarios
-    port: 9010
+    port: ${oc.env:QWEN3_PORT}
     trust_remote_code: true
-    enable_chunked_prefill: true
+    enforce_eager: True

