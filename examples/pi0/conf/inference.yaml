defaults:
  - _self_
  - inference: pi0

experiment:
  exp_name: pi0_inference
  exp_dir: outputs/${experiment.exp_name}
  model: /models/lerobot/pi0
  task:
    type: inference
    backend: vllm # TODO: Remove this restriction
    entrypoint: flagscale/inference/inference_pi.py
  runner:
    hostfile: null
  cmds:
    before_start: null
  envs:
    CUDA_VISIBLE_DEVICES: 0
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    # Optionally, set HF_HOME and HF_ENDPOINT

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
